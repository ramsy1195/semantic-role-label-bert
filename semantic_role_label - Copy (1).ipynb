{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42e52a7-714d-4248-b5ad-9a5da2981faf",
   "metadata": {
    "id": "6703077a"
   },
   "source": [
    "# Semantic Role Labelling with BERT\n",
    "\n",
    "The goal of this project is to train and evaluate a PropBank-style semantic role labeling (SRL) system. Following (Collobert et al. 2011) and others, we will treat this problem as a sequence-labeling task. For each input token, the system will predict a B-I-O tag, as illustrated in the following example:\n",
    "\n",
    "|The|judge|scheduled|to|preside|over|his|trial|was|removed|from|the|case|today|.|             \n",
    "|---|-----|---------|--|-------|----|---|-----|---|-------|----|---|----|-----|-|             \n",
    "|B-ARG1|I-ARG1|B-V|B-ARG2|I-ARG2|I-ARG2|I-ARG2|I-ARG2|O|O|O|O|O|O|O|\n",
    "|||schedule.01|||||||||||||\n",
    "\n",
    "Note that the same sentence may have multiple annotations for different predicates\n",
    "\n",
    "|The|judge|scheduled|to|preside|over|his|trial|was|removed|from|the|case|today|.|             \n",
    "|---|-----|---------|--|-------|----|---|-----|---|-------|----|---|----|-----|-|             \n",
    "|B-ARG1|I-ARG1|I-ARG1|I-ARG1|I-ARG1|I-ARG1|I-ARG1|I-ARG1|O|B-V|B-ARG2|I-ARG2|I-ARG2|B-ARGM-TMP|O|\n",
    "||||||||||remove.01||||||\n",
    "\n",
    "and not all predicates need to be verbs\n",
    "\n",
    "|The|judge|scheduled|to|preside|over|his|trial|was|removed|from|the|case|today|.|             \n",
    "|---|-----|---------|--|-------|----|---|-----|---|-------|----|---|----|-----|-|    \n",
    "|O|O|O|O|O|O|B-ARG1|B-V|O|O|O|O|O|O|O|\n",
    "||||||||try.02||||||||\n",
    "\n",
    "The SRL system will be implemented in [PyTorch](https://pytorch.org/). We will use BERT (in the implementation provided by the [Huggingface transformers](https://huggingface.co/docs/transformers/index) library) to compute contextualized token representations and a custom classification head to predict semantic roles. We will fine-tune the pretrained BERT model on the SRL task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12fe94",
   "metadata": {
    "id": "ff12fe94"
   },
   "source": [
    "### Overview of the Approach\n",
    "\n",
    "The model we will train is pretty straightforward. Essentially, we will just encode the sentence with BERT, then take the contextualized embedding for each token and feed it into a classifier to predict the corresponding tag.\n",
    "\n",
    "Because we are only working on argument identification and labeling (not predicate identification), it is essentially that we tell the model where the predicate is. This can be accomplished in various ways. The approach we will choose here repurposes Bert's *segment embeddings*.\n",
    "\n",
    "Recall that BERT is trained on two input sentences, seperated by [SEP], and on a next-sentence-prediction objective (in addition to the masked LM objective). To help BERT comprehend which sentence a given token belongs to, the original BERT uses a segment embedding, using A for the first sentene, and B for the second sentence 2.\n",
    "Because we are labeling only a single sentence at a time, we can use the segment embeddings to indicate the predicate position instead: The predicate is labeled as segment B (1) and all other tokens will be labeled as segment A (0).\n",
    "\n",
    "<img src=\"https://github.com/daniel-bauer/4705-f23-hw5/blob/main/bert_srl_model.png?raw=true\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68325e53",
   "metadata": {
    "id": "68325e53"
   },
   "source": [
    "## Setup: GCP, Jupyter, PyTorch, GPU\n",
    "\n",
    "To make sure that PyTorch is available and can use the GPU, we run the following cell which should return True. If it doesn't, we should recheck if the GPU drivers and CUDA are installed correctly.\n",
    "\n",
    "Note: GPU support is required for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad94eef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aad94eef",
    "outputId": "f1336c50-7e8b-4629-e4e2-cdc876aab01e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1c8ac",
   "metadata": {
    "id": "3fe1c8ac"
   },
   "source": [
    "## Dataset: Ontonotes 5.0 English SRL annotations\n",
    "\n",
    "We will work with the English part of the [Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19) data. This is an extension of PropBank, using the same type of annotation. Ontonotes contains annotations other than predicate/argument structures, but we will use the PropBank style SRL annotations only. *Important*: The data set used in this project was provided to me for use by my university. My university is a subscriber to LDC and is allowed to use the data for educational purposes. However, the dataset may not be used in projects unrelated to teaching or research conducted at my university. Hence the following cell where the data source link is provided for download is hidden from view. The data is downloaded in ontonotes_srl.zip.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14aad58e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14aad58e",
    "outputId": "d8ec75f9-a039-4899-8141-46d8be119c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ontonotes_srl.zip\n",
      "  inflating: propbank_dev.tsv        \n",
      "  inflating: propbank_test.tsv       \n",
      "  inflating: propbank_train.tsv      \n",
      "  inflating: role_list.txt           \n"
     ]
    }
   ],
   "source": [
    "! unzip ontonotes_srl.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae08a9",
   "metadata": {
    "id": "8cae08a9"
   },
   "source": [
    "The data has been pre-processed in the following format. There are three files:\n",
    "\n",
    "`propbank_dev.tsv`\t`propbank_test.tsv`\t`propbank_train.tsv`\n",
    "\n",
    "Each of these files is in a tab-separated value format. A single predicate/argument structure annotation consists of four rows. For example\n",
    "\n",
    "```\n",
    "ontonotes/bc/cnn/00/cnn_0000.152.1\n",
    "The     judge   scheduled       to      preside over    his     trial   was     removed from    the     case    today   /.\n",
    "                schedule.01\n",
    "B-ARG1  I-ARG1  B-V     B-ARG2  I-ARG2  I-ARG2  I-ARG2  I-ARG2  O       O       O       O       O       O       O\n",
    "```\n",
    "\n",
    "* The first row is a unique identifier (1st annotation of the 152nd sentence in the file ontonotes/bc/cnn/00/cnn_0000).\n",
    "* The second row contains the tokens of the sentence (tab-separated).\n",
    "* The third row contains the probank frame name for the predicate (empty field for all other tokens).\n",
    "* The fourth row contains the B-I-O tag for each token.\n",
    "\n",
    "The file `rolelist.txt` contains a list of propbank BIO labels in the dataset (i.e. possible output tokens). This list has been filtered to contain only roles that appeared more than 1000 times in the training data.\n",
    "We will load this list and create mappings from numeric ids to BIO tags and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4c027a",
   "metadata": {
    "id": "dc4c027a"
   },
   "outputs": [],
   "source": [
    "role_to_id = {}\n",
    "with open(\"role_list.txt\",'r') as f:\n",
    "    role_list = [x.strip() for x in f.readlines()]\n",
    "    role_to_id = dict((role, index) for (index, role) in enumerate(role_list))\n",
    "    role_to_id['[PAD]'] = -100\n",
    "\n",
    "    id_to_role = dict((index, role) for (role, index) in role_to_id.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07cdf31",
   "metadata": {
    "id": "f07cdf31"
   },
   "source": [
    "Note that we are also mapping the '[PAD]' token to the value -100. This allows the loss function to ignore these tokens during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88096be5",
   "metadata": {
    "id": "88096be5"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae727bf",
   "metadata": {
    "id": "aae727bf"
   },
   "source": [
    "## Part 1 - Data Preparation\n",
    "\n",
    "Before we can build the SRL model, we first need to preprocess the data.\n",
    "\n",
    "\n",
    "### 1.1 - Tokenization\n",
    "\n",
    "One challenge is that the pre-trained BERT model uses subword (\"WordPiece\") tokenization, but the Ontonotes data does not. Fortunately Huggingface transformers provides a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a2d7d03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457,
     "referenced_widgets": [
      "083fb0fe0cf845aca6e4433554f47472",
      "c25773ebcb154ae58f81883a060318a8",
      "d569d613d3a444deaafa9e86d3d63cc7",
      "629361ffe8774d34bca7729a35de39af",
      "7cafbd762f5f4d29902610d7c0192258",
      "3278fb3ef6674bda96eb2be68622ca7f",
      "5a173c15ff884df8ac16fd07f93a6a21",
      "883c04ea96ab44cd818f138fb752d87a",
      "68ac994bfd3249ed856b70cb58ccd1d3",
      "4487566f87a4471f8d51d763e5983f19",
      "340c2f2842014ecdb27c3d594f3978b4",
      "684a8ed0e41a44fbbbdd10083a9cd904",
      "5024059699ed40ca800f35ccef648aae",
      "09780765ba8846918f8eb0400fb36fa5",
      "3fcea0cf640845bfad8a2c2f2ae55016",
      "fb0f1e9cf1ae49faa2e36b4ae178d410",
      "e4e5ac4583f84fa6a52067ebadda9bd9",
      "9c704f72fa994254a147e68b08087552",
      "a1c7f599df1f436facaa0a438a0e1c19",
      "5c751ebfd23b4a488ab8f0d2ed76078f",
      "2204eaa52a9b4b0c84929d9fc9f7a9dd",
      "8ddf2a3bd5ee4c33b7f914106fa7b5c6",
      "396859958cd043f89e7677150fd143b5",
      "a6308c945da04be5b2a90c082035b6cf",
      "97607619aff44bc790da6d1cc5023844",
      "4175dba6cf9c46d6b22a6de5d4923a6c",
      "2bcf0f0129534298afeef87a31d7946d",
      "ee2cc72bcf0c4524ab8c7d94411dabd3",
      "549688f0959347bba77211cde997d872",
      "0a251cb20ac041cbacf24e335cca385a",
      "bf251bb6b988495eb6bc015f7e40d3e1",
      "6a2625704931408c871aef791aabee90",
      "74a7dc2aaa1e49738df23471fb030f65",
      "f76a3f18e62a4066b155ab7b81e1f15e",
      "90af5cf87c8947b7be8ef2914851758e",
      "2bb12916e80a405996dc5ac1e20d354b",
      "2fa9db4610b04313806f51d27fb7c40b",
      "04319cfa3c2f4378bda7f141faa1215c",
      "e0e6030fd2e9424f97f82a50b1823f58",
      "b34d8cf8cb4e49c9aeda9a468c7b4172",
      "7df23a4094a541c69d097322630385e8",
      "413cab8e3725427ea7e70d75b025e73a",
      "e26f23a7eefa4063ac882bfdb3c4097d",
      "eb1c22115ab2486a944777fbf2ec9851"
     ]
    },
    "id": "8a2d7d03",
    "outputId": "7eedc8e4-fa30-45d7-9755-216b13e7292d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083fb0fe0cf845aca6e4433554f47472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684a8ed0e41a44fbbbdd10083a9cd904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396859958cd043f89e7677150fd143b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76a3f18e62a4066b155ab7b81e1f15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'un',\n",
       " '##bel',\n",
       " '##ie',\n",
       " '##va',\n",
       " '##bly',\n",
       " 'boring',\n",
       " 'test',\n",
       " 'sentence',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer.tokenize(\"This is an unbelievably boring test sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1924bd",
   "metadata": {
    "id": "ac1924bd"
   },
   "source": [
    "We need to be able to maintain the correct labels (B-I-O tags) for each of the subwords.\n",
    "The following function that takes a list of tokens and a list of B-I-O labels of the same length as parameters, and returns a new token / label pair, as illustrated in the following example.\n",
    "\n",
    "\n",
    "```\n",
    ">>> tokenize_with_labels(\"the fancyful penguin devoured yummy fish .\".split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\".split(), tokenizer)\n",
    "(['the',\n",
    "  'fancy',\n",
    "  '##ful',\n",
    "  'penguin',\n",
    "  'dev',\n",
    "  '##oured',\n",
    "  'yu',\n",
    "  '##mmy',\n",
    "  'fish',\n",
    "  '.'],\n",
    " ['B-ARG0',\n",
    "  'I-ARG0',\n",
    "  'I-ARG0',\n",
    "  'I-ARG0',\n",
    "  'B-V',\n",
    "  'I-V',\n",
    "  'B-ARG1',\n",
    "  'I-ARG1',\n",
    "  'I-ARG1',\n",
    "  'O'])\n",
    "\n",
    "```\n",
    "\n",
    "To approach this problem, we will iterate through each word/label pair in the sentence and call the tokenizer on the word. This may result in one or more tokens. The, we will create the correct number of labels to match the number of tokens. We have to take care to not generate multiple B- tokens.\n",
    "\n",
    "\n",
    "This approach is a bit slower than tokenizing the entire sentence, but is necessary to produce proper input tokenization for the pre-trained BERT model, and the matching target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8140fc39",
   "metadata": {
    "id": "8140fc39"
   },
   "outputs": [],
   "source": [
    "def tokenize_with_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "      tokens = tokenizer.tokenize(word)\n",
    "\n",
    "      for i, token in enumerate(tokens):\n",
    "          if i == 0 or label == 'O':\n",
    "              tokenized_sentence.append(token)\n",
    "              labels.append(label)\n",
    "          else:\n",
    "              tokenized_sentence.append(token)\n",
    "              labels.append('I-' + label[2:])  # Remove 'B-' and add 'I-'\n",
    "\n",
    "    return tokenized_sentence, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f748d120",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f748d120",
    "outputId": "f9b58a80-c68f-42a6-eb0b-7d1d434da682"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'fancy',\n",
       "  '##ful',\n",
       "  'penguin',\n",
       "  'dev',\n",
       "  '##oured',\n",
       "  'yu',\n",
       "  '##mmy',\n",
       "  'fish',\n",
       "  '.'],\n",
       " ['B-ARG0',\n",
       "  'I-ARG0',\n",
       "  'I-ARG0',\n",
       "  'I-ARG0',\n",
       "  'B-V',\n",
       "  'I-V',\n",
       "  'B-ARG1',\n",
       "  'I-ARG1',\n",
       "  'I-ARG1',\n",
       "  'O'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_labels(\"the fancyful penguin devoured yummy fish .\".split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\".split(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb9076",
   "metadata": {
    "id": "77bb9076"
   },
   "source": [
    "### 1.2 Loading the Dataset\n",
    "\n",
    "Next, we are creating a PyTorch [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. This class acts as a contained for the training, development, and testing data in memory. \n",
    "\n",
    "1.2.1 Let us write the \\_\\_init\\_\\_(self, filename) method that reads in the data from a data file (specified by the filename).\n",
    "\n",
    "For each annotation we start with  the tokens in the sentence, and the BIO tags. Then we create the following\n",
    "\n",
    "1. calling the `tokenize_with_labels` function to tokenize the sentence.\n",
    "2. adding the (token, label) pair to the self.items list.\n",
    "\n",
    "1.2.2 Let us write the \\_\\_len\\_\\_(self) method that returns the total number of items.\n",
    "\n",
    "1.2.3 Let us write the \\_\\_getitem\\_\\_(self, k) method that returns a single item in a format BERT will understand.\n",
    "* We need to process the sentence by adding \"\\[CLS\\]\" as the first token and \"\\[SEP\\]\" as the last token. The need to pad the token sequence to 128 tokens using the \"\\[PAD\\]\" symbol. This needs to happen both for the inputs (sentence token sequence) and outputs (BIO tag sequence).\n",
    "* We need to create an *attention mask*, which is a sequence of 128 tokens indicating the actual input symbols (as a 1) and \\[PAD\\] symbols (as a 0).\n",
    "* We need to create a *predicate indicator* mask, which is a sequence of 128 tokens with at most one 1, in the position of the \"B-V\" tag. All other entries should be 0. The model will use this information to understand where the predicate is located.\n",
    "\n",
    "* Finally, we need to convert the token and tag sequence into numeric indices. For the tokens, this can be done using the `tokenizer.convert_tokens_to_ids` method. For the tags, use the `role_to_id` dictionary.\n",
    "Each sequence must be a pytorch tensor of shape (1,128). You can convert a list of integer values like this `torch.tensor(token_ids, dtype=torch.long)`.\n",
    "\n",
    "To keep everything organized, we will return a dictionary in the following format\n",
    "\n",
    "```\n",
    "{'ids': token_tensor,\n",
    " 'targets': tag_tensor,\n",
    " 'mask': attention_mask_tensor,\n",
    " 'pred': predicate_indicator_tensor}\n",
    "```\n",
    "\n",
    "\n",
    "(To debug these, we will read in the first annotation only / the first few annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5bd32f",
   "metadata": {
    "id": "9f5bd32f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SrlData(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        super(SrlData, self).__init__()\n",
    "\n",
    "        self.max_len = 128  # the max number of tokens inputted to the transformer.\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        self.items = []\n",
    "\n",
    "        # Read the data from the provided file\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for i in range(0, len(lines), 4):\n",
    "            sentence_tokens = lines[i + 1].strip().split()\n",
    "            frame_name = lines[i + 2].strip()\n",
    "            tags = lines[i + 3].strip().split()\n",
    "\n",
    "            # Print each sentence's tokenized information for debugging\n",
    "            #print(f\"\\nProcessing sentence {i//4 + 1} (line {i+1}):\")\n",
    "            #print(\"Sentence tokens:\", sentence_tokens)\n",
    "            #print(\"Frame name:\", frame_name)\n",
    "            #print(\"Tags:\", tags)\n",
    "\n",
    "            # Tokenize with labels\n",
    "            tokenized_sentence, tokenized_labels = tokenize_with_labels(sentence_tokens, tags, self.tokenizer)\n",
    "            #print(\"Tokenized sentence:\", tokenized_sentence)\n",
    "            #print(\"Tokenized labels:\", tokenized_labels)\n",
    "\n",
    "            self.items.append((tokenized_sentence, tokenized_labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "\n",
    "        tokenized_sentence, tokenized_labels = self.items[k]\n",
    "        tokens = ['[CLS]'] + tokenized_sentence + ['[SEP]']\n",
    "        labels = ['O'] + tokenized_labels + ['O']\n",
    "\n",
    "        padding_length = self.max_len - len(tokens)\n",
    "        if padding_length > 0:\n",
    "            tokens = tokens + ['[PAD]'] * padding_length\n",
    "            labels = labels + ['[PAD]'] * padding_length\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            labels = labels[:self.max_len]\n",
    "\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        label_ids = [role_to_id.get(label, -100) for label in labels]\n",
    "\n",
    "        attention_mask = [1] * len(tokens) + [0] * (self.max_len - len(tokens))\n",
    "        predicate_mask = [0] * self.max_len\n",
    "        try:\n",
    "            pred_index = labels.index('B-V')\n",
    "            predicate_mask[pred_index] = 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Convert the lists to tensors\n",
    "        token_tensor = torch.tensor(token_ids, dtype=torch.long)  # Shape (128)\n",
    "        label_tensor = torch.tensor(label_ids, dtype=torch.long)  # Shape (128)\n",
    "        attn_mask_tensor = torch.tensor(attention_mask, dtype=torch.long)  # Shape (128)\n",
    "        pred_tensor = torch.tensor(predicate_mask, dtype=torch.long)  # Shape (128)\n",
    "\n",
    "        return {\n",
    "            'ids': token_tensor,\n",
    "            'targets': label_tensor,\n",
    "            'mask': attn_mask_tensor,\n",
    "            'pred': pred_tensor\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0b476d",
   "metadata": {
    "id": "da0b476d"
   },
   "outputs": [],
   "source": [
    "# Reading the training data takes a while for the entire data because we preprocess all data offline\n",
    "data = SrlData(\"propbank_train.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730aff9",
   "metadata": {
    "id": "0730aff9"
   },
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb11f9c0",
   "metadata": {
    "id": "cb11f9c0"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, CrossEntropyLoss\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da684eff",
   "metadata": {
    "id": "da684eff"
   },
   "source": [
    "We will define the pyTorch model as a subclass of the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37515695",
   "metadata": {
    "id": "37515695"
   },
   "outputs": [],
   "source": [
    "class SrlModel(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(SrlModel, self).__init__()\n",
    "\n",
    "        self.encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # The following two lines would freeze the BERT parameters and allow us to train the classifier by itself.\n",
    "        # We are fine-tuning the model, so you can leave this commented out!\n",
    "        # for param in self.encoder.parameters():\n",
    "        #    param.requires_grad = False\n",
    "\n",
    "        # The linear classifier head, see model figure in the introduction.\n",
    "        self.classifier = Linear(768, len(role_to_id))\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, pred_indicator):\n",
    "\n",
    "        # This defines the flow of data through the model\n",
    "\n",
    "        # Note the use of the \"token type ids\" which represents the segment encoding explained in the introduction.\n",
    "        # In our segment encoding, 1 indicates the predicate, and 0 indicates everything else.\n",
    "        bert_output =  self.encoder(input_ids=input_ids, attention_mask=attn_mask, token_type_ids=pred_indicator)\n",
    "\n",
    "        enc_tokens = bert_output[0] # the result of encoding the input with BERT\n",
    "        logits = self.classifier(enc_tokens) #feed into the classification layer to produce scores for each tag.\n",
    "\n",
    "        # Note that we are only interested in the argmax for each token, so we do not have to normalize\n",
    "        # to a probability distribution using softmax. The CrossEntropyLoss loss function takes this into account.\n",
    "        # It essentially computes the softmax first and then computes the negative log-likelihood for the target classes.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba23ec3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "862bc8207df6476d9d43764adc231512",
      "a509898b9f95417bbbf825cb5dc21b04",
      "b1c34a51205042eb8b9f468ff6765e6a",
      "73e7021be68b44d99da5004911fae79b",
      "5751dca99ea04fd4960716622916cc29",
      "1f3d9e742ff64c16a6f83afcb770af08",
      "a6ba50d691994965b2c24d44b02f6280",
      "8c418afa0611434f8293116b6f57d655",
      "2fd87c5541fc4107bf081c4e89ce9fd6",
      "020acbc43d2b449481fa165414c07b8a",
      "f7707a6351ef44b3aa6a0a1483a36762"
     ]
    },
    "id": "ba23ec3f",
    "outputId": "df7a1317-39a6-47ed-9f8e-37203966fca1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862bc8207df6476d9d43764adc231512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SrlModel().to('cuda') # create new model and store weights in GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103d32e",
   "metadata": {
    "id": "9103d32e"
   },
   "source": [
    "Now we are ready to try running the model with just a single input example to check if it is working correctly. Clearly it has not been trained, so the output is not what we expect. But we can see what the loss looks like for an initial sanity check.\n",
    "\n",
    "Next steps:\n",
    "* Taking a single data item from the dev set, as provided by our Dataset class defined above. Obtaining the input token ids, attention mask, predicate indicator mask, and target labels.\n",
    "* Running the model on the ids, attention mask, and predicate mask like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1805480",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1805480",
    "outputId": "eea58b7f-eb4f-4ec0-f52c-582c7921aed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs of the model: tensor([[[-0.1954, -0.1678,  0.2691,  ...,  0.0978,  0.1336, -0.0766],\n",
      "         [-0.3415,  0.8121,  0.0829,  ..., -0.6333,  0.2712,  0.4700],\n",
      "         [-0.5718,  0.4183,  0.5084,  ..., -0.1510, -0.2816,  0.6788],\n",
      "         ...,\n",
      "         [-0.2610, -0.4558,  0.0321,  ...,  0.0683,  0.0917, -0.0017],\n",
      "         [-0.2162, -0.5150,  0.0657,  ...,  0.0234,  0.1865,  0.0235],\n",
      "         [-0.1982, -0.5447, -0.0313,  ...,  0.0559,  0.2737,  0.0565]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# pick an item from the dataset. Then run\n",
    "\n",
    "devData = SrlData(\"propbank_dev.tsv\")\n",
    "example = devData[0]\n",
    "\n",
    "# Extract inputs from the example and remove the extra dimension by using squeeze(0)\n",
    "ids = example['ids'].to('cuda')  # Shape: (1, 128)\n",
    "mask = example['mask'].to('cuda')  # Shape: (1, 128)\n",
    "pred = example['pred'].to('cuda')  # Shape: (1, 128)\n",
    "targets = example['targets'].to('cuda')  # Shape: (1, 128)\n",
    "\n",
    "outputs = model(ids.unsqueeze(0), mask.unsqueeze(0), pred.unsqueeze(0))\n",
    "\n",
    "print(\"Outputs of the model:\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7fd38b",
   "metadata": {
    "id": "cd7fd38b"
   },
   "source": [
    "Let us compute the loss on this one item only.\n",
    "The initial loss should be close to -ln(1/num_labels)\n",
    "\n",
    "Without training we would assume that all labels for each token (including the target label) are equally likely, so the negative log probability for the targets should be approximately $$-\\ln(\\frac{1}{\\text{num_labels}}).$$ This is what the loss function should return on a single example. This is a good sanity check to run for any multi-class prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a5f881d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a5f881d",
    "outputId": "f89ec2bd-fd1a-4051-d27c-b4d489bae5e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.970291913552122"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-math.log(1 / len(role_to_id), math.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a124d4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a124d4f",
    "outputId": "e575c86b-0708-4e15-be39-f8a83dae91b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed loss: 3.9103169441223145\n"
     ]
    }
   ],
   "source": [
    "loss_function = CrossEntropyLoss(ignore_index = -100, reduction='mean')\n",
    "\n",
    "num_labels = len(role_to_id)\n",
    "\n",
    "# Reshape the output to (batch_size * sequence_length, num_labels)\n",
    "logits = outputs.view(-1, num_labels)\n",
    "\n",
    "# Reshape the target labels to (batch_size * sequence_length)\n",
    "targets = targets.view(-1)\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_function(logits, targets)\n",
    "\n",
    "# Print the loss value\n",
    "print(f\"Computed loss: {loss.item()}\")\n",
    "# complete this. Note that you still have to provide a (batch_size, input_pos)\n",
    "# tensor for each parameter, where batch_size =1\n",
    "\n",
    "# outputs = model(ids, mask, pred)\n",
    "# loss = loss_function(...)\n",
    "# loss.item()   #this should be approximately the score from the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a1037",
   "metadata": {
    "id": "669a1037"
   },
   "source": [
    "At this point, we should also obtain the actual predictions by taking the argmax over each position.\n",
    "The result should look something like this (values will differ).\n",
    "\n",
    "```\n",
    "tensor([[ 1,  4,  4,  4,  4,  4,  5, 29, 29, 29,  4, 28,  6, 32, 32, 32, 32, 32,\n",
    "         32, 32, 30, 30, 32, 30, 32,  4, 32, 32, 30,  4, 49,  4, 49, 32, 30,  4,\n",
    "         32,  4, 32, 32,  4,  2,  4,  4, 32,  4, 32, 32, 32, 32, 30, 32, 32, 30,\n",
    "         32,  4,  4, 49,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  6,  6, 32, 32,\n",
    "         30, 32, 32, 32, 32, 32, 30, 30, 30, 32, 30, 49, 49, 32, 32, 30,  4,  4,\n",
    "          4,  4, 29,  4,  4,  4,  4,  4,  4, 32,  4,  4,  4, 32,  4, 30,  4, 32,\n",
    "         30,  4, 32,  4,  4,  4,  4,  4, 32,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
    "          4,  4]], device='cuda:0')\n",
    "```\n",
    "\n",
    "Then we will use the id_to_role dictionary to decode to actual tokens.\n",
    "\n",
    "```\n",
    "['[CLS]', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'O', 'B-V', 'B-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG1', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'O', 'I-ARGM-TMP', 'O', 'I-ARGM-TMP', 'I-ARG2', 'I-ARG1', 'O', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'O', '[SEP]', 'O', 'O', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'O', 'O', 'I-ARGM-TMP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG2', 'I-ARG1', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'O', 'O', 'O', 'O', 'I-ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ARG2', 'O', 'O', 'O', 'I-ARG2', 'O', 'I-ARG1', 'O', 'I-ARG2', 'I-ARG1', 'O', 'I-ARG2', 'O', 'O', 'O', 'O', 'O', 'I-ARG2', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "```\n",
    "\n",
    "For now, we just make sure we understand how to do this for a single example. Later, we will write a more formal function to do this once we have trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69c2dcc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69c2dcc0",
    "outputId": "e6079c42-7a19-446d-ef0b-a0e632b03fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([38,  3, 52, 38, 38, 38, 12, 12, 22, 40,  3, 15, 19, 15, 12, 17, 27, 20,\n",
      "        20, 34, 12, 20, 31, 17, 14, 35, 20,  5,  5, 44, 44, 44, 44, 44, 19, 19,\n",
      "        19, 19, 19, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "        38, 38, 38, 38, 38, 38, 38, 26, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "        38, 38, 38, 38, 38, 38, 38, 38, 37, 44, 44, 44, 44, 38,  8, 44, 44, 44,\n",
      "         8, 44, 44, 44, 44, 44, 44, 44, 44, 38, 37, 44, 44, 44, 44, 44, 44, 44,\n",
      "        44, 44, 44, 44, 44, 44, 44,  8,  8,  8, 44,  8,  8, 44, 44, 44, 44, 44,\n",
      "        44, 38], device='cuda:0')\n",
      "Input Tokens: ['[CLS]', 'in', 'the', 'summer', 'of', '2005', ',', 'a', 'picture', 'that', 'people', 'have', 'long', 'been', 'looking', 'forward', 'to', 'started', 'emerging', 'with', 'frequency', 'in', 'various', 'major', 'hong', 'kong', 'media', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Predicted Labels (indices): tensor([38,  3, 52, 38, 38, 38, 12, 12, 22, 40,  3, 15, 19, 15, 12, 17, 27, 20,\n",
      "        20, 34, 12, 20, 31, 17, 14, 35, 20,  5,  5, 44, 44, 44, 44, 44, 19, 19,\n",
      "        19, 19, 19, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "        38, 38, 38, 38, 38, 38, 38, 26, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "        38, 38, 38, 38, 38, 38, 38, 38, 37, 44, 44, 44, 44, 38,  8, 44, 44, 44,\n",
      "         8, 44, 44, 44, 44, 44, 44, 44, 44, 38, 37, 44, 44, 44, 44, 44, 44, 44,\n",
      "        44, 44, 44, 44, 44, 44, 44,  8,  8,  8, 44,  8,  8, 44, 44, 44, 44, 44,\n",
      "        44, 38], device='cuda:0')\n",
      "Predicted Labels (names): ['I-ARGM-DIR', '[prd]', 'I-V', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'B-ARGM-ADV', 'B-ARGM-ADV', 'B-ARGM-PRD', 'I-ARGM-EXT', '[prd]', 'B-ARGM-DIS', 'B-ARGM-MNR', 'B-ARGM-DIS', 'B-ARGM-ADV', 'B-ARGM-GOL', 'B-ARGM-LVB', 'B-ARGM-MOD', 'B-ARGM-MOD', 'I-ARG4', 'B-ARGM-ADV', 'B-ARGM-MOD', 'I-ARG1-DSP', 'B-ARGM-GOL', 'B-ARGM-DIR', 'I-ARGM-ADJ', 'B-ARGM-MOD', 'B-ARG0', 'B-ARG0', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'B-ARGM-MNR', 'B-ARGM-MNR', 'B-ARGM-MNR', 'B-ARGM-MNR', 'B-ARGM-MNR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'B-ARGM-CXN', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-CAU', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-DIR', 'B-ARG2', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'B-ARG2', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-DIR', 'I-ARGM-CAU', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'B-ARG2', 'B-ARG2', 'B-ARG2', 'I-ARGM-MOD', 'B-ARG2', 'B-ARG2', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-MOD', 'I-ARGM-DIR']\n"
     ]
    }
   ],
   "source": [
    "# The outputs are raw logits with shape (batch_size, sequence_length, num_labels)\n",
    "# Apply argmax to get the predicted label index for each token (batch_size, sequence_length)\n",
    "predicted_labels = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "# Flatten the predicted labels for easier access\n",
    "predicted_labels = predicted_labels.view(-1)  # Flatten to (batch_size * sequence_length)\n",
    "print(predicted_labels)\n",
    "\n",
    "# Convert predicted label indices to their corresponding label names using id_to_role\n",
    "predicted_label_names = [id_to_role.get(label.item(), -100) for label in predicted_labels]\n",
    "\n",
    "# Convert the input tokens to their string representations\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(ids.view(-1).cpu().numpy())\n",
    "\n",
    "# Print the results for comparison\n",
    "print(f\"Input Tokens: {input_tokens}\")\n",
    "print(f\"Predicted Labels (indices): {predicted_labels}\")\n",
    "print(f\"Predicted Labels (names): {predicted_label_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d52991",
   "metadata": {
    "id": "20d52991"
   },
   "source": [
    "## 3. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba250d",
   "metadata": {
    "id": "edba250d"
   },
   "source": [
    "pytorch provides a DataLoader class that can be wrapped around a Dataset to easily use the dataset for training. The DataLoader allows us to easily adjust the batch size and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ecd7448",
   "metadata": {
    "id": "2ecd7448"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(data, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7496c62",
   "metadata": {
    "id": "e7496c62"
   },
   "source": [
    "The following cell contains the main training loop. The code should work as written and report the loss after each batch,\n",
    "cumulative average loss after each 100 batches, and print out the final average loss after the epoch.\n",
    "\n",
    "Let us modify the training loop below so that it also computes the accuracy for each batch and reports the\n",
    "average accuracy after the epoch.\n",
    "The accuracy is the number of correctly predicted token labels out of the number of total predictions.\n",
    "We ensure that we exclude [PAD] tokens, i.e. tokens for which the target label is -100. It's okay to include [CLS] and [SEP] in the accuracy calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a7abad9",
   "metadata": {
    "id": "7a7abad9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLoss(ignore_index = -100, reduction='mean')\n",
    "\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(loader):\n",
    "\n",
    "        # Get the encoded data for this batch and push it to the GPU\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "        pred_mask = batch['pred'].to(device, dtype = torch.long)\n",
    "\n",
    "        # Run the forward pass of the model\n",
    "        logits = model(input_ids=ids, attn_mask=mask, pred_indicator=pred_mask)\n",
    "        loss = loss_function(logits.transpose(2,1), targets)\n",
    "        tr_loss += loss.item()\n",
    "        print(\"Batch loss: \", loss.item()) # can comment out if too verbose.\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            #torch.cuda.empty_cache() # can help if you run into memory issues\n",
    "            curr_avg_loss = tr_loss/nb_tr_steps\n",
    "            print(f\"Current average loss: {curr_avg_loss}\")\n",
    "\n",
    "        # Compute accuracy for this batch\n",
    "        # Exclude [PAD] tokens by ignoring targets == -100\n",
    "        mask = targets != -100  # mask for valid tokens (non-PAD tokens)\n",
    "\n",
    "        # Get the predicted labels (argmax over logits)\n",
    "        predicted_labels = torch.argmax(logits, dim=2)\n",
    "\n",
    "        # Count correct predictions: compare predicted labels with the true targets\n",
    "        correct_predictions += torch.sum((predicted_labels == targets) & mask).item()\n",
    "        total_predictions += torch.sum(mask).item()\n",
    "\n",
    "        # Run the backward pass to update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    avg_accuracy = correct_predictions / total_predictions  # Calculate accuracy for the epoch\n",
    "\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {avg_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d12b0",
   "metadata": {
    "id": "890d12b0"
   },
   "source": [
    "Now let's train the model for one epoch. This will take a while (up to a few hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bef88882",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bef88882",
    "outputId": "cecd84f0-b755-4625-9e1c-4ed2798b4c7e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Batch loss:  0.29186463356018066\n",
      "Batch loss:  0.3249627649784088\n",
      "Batch loss:  0.3057619035243988\n",
      "Batch loss:  0.4476032853126526\n",
      "Batch loss:  0.2996622920036316\n",
      "Batch loss:  0.18953347206115723\n",
      "Batch loss:  0.31552767753601074\n",
      "Batch loss:  0.3171943426132202\n",
      "Batch loss:  0.23269301652908325\n",
      "Batch loss:  0.44150349497795105\n",
      "Batch loss:  0.2682865262031555\n",
      "Batch loss:  0.22509674727916718\n",
      "Batch loss:  0.16724789142608643\n",
      "Batch loss:  0.32386514544487\n",
      "Batch loss:  0.18705905973911285\n",
      "Batch loss:  0.23077918589115143\n",
      "Batch loss:  0.331698477268219\n",
      "Batch loss:  0.3523925840854645\n",
      "Batch loss:  0.23889921605587006\n",
      "Batch loss:  0.3112238049507141\n",
      "Batch loss:  0.23188883066177368\n",
      "Batch loss:  0.3009835481643677\n",
      "Batch loss:  0.46080729365348816\n",
      "Batch loss:  0.3716357350349426\n",
      "Batch loss:  0.4000866711139679\n",
      "Batch loss:  0.26790717244148254\n",
      "Batch loss:  0.26183831691741943\n",
      "Batch loss:  0.23459206521511078\n",
      "Batch loss:  0.30453118681907654\n",
      "Batch loss:  0.146746426820755\n",
      "Batch loss:  0.29451310634613037\n",
      "Current average loss: 0.5438566517994686\n",
      "Batch loss:  0.46150651574134827\n",
      "Batch loss:  0.27884432673454285\n",
      "Batch loss:  0.32008400559425354\n",
      "Batch loss:  0.15213336050510406\n",
      "Batch loss:  0.20048099756240845\n",
      "Batch loss:  0.24375669658184052\n",
      "Batch loss:  0.42725592851638794\n",
      "Batch loss:  0.3253598213195801\n",
      "Batch loss:  0.281218945980072\n",
      "Batch loss:  0.3791307508945465\n",
      "Batch loss:  0.2987297475337982\n",
      "Batch loss:  0.1823464035987854\n",
      "Batch loss:  0.2922571897506714\n",
      "Batch loss:  0.23834237456321716\n",
      "Batch loss:  0.23635603487491608\n",
      "Batch loss:  0.4912019968032837\n",
      "Batch loss:  0.26210150122642517\n",
      "Batch loss:  0.30588024854660034\n",
      "Batch loss:  0.24975654482841492\n",
      "Batch loss:  0.34808459877967834\n",
      "Batch loss:  0.31367287039756775\n",
      "Batch loss:  0.5035187602043152\n",
      "Batch loss:  0.23471951484680176\n",
      "Batch loss:  0.20857425034046173\n",
      "Batch loss:  0.4442655146121979\n",
      "Batch loss:  0.1991390883922577\n",
      "Batch loss:  0.2261337786912918\n",
      "Batch loss:  0.22376768290996552\n",
      "Batch loss:  0.6361373066902161\n",
      "Batch loss:  0.18750159442424774\n",
      "Batch loss:  0.2628442943096161\n",
      "Batch loss:  0.24166257679462433\n",
      "Batch loss:  0.41577547788619995\n",
      "Batch loss:  0.2681797444820404\n",
      "Batch loss:  0.34646472334861755\n",
      "Batch loss:  0.3822740912437439\n",
      "Batch loss:  0.22719787061214447\n",
      "Batch loss:  0.21396887302398682\n",
      "Batch loss:  0.24522137641906738\n",
      "Batch loss:  0.5206723809242249\n",
      "Batch loss:  0.276753693819046\n",
      "Batch loss:  0.17436638474464417\n",
      "Batch loss:  0.48195987939834595\n",
      "Batch loss:  0.20275907218456268\n",
      "Batch loss:  0.2923418879508972\n",
      "Batch loss:  0.18343226611614227\n",
      "Batch loss:  0.37796372175216675\n",
      "Batch loss:  0.35437098145484924\n",
      "Batch loss:  0.21014980971813202\n",
      "Batch loss:  0.4316330552101135\n",
      "Batch loss:  0.26809653639793396\n",
      "Batch loss:  0.2833978533744812\n",
      "Batch loss:  0.24664457142353058\n",
      "Batch loss:  0.6686480045318604\n",
      "Batch loss:  0.17276126146316528\n",
      "Batch loss:  0.41950374841690063\n",
      "Batch loss:  0.27041226625442505\n",
      "Batch loss:  0.39809650182724\n",
      "Batch loss:  0.17884185910224915\n",
      "Batch loss:  0.2879765033721924\n",
      "Batch loss:  0.22979865968227386\n",
      "Batch loss:  0.31488093733787537\n",
      "Batch loss:  0.3613838255405426\n",
      "Batch loss:  0.2839088439941406\n",
      "Batch loss:  0.3565707802772522\n",
      "Batch loss:  0.17970523238182068\n",
      "Batch loss:  0.6989878416061401\n",
      "Batch loss:  0.290606826543808\n",
      "Batch loss:  0.3713637590408325\n",
      "Batch loss:  0.24170207977294922\n",
      "Batch loss:  0.163395494222641\n",
      "Batch loss:  0.2314697951078415\n",
      "Batch loss:  0.3103554844856262\n",
      "Batch loss:  0.403642475605011\n",
      "Batch loss:  0.39716920256614685\n",
      "Batch loss:  0.21000252664089203\n",
      "Batch loss:  0.26147082448005676\n",
      "Batch loss:  0.4795824885368347\n",
      "Batch loss:  0.268566370010376\n",
      "Batch loss:  0.1973986029624939\n",
      "Batch loss:  0.22108811140060425\n",
      "Batch loss:  0.3299936354160309\n",
      "Batch loss:  0.31627559661865234\n",
      "Batch loss:  0.24005138874053955\n",
      "Batch loss:  0.2859107553958893\n",
      "Batch loss:  0.1845260113477707\n",
      "Batch loss:  0.3643718659877777\n",
      "Batch loss:  0.27022504806518555\n",
      "Batch loss:  0.2288704514503479\n",
      "Batch loss:  0.24980586767196655\n",
      "Batch loss:  0.554162859916687\n",
      "Batch loss:  0.47244927287101746\n",
      "Batch loss:  0.3212237060070038\n",
      "Batch loss:  0.5503315925598145\n",
      "Batch loss:  0.23684276640415192\n",
      "Batch loss:  0.2978411912918091\n",
      "Batch loss:  0.19388888776302338\n",
      "Batch loss:  0.3722894787788391\n",
      "Batch loss:  0.2420935034751892\n",
      "Batch loss:  0.32378727197647095\n",
      "Current average loss: 0.5391733796045056\n",
      "Batch loss:  0.3065018057823181\n",
      "Batch loss:  0.5077229142189026\n",
      "Batch loss:  0.23398526012897491\n",
      "Batch loss:  0.2848875820636749\n",
      "Batch loss:  0.32906466722488403\n",
      "Batch loss:  0.250529408454895\n",
      "Batch loss:  0.24467840790748596\n",
      "Batch loss:  0.34488821029663086\n",
      "Batch loss:  0.30090612173080444\n",
      "Batch loss:  0.22170360386371613\n",
      "Batch loss:  0.6438296437263489\n",
      "Batch loss:  0.32822033762931824\n",
      "Batch loss:  0.2491685003042221\n",
      "Batch loss:  0.3063962757587433\n",
      "Batch loss:  0.3064876198768616\n",
      "Batch loss:  0.321628212928772\n",
      "Batch loss:  0.2194378525018692\n",
      "Batch loss:  0.1692304015159607\n",
      "Batch loss:  0.3523457944393158\n",
      "Batch loss:  0.2523958086967468\n",
      "Batch loss:  0.25674596428871155\n",
      "Batch loss:  0.27401962876319885\n",
      "Batch loss:  0.22072501480579376\n",
      "Batch loss:  0.41467663645744324\n",
      "Batch loss:  0.1440817415714264\n",
      "Batch loss:  0.1992725282907486\n",
      "Batch loss:  0.4077322483062744\n",
      "Batch loss:  0.35687950253486633\n",
      "Batch loss:  0.3076604902744293\n",
      "Batch loss:  0.2076241374015808\n",
      "Batch loss:  0.4609215259552002\n",
      "Batch loss:  0.24939276278018951\n",
      "Batch loss:  0.4351907968521118\n",
      "Batch loss:  0.3676266372203827\n",
      "Batch loss:  0.3198432922363281\n",
      "Batch loss:  0.3367130756378174\n",
      "Batch loss:  0.25547122955322266\n",
      "Batch loss:  0.27780070900917053\n",
      "Batch loss:  0.49796509742736816\n",
      "Batch loss:  0.1887320727109909\n",
      "Batch loss:  0.17514784634113312\n",
      "Batch loss:  0.322510689496994\n",
      "Batch loss:  0.46729907393455505\n",
      "Batch loss:  0.27593299746513367\n",
      "Batch loss:  0.2800966799259186\n",
      "Batch loss:  0.2112770825624466\n",
      "Batch loss:  0.21405838429927826\n",
      "Batch loss:  0.22979488968849182\n",
      "Batch loss:  0.2196803092956543\n",
      "Batch loss:  0.3307257294654846\n",
      "Batch loss:  0.13696464896202087\n",
      "Batch loss:  0.49486419558525085\n",
      "Batch loss:  0.3190525472164154\n",
      "Batch loss:  0.35428768396377563\n",
      "Batch loss:  0.23006601631641388\n",
      "Batch loss:  0.36115843057632446\n",
      "Batch loss:  0.25152358412742615\n",
      "Batch loss:  0.3030673563480377\n",
      "Batch loss:  0.23456375300884247\n",
      "Batch loss:  0.3031812906265259\n",
      "Batch loss:  0.22647614777088165\n",
      "Batch loss:  0.27181488275527954\n",
      "Batch loss:  0.404071569442749\n",
      "Batch loss:  0.21752430498600006\n",
      "Batch loss:  0.31441861391067505\n",
      "Batch loss:  0.28637194633483887\n",
      "Batch loss:  0.2875171899795532\n",
      "Batch loss:  0.33406731486320496\n",
      "Batch loss:  0.23070257902145386\n",
      "Batch loss:  0.27244991064071655\n",
      "Batch loss:  0.3424237072467804\n",
      "Batch loss:  0.3353337347507477\n",
      "Batch loss:  0.14847350120544434\n",
      "Batch loss:  0.33200910687446594\n",
      "Batch loss:  0.31797605752944946\n",
      "Batch loss:  0.2599387764930725\n",
      "Batch loss:  0.21588119864463806\n",
      "Batch loss:  0.18499860167503357\n",
      "Batch loss:  0.5227208137512207\n",
      "Batch loss:  0.3805130124092102\n",
      "Batch loss:  0.3518829941749573\n",
      "Batch loss:  0.5036771297454834\n",
      "Batch loss:  0.2671937644481659\n",
      "Batch loss:  0.20587390661239624\n",
      "Batch loss:  0.5354427099227905\n",
      "Batch loss:  0.3516433537006378\n",
      "Batch loss:  0.3978056311607361\n",
      "Batch loss:  0.41660723090171814\n",
      "Batch loss:  0.1891140341758728\n",
      "Batch loss:  0.2580813765525818\n",
      "Batch loss:  0.4208894670009613\n",
      "Batch loss:  0.32922667264938354\n",
      "Batch loss:  0.2570685148239136\n",
      "Batch loss:  0.1570754200220108\n",
      "Batch loss:  0.6147214770317078\n",
      "Batch loss:  0.36802956461906433\n",
      "Batch loss:  0.34809577465057373\n",
      "Batch loss:  0.29263854026794434\n",
      "Batch loss:  0.3028167486190796\n",
      "Batch loss:  0.2715831398963928\n",
      "Current average loss: 0.5346401800693449\n",
      "Batch loss:  0.35973846912384033\n",
      "Batch loss:  0.37237632274627686\n",
      "Batch loss:  0.34974777698516846\n",
      "Batch loss:  0.5205444693565369\n",
      "Batch loss:  0.2594371736049652\n",
      "Batch loss:  0.14460672438144684\n",
      "Batch loss:  0.21852067112922668\n",
      "Batch loss:  0.2022656500339508\n",
      "Batch loss:  0.40910211205482483\n",
      "Batch loss:  0.3180559277534485\n",
      "Batch loss:  0.2533835172653198\n",
      "Batch loss:  0.4294043779373169\n",
      "Batch loss:  0.3412391245365143\n",
      "Batch loss:  0.23821449279785156\n",
      "Batch loss:  0.3481023907661438\n",
      "Batch loss:  0.1801164299249649\n",
      "Batch loss:  0.32166552543640137\n",
      "Batch loss:  0.23899249732494354\n",
      "Batch loss:  0.3138848543167114\n",
      "Batch loss:  0.24528710544109344\n",
      "Batch loss:  0.40943071246147156\n",
      "Batch loss:  0.2816668152809143\n",
      "Batch loss:  0.26987424492836\n",
      "Batch loss:  0.28837668895721436\n",
      "Batch loss:  0.38415125012397766\n",
      "Batch loss:  0.3349774479866028\n",
      "Batch loss:  0.22361452877521515\n",
      "Batch loss:  0.2807612717151642\n",
      "Batch loss:  0.47381824254989624\n",
      "Batch loss:  0.29382383823394775\n",
      "Batch loss:  0.28708672523498535\n",
      "Batch loss:  0.2833254337310791\n",
      "Batch loss:  0.2602274417877197\n",
      "Batch loss:  0.18010899424552917\n",
      "Batch loss:  0.27943912148475647\n",
      "Batch loss:  0.2415471225976944\n",
      "Batch loss:  0.1573980748653412\n",
      "Batch loss:  0.27282828092575073\n",
      "Batch loss:  0.20355182886123657\n",
      "Batch loss:  0.39277729392051697\n",
      "Batch loss:  0.2735251784324646\n",
      "Batch loss:  0.3755577504634857\n",
      "Batch loss:  0.2892957925796509\n",
      "Batch loss:  0.40311235189437866\n",
      "Batch loss:  0.31604471802711487\n",
      "Batch loss:  0.2797676920890808\n",
      "Batch loss:  0.49510809779167175\n",
      "Batch loss:  0.23822535574436188\n",
      "Batch loss:  0.3125911355018616\n",
      "Batch loss:  0.29377931356430054\n",
      "Batch loss:  0.2615390419960022\n",
      "Batch loss:  0.29899248480796814\n",
      "Batch loss:  0.2514086663722992\n",
      "Batch loss:  0.2174564003944397\n",
      "Batch loss:  0.36734476685523987\n",
      "Batch loss:  0.23659564554691315\n",
      "Batch loss:  0.48004722595214844\n",
      "Batch loss:  0.18297435343265533\n",
      "Batch loss:  0.23256227374076843\n",
      "Batch loss:  0.3489243686199188\n",
      "Batch loss:  0.2221161276102066\n",
      "Batch loss:  0.3277570903301239\n",
      "Batch loss:  0.23051325976848602\n",
      "Batch loss:  0.3322148323059082\n",
      "Batch loss:  0.3453046381473541\n",
      "Batch loss:  0.1872272789478302\n",
      "Batch loss:  0.20964418351650238\n",
      "Batch loss:  0.4554835557937622\n",
      "Batch loss:  0.2996988892555237\n",
      "Batch loss:  0.31799477338790894\n",
      "Batch loss:  0.492768257856369\n",
      "Batch loss:  0.27445024251937866\n",
      "Batch loss:  0.32199639081954956\n",
      "Batch loss:  0.2868538796901703\n",
      "Batch loss:  0.33903419971466064\n",
      "Batch loss:  0.3316640853881836\n",
      "Batch loss:  0.19766253232955933\n",
      "Batch loss:  0.3526240289211273\n",
      "Batch loss:  0.22464235126972198\n",
      "Batch loss:  0.5429714322090149\n",
      "Batch loss:  0.4195719361305237\n",
      "Batch loss:  0.24532122910022736\n",
      "Batch loss:  0.3486657738685608\n",
      "Batch loss:  0.32600000500679016\n",
      "Batch loss:  0.23353344202041626\n",
      "Batch loss:  0.41324275732040405\n",
      "Batch loss:  0.2658575177192688\n",
      "Batch loss:  0.2589620351791382\n",
      "Batch loss:  0.13897010684013367\n",
      "Batch loss:  0.3958987295627594\n",
      "Batch loss:  0.24682986736297607\n",
      "Batch loss:  0.4180673658847809\n",
      "Batch loss:  0.6059291362762451\n",
      "Batch loss:  0.5039060711860657\n",
      "Batch loss:  0.2999373972415924\n",
      "Batch loss:  0.3162093460559845\n",
      "Batch loss:  0.2635241448879242\n",
      "Batch loss:  0.26614731550216675\n",
      "Batch loss:  0.2730146646499634\n",
      "Batch loss:  0.2947635054588318\n",
      "Current average loss: 0.5302912625449251\n",
      "Batch loss:  0.27800408005714417\n",
      "Batch loss:  0.3179576098918915\n",
      "Batch loss:  0.18824267387390137\n",
      "Batch loss:  0.3196665346622467\n",
      "Batch loss:  0.3373127579689026\n",
      "Batch loss:  0.22767101228237152\n",
      "Batch loss:  0.232190802693367\n",
      "Batch loss:  0.24772556126117706\n",
      "Batch loss:  0.20974507927894592\n",
      "Batch loss:  0.37031492590904236\n",
      "Batch loss:  0.16882072389125824\n",
      "Batch loss:  0.350279837846756\n",
      "Batch loss:  0.407662957906723\n",
      "Batch loss:  0.1855258345603943\n",
      "Batch loss:  0.35848304629325867\n",
      "Batch loss:  0.3353111743927002\n",
      "Batch loss:  0.2800148129463196\n",
      "Batch loss:  0.1604265421628952\n",
      "Batch loss:  0.26686835289001465\n",
      "Batch loss:  0.20606759190559387\n",
      "Batch loss:  0.3383660316467285\n",
      "Batch loss:  0.6477962136268616\n",
      "Batch loss:  0.24420048296451569\n",
      "Batch loss:  0.3922004997730255\n",
      "Batch loss:  0.23358578979969025\n",
      "Batch loss:  0.31271347403526306\n",
      "Batch loss:  0.21244697272777557\n",
      "Batch loss:  0.22982703149318695\n",
      "Batch loss:  0.25749313831329346\n",
      "Batch loss:  0.33327344059944153\n",
      "Batch loss:  0.24331063032150269\n",
      "Batch loss:  0.30444303154945374\n",
      "Batch loss:  0.27873045206069946\n",
      "Batch loss:  0.2831957936286926\n",
      "Batch loss:  0.2130364328622818\n",
      "Batch loss:  0.3098390996456146\n",
      "Batch loss:  0.6159655451774597\n",
      "Batch loss:  0.31591272354125977\n",
      "Batch loss:  0.19581648707389832\n",
      "Batch loss:  0.40770789980888367\n",
      "Batch loss:  0.2855341136455536\n",
      "Batch loss:  0.21554185450077057\n",
      "Batch loss:  0.36766761541366577\n",
      "Batch loss:  0.177429661154747\n",
      "Batch loss:  0.424312561750412\n",
      "Batch loss:  0.26275932788848877\n",
      "Batch loss:  0.32127514481544495\n",
      "Batch loss:  0.17884939908981323\n",
      "Batch loss:  0.3620128929615021\n",
      "Batch loss:  0.35631969571113586\n",
      "Batch loss:  0.16137810051441193\n",
      "Batch loss:  0.1941734105348587\n",
      "Batch loss:  0.2961869239807129\n",
      "Batch loss:  0.45430949330329895\n",
      "Batch loss:  0.24085359275341034\n",
      "Batch loss:  0.2336413860321045\n",
      "Batch loss:  0.3148213326931\n",
      "Batch loss:  0.2474672645330429\n",
      "Batch loss:  0.3519686460494995\n",
      "Batch loss:  0.42831432819366455\n",
      "Batch loss:  0.3265558183193207\n",
      "Batch loss:  0.38530996441841125\n",
      "Batch loss:  0.29386645555496216\n",
      "Batch loss:  0.2606440782546997\n",
      "Batch loss:  0.40005144476890564\n",
      "Batch loss:  0.28747686743736267\n",
      "Batch loss:  0.3795471787452698\n",
      "Batch loss:  0.25774112343788147\n",
      "Batch loss:  0.3566800653934479\n",
      "Batch loss:  0.28987008333206177\n",
      "Batch loss:  0.2459087073802948\n",
      "Batch loss:  0.3708486557006836\n",
      "Batch loss:  0.40192511677742004\n",
      "Batch loss:  0.5721420049667358\n",
      "Batch loss:  0.380281925201416\n",
      "Batch loss:  0.3866237998008728\n",
      "Batch loss:  0.1771499365568161\n",
      "Batch loss:  0.2932566702365875\n",
      "Batch loss:  0.19803546369075775\n",
      "Batch loss:  0.3366696238517761\n",
      "Batch loss:  0.2940119206905365\n",
      "Batch loss:  0.3674499988555908\n",
      "Batch loss:  0.33237308263778687\n",
      "Batch loss:  0.38691437244415283\n",
      "Batch loss:  0.3735794126987457\n",
      "Batch loss:  0.2585866451263428\n",
      "Batch loss:  0.3782036304473877\n",
      "Batch loss:  0.5716010928153992\n",
      "Batch loss:  0.23138298094272614\n",
      "Batch loss:  0.28334832191467285\n",
      "Batch loss:  0.28178077936172485\n",
      "Batch loss:  0.3052689731121063\n",
      "Batch loss:  0.21168649196624756\n",
      "Batch loss:  0.18458184599876404\n",
      "Batch loss:  0.19371123611927032\n",
      "Batch loss:  0.24110984802246094\n",
      "Batch loss:  0.20851515233516693\n",
      "Batch loss:  0.35380473732948303\n",
      "Batch loss:  0.18685874342918396\n",
      "Batch loss:  0.3188232481479645\n",
      "Current average loss: 0.525995473273575\n",
      "Batch loss:  0.25653985142707825\n",
      "Batch loss:  0.446889191865921\n",
      "Batch loss:  0.29608848690986633\n",
      "Batch loss:  0.28726205229759216\n",
      "Batch loss:  0.2463679015636444\n",
      "Batch loss:  0.2459651231765747\n",
      "Batch loss:  0.18310293555259705\n",
      "Batch loss:  0.29794058203697205\n",
      "Batch loss:  0.18131276965141296\n",
      "Batch loss:  0.35931625962257385\n",
      "Batch loss:  0.18905967473983765\n",
      "Batch loss:  0.34711331129074097\n",
      "Batch loss:  0.33009323477745056\n",
      "Batch loss:  0.41920241713523865\n",
      "Batch loss:  0.20952264964580536\n",
      "Batch loss:  0.23218512535095215\n",
      "Batch loss:  0.20557260513305664\n",
      "Batch loss:  0.24023176729679108\n",
      "Batch loss:  0.3808567523956299\n",
      "Batch loss:  0.5275377631187439\n",
      "Batch loss:  0.31888696551322937\n",
      "Batch loss:  0.32206442952156067\n",
      "Batch loss:  0.2558700740337372\n",
      "Batch loss:  0.14640282094478607\n",
      "Batch loss:  0.31061655282974243\n",
      "Batch loss:  0.15434394776821136\n",
      "Batch loss:  0.7322865724563599\n",
      "Batch loss:  0.3409346044063568\n",
      "Batch loss:  0.3040092885494232\n",
      "Batch loss:  0.20239678025245667\n",
      "Batch loss:  0.5273699164390564\n",
      "Batch loss:  0.37205561995506287\n",
      "Batch loss:  0.2147034853696823\n",
      "Batch loss:  0.2585635781288147\n",
      "Batch loss:  0.4131111204624176\n",
      "Batch loss:  0.3087896406650543\n",
      "Batch loss:  0.21446888148784637\n",
      "Batch loss:  0.18248531222343445\n",
      "Batch loss:  0.3449791669845581\n",
      "Batch loss:  0.24357618391513824\n",
      "Batch loss:  0.2582949101924896\n",
      "Batch loss:  0.2754824161529541\n",
      "Batch loss:  0.1859341561794281\n",
      "Batch loss:  0.3725031614303589\n",
      "Batch loss:  0.35910332202911377\n",
      "Batch loss:  0.3126094937324524\n",
      "Batch loss:  0.28734666109085083\n",
      "Batch loss:  0.23591779172420502\n",
      "Batch loss:  0.271484911441803\n",
      "Batch loss:  0.4989498257637024\n",
      "Batch loss:  0.26954299211502075\n",
      "Batch loss:  0.3885476291179657\n",
      "Batch loss:  0.18266646564006805\n",
      "Batch loss:  0.3556651771068573\n",
      "Batch loss:  0.21661658585071564\n",
      "Batch loss:  0.22727122902870178\n",
      "Batch loss:  0.26019906997680664\n",
      "Batch loss:  0.3787049353122711\n",
      "Batch loss:  0.2072019875049591\n",
      "Batch loss:  0.35880810022354126\n",
      "Batch loss:  0.2577108144760132\n",
      "Batch loss:  0.18021662533283234\n",
      "Batch loss:  0.23148469626903534\n",
      "Batch loss:  0.2771325707435608\n",
      "Batch loss:  0.23099903762340546\n",
      "Batch loss:  0.3164008855819702\n",
      "Batch loss:  0.16669102013111115\n",
      "Batch loss:  0.11576958000659943\n",
      "Batch loss:  0.2807239592075348\n",
      "Batch loss:  0.19216489791870117\n",
      "Batch loss:  0.2986530065536499\n",
      "Batch loss:  0.14749813079833984\n",
      "Batch loss:  0.2084740847349167\n",
      "Batch loss:  0.22866322100162506\n",
      "Batch loss:  0.30575865507125854\n",
      "Batch loss:  0.35754460096359253\n",
      "Batch loss:  0.3012724816799164\n",
      "Batch loss:  0.34671297669410706\n",
      "Batch loss:  0.3547152876853943\n",
      "Batch loss:  0.32642388343811035\n",
      "Batch loss:  0.2940049171447754\n",
      "Batch loss:  0.2312498241662979\n",
      "Batch loss:  0.3236839473247528\n",
      "Batch loss:  0.2735697627067566\n",
      "Batch loss:  0.20574651658535004\n",
      "Batch loss:  0.27321097254753113\n",
      "Batch loss:  0.22415244579315186\n",
      "Batch loss:  0.31418561935424805\n",
      "Batch loss:  0.5630375146865845\n",
      "Batch loss:  0.14254797995090485\n",
      "Batch loss:  0.2362307906150818\n",
      "Batch loss:  0.34596988558769226\n",
      "Batch loss:  0.28690528869628906\n",
      "Batch loss:  0.4799363911151886\n",
      "Batch loss:  0.3191583752632141\n",
      "Batch loss:  0.2859472632408142\n",
      "Batch loss:  0.33760544657707214\n",
      "Batch loss:  0.5303181409835815\n",
      "Batch loss:  0.4622880220413208\n",
      "Batch loss:  0.17066457867622375\n",
      "Current average loss: 0.5216960477949822\n",
      "Batch loss:  0.419844388961792\n",
      "Batch loss:  0.39207255840301514\n",
      "Batch loss:  0.24339406192302704\n",
      "Batch loss:  0.42881113290786743\n",
      "Batch loss:  0.26031455397605896\n",
      "Batch loss:  0.3400665819644928\n",
      "Batch loss:  0.32336828112602234\n",
      "Batch loss:  0.2856454849243164\n",
      "Batch loss:  0.45324206352233887\n",
      "Batch loss:  0.2761742174625397\n",
      "Batch loss:  0.43188798427581787\n",
      "Batch loss:  0.1915636658668518\n",
      "Batch loss:  0.37300345301628113\n",
      "Batch loss:  0.32037782669067383\n",
      "Batch loss:  0.23453670740127563\n",
      "Batch loss:  0.2463165819644928\n",
      "Batch loss:  0.23699046671390533\n",
      "Batch loss:  0.19955086708068848\n",
      "Batch loss:  0.5603544116020203\n",
      "Batch loss:  0.2663373351097107\n",
      "Batch loss:  0.28384798765182495\n",
      "Batch loss:  0.3183928430080414\n",
      "Batch loss:  0.3271956145763397\n",
      "Batch loss:  0.288983553647995\n",
      "Batch loss:  0.19475381076335907\n",
      "Batch loss:  0.27443042397499084\n",
      "Batch loss:  0.20838563144207\n",
      "Batch loss:  0.33587923645973206\n",
      "Batch loss:  0.2658713161945343\n",
      "Batch loss:  0.25345027446746826\n",
      "Batch loss:  0.2870672643184662\n",
      "Batch loss:  0.2317947894334793\n",
      "Batch loss:  0.22660277783870697\n",
      "Batch loss:  0.5143580436706543\n",
      "Batch loss:  0.1752314418554306\n",
      "Batch loss:  0.2903948724269867\n",
      "Batch loss:  0.4447539746761322\n",
      "Batch loss:  0.2164030373096466\n",
      "Batch loss:  0.3191245198249817\n",
      "Batch loss:  0.3313053250312805\n",
      "Batch loss:  0.4988766014575958\n",
      "Batch loss:  0.5906009674072266\n",
      "Batch loss:  0.18822826445102692\n",
      "Batch loss:  0.29310932755470276\n",
      "Batch loss:  0.2770552933216095\n",
      "Batch loss:  0.3068266808986664\n",
      "Batch loss:  0.2659585475921631\n",
      "Batch loss:  0.29338404536247253\n",
      "Batch loss:  0.4976566433906555\n",
      "Batch loss:  0.32993292808532715\n",
      "Batch loss:  0.36311838030815125\n",
      "Batch loss:  0.2229727953672409\n",
      "Batch loss:  0.3037112355232239\n",
      "Batch loss:  0.38869720697402954\n",
      "Batch loss:  0.33418697118759155\n",
      "Batch loss:  0.22894741594791412\n",
      "Batch loss:  0.3063674569129944\n",
      "Batch loss:  0.34467190504074097\n",
      "Batch loss:  0.2181573212146759\n",
      "Batch loss:  0.29827162623405457\n",
      "Batch loss:  0.17604167759418488\n",
      "Batch loss:  0.3102204203605652\n",
      "Batch loss:  0.24128271639347076\n",
      "Batch loss:  0.14835494756698608\n",
      "Batch loss:  0.31210988759994507\n",
      "Batch loss:  0.27303051948547363\n",
      "Batch loss:  0.24415737390518188\n",
      "Batch loss:  0.35756510496139526\n",
      "Batch loss:  0.37193816900253296\n",
      "Batch loss:  0.2192961573600769\n",
      "Batch loss:  0.16793008148670197\n",
      "Batch loss:  0.2498871237039566\n",
      "Batch loss:  0.30339497327804565\n",
      "Batch loss:  0.36977219581604004\n",
      "Batch loss:  0.33570632338523865\n",
      "Batch loss:  0.2657097280025482\n",
      "Batch loss:  0.24238468706607819\n",
      "Batch loss:  0.28253671526908875\n",
      "Batch loss:  0.3176909387111664\n",
      "Batch loss:  0.2642576992511749\n",
      "Batch loss:  0.2253437489271164\n",
      "Batch loss:  0.2820334732532501\n",
      "Batch loss:  0.25342857837677\n",
      "Batch loss:  0.2550065815448761\n",
      "Batch loss:  0.295684278011322\n",
      "Batch loss:  0.3425268232822418\n",
      "Batch loss:  0.4752521514892578\n",
      "Batch loss:  0.2690301537513733\n",
      "Batch loss:  0.2610698342323303\n",
      "Batch loss:  0.3381350040435791\n",
      "Batch loss:  0.16140218079090118\n",
      "Batch loss:  0.37837669253349304\n",
      "Batch loss:  0.2699865400791168\n",
      "Batch loss:  0.2204064428806305\n",
      "Batch loss:  0.27189186215400696\n",
      "Batch loss:  0.28682664036750793\n",
      "Batch loss:  0.3420548737049103\n",
      "Batch loss:  0.14356476068496704\n",
      "Batch loss:  0.28456124663352966\n",
      "Batch loss:  0.2713789939880371\n",
      "Current average loss: 0.5176484990764089\n",
      "Batch loss:  0.27155888080596924\n",
      "Batch loss:  0.26191776990890503\n",
      "Batch loss:  0.4644924998283386\n",
      "Batch loss:  0.4360625147819519\n",
      "Batch loss:  0.2463337928056717\n",
      "Batch loss:  0.24574272334575653\n",
      "Batch loss:  0.2975414991378784\n",
      "Batch loss:  0.4240425229072571\n",
      "Batch loss:  0.22558477520942688\n",
      "Batch loss:  0.22760573029518127\n",
      "Batch loss:  0.34306108951568604\n",
      "Batch loss:  0.3128235638141632\n",
      "Batch loss:  0.17299902439117432\n",
      "Batch loss:  0.12287452071905136\n",
      "Batch loss:  0.2512371242046356\n",
      "Batch loss:  0.23843736946582794\n",
      "Batch loss:  0.24913877248764038\n",
      "Batch loss:  0.23049834370613098\n",
      "Batch loss:  0.33808690309524536\n",
      "Batch loss:  0.32441478967666626\n",
      "Batch loss:  0.36441031098365784\n",
      "Batch loss:  0.3564191162586212\n",
      "Batch loss:  0.22953741252422333\n",
      "Batch loss:  0.36092013120651245\n",
      "Batch loss:  0.11332588642835617\n",
      "Batch loss:  0.21522514522075653\n",
      "Batch loss:  0.37434762716293335\n",
      "Batch loss:  0.5871521830558777\n",
      "Batch loss:  0.35638242959976196\n",
      "Batch loss:  0.33778005838394165\n",
      "Batch loss:  0.1650839000940323\n",
      "Batch loss:  0.14893728494644165\n",
      "Batch loss:  0.3247627317905426\n",
      "Batch loss:  0.22938354313373566\n",
      "Batch loss:  0.34529682993888855\n",
      "Batch loss:  0.21392473578453064\n",
      "Batch loss:  0.3988281786441803\n",
      "Batch loss:  0.34257614612579346\n",
      "Batch loss:  0.5976858735084534\n",
      "Batch loss:  0.3243196904659271\n",
      "Batch loss:  0.3046283721923828\n",
      "Batch loss:  0.33180588483810425\n",
      "Batch loss:  0.2838760316371918\n",
      "Batch loss:  0.2966662347316742\n",
      "Batch loss:  0.6747654676437378\n",
      "Batch loss:  0.314443975687027\n",
      "Batch loss:  0.25987377762794495\n",
      "Batch loss:  0.3294503390789032\n",
      "Batch loss:  0.26935693621635437\n",
      "Batch loss:  0.42822542786598206\n",
      "Batch loss:  0.4725716710090637\n",
      "Batch loss:  0.457097589969635\n",
      "Batch loss:  0.3051351308822632\n",
      "Batch loss:  0.18477843701839447\n",
      "Batch loss:  0.17652840912342072\n",
      "Batch loss:  0.24637536704540253\n",
      "Batch loss:  0.3694247305393219\n",
      "Batch loss:  0.22179213166236877\n",
      "Batch loss:  0.22594904899597168\n",
      "Batch loss:  0.5485908389091492\n",
      "Batch loss:  0.44266703724861145\n",
      "Batch loss:  0.24065330624580383\n",
      "Batch loss:  0.2236952781677246\n",
      "Batch loss:  0.33595749735832214\n",
      "Batch loss:  0.3564740717411041\n",
      "Batch loss:  0.3003397583961487\n",
      "Batch loss:  0.2988845705986023\n",
      "Batch loss:  0.2754846513271332\n",
      "Batch loss:  0.2966451346874237\n",
      "Batch loss:  0.31531187891960144\n",
      "Batch loss:  0.26550114154815674\n",
      "Batch loss:  0.298162043094635\n",
      "Batch loss:  0.2515757083892822\n",
      "Batch loss:  0.20920506119728088\n",
      "Batch loss:  0.2373223453760147\n",
      "Batch loss:  0.3806130588054657\n",
      "Batch loss:  0.23068684339523315\n",
      "Batch loss:  0.21863095462322235\n",
      "Batch loss:  0.38724130392074585\n",
      "Batch loss:  0.21056413650512695\n",
      "Batch loss:  0.219125434756279\n",
      "Batch loss:  0.38551971316337585\n",
      "Batch loss:  0.21200354397296906\n",
      "Batch loss:  0.2742760479450226\n",
      "Batch loss:  0.3529512584209442\n",
      "Batch loss:  0.3819746971130371\n",
      "Batch loss:  0.47756731510162354\n",
      "Batch loss:  0.24694469571113586\n",
      "Batch loss:  0.29810550808906555\n",
      "Batch loss:  0.15559940040111542\n",
      "Batch loss:  0.5586351752281189\n",
      "Batch loss:  0.22005629539489746\n",
      "Batch loss:  0.27183929085731506\n",
      "Batch loss:  0.13638286292552948\n",
      "Batch loss:  0.3225930333137512\n",
      "Batch loss:  0.20132310688495636\n",
      "Batch loss:  0.23895640671253204\n",
      "Batch loss:  0.3572307825088501\n",
      "Batch loss:  0.2814379632472992\n",
      "Batch loss:  0.23312726616859436\n",
      "Current average loss: 0.5138292712420678\n",
      "Batch loss:  0.2152172476053238\n",
      "Batch loss:  0.3250276446342468\n",
      "Batch loss:  0.3420872092247009\n",
      "Batch loss:  0.22430302202701569\n",
      "Batch loss:  0.23701317608356476\n",
      "Batch loss:  0.19984444975852966\n",
      "Batch loss:  0.23548300564289093\n",
      "Batch loss:  0.2475554645061493\n",
      "Batch loss:  0.2578134834766388\n",
      "Batch loss:  0.26083090901374817\n",
      "Batch loss:  0.48788711428642273\n",
      "Batch loss:  0.24958252906799316\n",
      "Batch loss:  0.21086564660072327\n",
      "Batch loss:  0.265074223279953\n",
      "Batch loss:  0.4321216940879822\n",
      "Batch loss:  0.33335965871810913\n",
      "Batch loss:  0.34855321049690247\n",
      "Batch loss:  0.20193468034267426\n",
      "Batch loss:  0.2918674349784851\n",
      "Batch loss:  0.2150561362504959\n",
      "Batch loss:  0.4089057147502899\n",
      "Batch loss:  0.2704658508300781\n",
      "Batch loss:  0.2727358937263489\n",
      "Batch loss:  0.386871337890625\n",
      "Batch loss:  0.23005734384059906\n",
      "Batch loss:  0.6005204319953918\n",
      "Batch loss:  0.2490660399198532\n",
      "Batch loss:  0.2127128690481186\n",
      "Batch loss:  0.2681546211242676\n",
      "Batch loss:  0.21438907086849213\n",
      "Batch loss:  0.19609111547470093\n",
      "Batch loss:  0.1719198375940323\n",
      "Batch loss:  0.43367257714271545\n",
      "Batch loss:  0.18255585432052612\n",
      "Batch loss:  0.355511337518692\n",
      "Batch loss:  0.46870020031929016\n",
      "Batch loss:  0.2040766179561615\n",
      "Batch loss:  0.23831477761268616\n",
      "Batch loss:  0.2086779773235321\n",
      "Batch loss:  0.22942247986793518\n",
      "Batch loss:  0.3624682128429413\n",
      "Batch loss:  0.2879016697406769\n",
      "Batch loss:  0.1641990691423416\n",
      "Batch loss:  0.2613024115562439\n",
      "Batch loss:  0.12567132711410522\n",
      "Batch loss:  0.44454267621040344\n",
      "Batch loss:  0.2673092782497406\n",
      "Batch loss:  0.49554967880249023\n",
      "Batch loss:  0.23701001703739166\n",
      "Batch loss:  0.33190658688545227\n",
      "Batch loss:  0.4224264919757843\n",
      "Batch loss:  0.3714831471443176\n",
      "Batch loss:  0.3340616822242737\n",
      "Batch loss:  0.1874552220106125\n",
      "Batch loss:  0.41836017370224\n",
      "Batch loss:  0.4031095504760742\n",
      "Batch loss:  0.27504318952560425\n",
      "Batch loss:  0.3044118583202362\n",
      "Batch loss:  0.36299118399620056\n",
      "Batch loss:  0.29806163907051086\n",
      "Batch loss:  0.41180914640426636\n",
      "Batch loss:  0.4394223690032959\n",
      "Batch loss:  0.18845073878765106\n",
      "Batch loss:  0.26491594314575195\n",
      "Batch loss:  0.22167927026748657\n",
      "Batch loss:  0.23145939409732819\n",
      "Batch loss:  0.1379980444908142\n",
      "Batch loss:  0.2715253233909607\n",
      "Batch loss:  0.17549259960651398\n",
      "Batch loss:  0.7833683490753174\n",
      "Batch loss:  0.34513795375823975\n",
      "Batch loss:  0.344963401556015\n",
      "Batch loss:  0.5010406970977783\n",
      "Batch loss:  0.42153069376945496\n",
      "Batch loss:  0.29062411189079285\n",
      "Batch loss:  0.732869029045105\n",
      "Batch loss:  0.2952176332473755\n",
      "Batch loss:  0.4514025151729584\n",
      "Batch loss:  0.2239389419555664\n",
      "Batch loss:  0.16980142891407013\n",
      "Batch loss:  0.32102978229522705\n",
      "Batch loss:  0.1241641417145729\n",
      "Batch loss:  0.2700435519218445\n",
      "Batch loss:  0.266924649477005\n",
      "Batch loss:  0.23921503126621246\n",
      "Batch loss:  0.39701327681541443\n",
      "Batch loss:  0.3116743266582489\n",
      "Batch loss:  0.2107914686203003\n",
      "Batch loss:  0.38537168502807617\n",
      "Batch loss:  0.28325942158699036\n",
      "Batch loss:  0.341406911611557\n",
      "Batch loss:  0.40878114104270935\n",
      "Batch loss:  0.5262380242347717\n",
      "Batch loss:  0.24897606670856476\n",
      "Batch loss:  0.16144444048404694\n",
      "Batch loss:  0.3434320092201233\n",
      "Batch loss:  0.29088351130485535\n",
      "Batch loss:  0.19997519254684448\n",
      "Batch loss:  0.323148638010025\n",
      "Batch loss:  0.2988118529319763\n",
      "Current average loss: 0.5101828707051766\n",
      "Batch loss:  0.44333869218826294\n",
      "Batch loss:  0.26873427629470825\n",
      "Batch loss:  0.4108647108078003\n",
      "Batch loss:  0.36833998560905457\n",
      "Batch loss:  0.244534432888031\n",
      "Batch loss:  0.35200753808021545\n",
      "Batch loss:  0.2832649350166321\n",
      "Batch loss:  0.3439862132072449\n",
      "Batch loss:  0.24510711431503296\n",
      "Batch loss:  0.18404169380664825\n",
      "Batch loss:  0.28178566694259644\n",
      "Batch loss:  0.3018650412559509\n",
      "Batch loss:  0.21479946374893188\n",
      "Batch loss:  0.42954593896865845\n",
      "Batch loss:  0.29702526330947876\n",
      "Batch loss:  0.25533121824264526\n",
      "Batch loss:  0.24566812813282013\n",
      "Batch loss:  0.22668898105621338\n",
      "Batch loss:  0.21480008959770203\n",
      "Batch loss:  0.23028495907783508\n",
      "Batch loss:  0.3156674802303314\n",
      "Batch loss:  0.3474472463130951\n",
      "Batch loss:  0.3212389349937439\n",
      "Batch loss:  0.31322118639945984\n",
      "Batch loss:  0.4883202016353607\n",
      "Batch loss:  0.5257168412208557\n",
      "Batch loss:  0.5003836750984192\n",
      "Batch loss:  0.3901209235191345\n",
      "Batch loss:  0.2104339301586151\n",
      "Batch loss:  0.5429140329360962\n",
      "Batch loss:  0.38087210059165955\n",
      "Batch loss:  0.2958764433860779\n",
      "Batch loss:  0.3270778954029083\n",
      "Batch loss:  0.18906870484352112\n",
      "Batch loss:  0.23724929988384247\n",
      "Batch loss:  0.39315065741539\n",
      "Batch loss:  0.29097363352775574\n",
      "Batch loss:  0.17587238550186157\n",
      "Batch loss:  0.19385260343551636\n",
      "Batch loss:  0.28753307461738586\n",
      "Batch loss:  0.3143518567085266\n",
      "Batch loss:  0.3386663794517517\n",
      "Batch loss:  0.3690473437309265\n",
      "Batch loss:  0.21662527322769165\n",
      "Batch loss:  0.24539567530155182\n",
      "Batch loss:  0.28691577911376953\n",
      "Batch loss:  0.24790234863758087\n",
      "Batch loss:  0.3674989342689514\n",
      "Batch loss:  0.2128007858991623\n",
      "Batch loss:  0.4921068251132965\n",
      "Batch loss:  0.2303120344877243\n",
      "Batch loss:  0.3452121615409851\n",
      "Batch loss:  0.36361411213874817\n",
      "Batch loss:  0.4229561388492584\n",
      "Batch loss:  0.24136917293071747\n",
      "Batch loss:  0.376254141330719\n",
      "Batch loss:  0.3254089951515198\n",
      "Batch loss:  0.3764911890029907\n",
      "Batch loss:  0.3408903479576111\n",
      "Batch loss:  0.17590948939323425\n",
      "Batch loss:  0.36203017830848694\n",
      "Batch loss:  0.2755747437477112\n",
      "Batch loss:  0.4265387952327728\n",
      "Batch loss:  0.2509585916996002\n",
      "Batch loss:  0.30121049284935\n",
      "Batch loss:  0.34297001361846924\n",
      "Batch loss:  0.4119223654270172\n",
      "Batch loss:  0.2837403416633606\n",
      "Batch loss:  0.2354571670293808\n",
      "Batch loss:  0.2346297651529312\n",
      "Batch loss:  0.24829666316509247\n",
      "Batch loss:  0.250734806060791\n",
      "Batch loss:  0.38773587346076965\n",
      "Batch loss:  0.19784173369407654\n",
      "Batch loss:  0.19497916102409363\n",
      "Batch loss:  0.2094302624464035\n",
      "Batch loss:  0.39340147376060486\n",
      "Batch loss:  0.3028683364391327\n",
      "Batch loss:  0.34154123067855835\n",
      "Batch loss:  0.29904091358184814\n",
      "Batch loss:  0.31805750727653503\n",
      "Batch loss:  0.3902689218521118\n",
      "Batch loss:  0.555352509021759\n",
      "Batch loss:  0.3025527000427246\n",
      "Batch loss:  0.20506905019283295\n",
      "Batch loss:  0.19500280916690826\n",
      "Batch loss:  0.15371248126029968\n",
      "Batch loss:  0.20679350197315216\n",
      "Batch loss:  0.4546666443347931\n",
      "Batch loss:  0.3329946994781494\n",
      "Batch loss:  0.1431455910205841\n",
      "Batch loss:  0.2366485297679901\n",
      "Batch loss:  0.2983191907405853\n",
      "Batch loss:  0.16135871410369873\n",
      "Batch loss:  0.27567312121391296\n",
      "Batch loss:  0.2576863467693329\n",
      "Batch loss:  0.22627347707748413\n",
      "Batch loss:  0.16224882006645203\n",
      "Batch loss:  0.1957789957523346\n",
      "Batch loss:  0.21909692883491516\n",
      "Current average loss: 0.5065769494771752\n",
      "Batch loss:  0.28116318583488464\n",
      "Batch loss:  0.28701871633529663\n",
      "Batch loss:  0.3257509171962738\n",
      "Batch loss:  0.23645929992198944\n",
      "Batch loss:  0.26407837867736816\n",
      "Batch loss:  0.43461450934410095\n",
      "Batch loss:  0.4391315281391144\n",
      "Batch loss:  0.17208503186702728\n",
      "Batch loss:  0.2930441200733185\n",
      "Batch loss:  0.41861864924430847\n",
      "Batch loss:  0.2839709222316742\n",
      "Batch loss:  0.21910914778709412\n",
      "Batch loss:  0.17632679641246796\n",
      "Batch loss:  0.27138465642929077\n",
      "Batch loss:  0.542729377746582\n",
      "Batch loss:  0.4389037787914276\n",
      "Batch loss:  0.3083367347717285\n",
      "Batch loss:  0.3885665237903595\n",
      "Batch loss:  0.31352248787879944\n",
      "Batch loss:  0.3964270353317261\n",
      "Batch loss:  0.23585233092308044\n",
      "Batch loss:  0.3279203474521637\n",
      "Batch loss:  0.2922402620315552\n",
      "Batch loss:  0.26507672667503357\n",
      "Batch loss:  0.2742960751056671\n",
      "Batch loss:  0.2508557438850403\n",
      "Batch loss:  0.3366151452064514\n",
      "Batch loss:  0.18429525196552277\n",
      "Batch loss:  0.33316341042518616\n",
      "Batch loss:  0.41733765602111816\n",
      "Batch loss:  0.24931970238685608\n",
      "Batch loss:  0.19995851814746857\n",
      "Batch loss:  0.30818188190460205\n",
      "Batch loss:  0.20507574081420898\n",
      "Batch loss:  0.29787009954452515\n",
      "Batch loss:  0.27770164608955383\n",
      "Batch loss:  0.37339070439338684\n",
      "Batch loss:  0.16488130390644073\n",
      "Batch loss:  0.4729791581630707\n",
      "Batch loss:  0.26804786920547485\n",
      "Batch loss:  0.31580817699432373\n",
      "Batch loss:  0.41675618290901184\n",
      "Batch loss:  0.3511236310005188\n",
      "Batch loss:  0.10314421355724335\n",
      "Batch loss:  0.1643935889005661\n",
      "Batch loss:  0.1860513687133789\n",
      "Batch loss:  0.2830684781074524\n",
      "Batch loss:  0.3583439886569977\n",
      "Batch loss:  0.18193551898002625\n",
      "Batch loss:  0.3588350713253021\n",
      "Batch loss:  0.28238165378570557\n",
      "Batch loss:  0.14276757836341858\n",
      "Batch loss:  0.3755749762058258\n",
      "Batch loss:  0.2199244350194931\n",
      "Batch loss:  0.2795027196407318\n",
      "Batch loss:  0.5333592295646667\n",
      "Batch loss:  0.27088242769241333\n",
      "Batch loss:  0.24997492134571075\n",
      "Batch loss:  0.32723477482795715\n",
      "Batch loss:  0.5089008212089539\n",
      "Batch loss:  0.2290765643119812\n",
      "Batch loss:  0.42537322640419006\n",
      "Batch loss:  0.28039315342903137\n",
      "Batch loss:  0.29981592297554016\n",
      "Batch loss:  0.2665855586528778\n",
      "Batch loss:  0.24055077135562897\n",
      "Batch loss:  0.631118655204773\n",
      "Batch loss:  0.22089265286922455\n",
      "Batch loss:  0.22961293160915375\n",
      "Batch loss:  0.2536247968673706\n",
      "Batch loss:  0.17606782913208008\n",
      "Batch loss:  0.15983787178993225\n",
      "Batch loss:  0.2234964668750763\n",
      "Batch loss:  0.46988001465797424\n",
      "Batch loss:  0.3498567044734955\n",
      "Batch loss:  0.30292847752571106\n",
      "Batch loss:  0.15165495872497559\n",
      "Batch loss:  0.2919260859489441\n",
      "Batch loss:  0.2781420350074768\n",
      "Batch loss:  0.3663938343524933\n",
      "Batch loss:  0.17717750370502472\n",
      "Batch loss:  0.36770695447921753\n",
      "Batch loss:  0.2971518635749817\n",
      "Batch loss:  0.40216103196144104\n",
      "Batch loss:  0.37046486139297485\n",
      "Batch loss:  0.46258193254470825\n",
      "Batch loss:  0.21962444484233856\n",
      "Batch loss:  0.1893664449453354\n",
      "Batch loss:  0.19146810472011566\n",
      "Batch loss:  0.3177413046360016\n",
      "Batch loss:  0.1828470081090927\n",
      "Batch loss:  0.44747141003608704\n",
      "Batch loss:  0.18257401883602142\n",
      "Batch loss:  0.23769907653331757\n",
      "Batch loss:  0.18027697503566742\n",
      "Batch loss:  0.4740450382232666\n",
      "Batch loss:  0.30652227997779846\n",
      "Batch loss:  0.35179173946380615\n",
      "Batch loss:  0.17732731997966766\n",
      "Batch loss:  0.27851802110671997\n",
      "Current average loss: 0.5030420089632631\n",
      "Batch loss:  0.11565066128969193\n",
      "Batch loss:  0.2194468230009079\n",
      "Batch loss:  0.4057960510253906\n",
      "Batch loss:  0.1377088874578476\n",
      "Batch loss:  0.17903073132038116\n",
      "Batch loss:  0.4016290605068207\n",
      "Batch loss:  0.4057810306549072\n",
      "Batch loss:  0.26360511779785156\n",
      "Batch loss:  0.18109363317489624\n",
      "Batch loss:  0.31113725900650024\n",
      "Batch loss:  0.27496835589408875\n",
      "Batch loss:  0.3067648410797119\n",
      "Batch loss:  0.4872336685657501\n",
      "Batch loss:  0.4312797784805298\n",
      "Batch loss:  0.23889340460300446\n",
      "Batch loss:  0.3802844285964966\n",
      "Batch loss:  0.1759202629327774\n",
      "Batch loss:  0.4200681447982788\n",
      "Batch loss:  0.2737707197666168\n",
      "Batch loss:  0.20333682000637054\n",
      "Batch loss:  0.43803420662879944\n",
      "Batch loss:  0.3463926315307617\n",
      "Batch loss:  0.5200023651123047\n",
      "Batch loss:  0.23219390213489532\n",
      "Batch loss:  0.5669183731079102\n",
      "Batch loss:  0.1608104556798935\n",
      "Batch loss:  0.41438254714012146\n",
      "Batch loss:  0.2545192241668701\n",
      "Batch loss:  0.273933470249176\n",
      "Batch loss:  0.18935832381248474\n",
      "Batch loss:  0.441386878490448\n",
      "Batch loss:  0.3466075658798218\n",
      "Batch loss:  0.31162166595458984\n",
      "Batch loss:  0.35278069972991943\n",
      "Batch loss:  0.25216370820999146\n",
      "Batch loss:  0.2676845192909241\n",
      "Batch loss:  0.1697331815958023\n",
      "Batch loss:  0.3981013596057892\n",
      "Batch loss:  0.20495612919330597\n",
      "Batch loss:  0.26865074038505554\n",
      "Batch loss:  0.29469484090805054\n",
      "Batch loss:  0.20716455578804016\n",
      "Batch loss:  0.38357675075531006\n",
      "Batch loss:  0.2704169452190399\n",
      "Batch loss:  0.13788717985153198\n",
      "Batch loss:  0.1827324628829956\n",
      "Batch loss:  0.3547743558883667\n",
      "Batch loss:  0.24832460284233093\n",
      "Batch loss:  0.350862592458725\n",
      "Batch loss:  0.20118512213230133\n",
      "Batch loss:  0.1746116280555725\n",
      "Batch loss:  0.37169691920280457\n",
      "Batch loss:  0.4671032130718231\n",
      "Batch loss:  0.300045907497406\n",
      "Batch loss:  0.2952450215816498\n",
      "Batch loss:  0.20773433148860931\n",
      "Batch loss:  0.3473220467567444\n",
      "Batch loss:  0.21164843440055847\n",
      "Batch loss:  0.17390166223049164\n",
      "Batch loss:  0.12891662120819092\n",
      "Batch loss:  0.11924218386411667\n",
      "Batch loss:  0.3158565163612366\n",
      "Batch loss:  0.23044940829277039\n",
      "Batch loss:  0.36481982469558716\n",
      "Batch loss:  0.353953093290329\n",
      "Batch loss:  0.4160608947277069\n",
      "Batch loss:  0.14149972796440125\n",
      "Batch loss:  0.3971599340438843\n",
      "Batch loss:  0.29915526509284973\n",
      "Batch loss:  0.2870751917362213\n",
      "Batch loss:  0.24249063432216644\n",
      "Batch loss:  0.2963774502277374\n",
      "Batch loss:  0.21580883860588074\n",
      "Batch loss:  0.2597467005252838\n",
      "Batch loss:  0.33794060349464417\n",
      "Batch loss:  0.28017669916152954\n",
      "Batch loss:  0.21449440717697144\n",
      "Batch loss:  0.32156097888946533\n",
      "Batch loss:  0.34930312633514404\n",
      "Batch loss:  0.2744552195072174\n",
      "Batch loss:  0.26762789487838745\n",
      "Batch loss:  0.1860370934009552\n",
      "Batch loss:  0.2521670460700989\n",
      "Batch loss:  0.21695196628570557\n",
      "Batch loss:  0.39118102192878723\n",
      "Batch loss:  0.24236100912094116\n",
      "Batch loss:  0.2848016619682312\n",
      "Batch loss:  0.3077322244644165\n",
      "Batch loss:  0.3137882947921753\n",
      "Batch loss:  0.2193729281425476\n",
      "Batch loss:  0.3154619336128235\n",
      "Batch loss:  0.2821483910083771\n",
      "Batch loss:  0.442649781703949\n",
      "Batch loss:  0.27992478013038635\n",
      "Batch loss:  0.34264975786209106\n",
      "Batch loss:  0.5734191536903381\n",
      "Batch loss:  0.3598509430885315\n",
      "Batch loss:  0.3893786370754242\n",
      "Batch loss:  0.39550548791885376\n",
      "Batch loss:  0.3528573215007782\n",
      "Current average loss: 0.49960304378291165\n",
      "Batch loss:  0.29514914751052856\n",
      "Batch loss:  0.2454381287097931\n",
      "Batch loss:  0.35077545046806335\n",
      "Batch loss:  0.28860530257225037\n",
      "Batch loss:  0.2563713788986206\n",
      "Batch loss:  0.29211685061454773\n",
      "Batch loss:  0.21186383068561554\n",
      "Batch loss:  0.200287327170372\n",
      "Batch loss:  0.23883993923664093\n",
      "Batch loss:  0.3320673108100891\n",
      "Batch loss:  0.181349515914917\n",
      "Batch loss:  0.12297297269105911\n",
      "Batch loss:  0.17461323738098145\n",
      "Batch loss:  0.11463660001754761\n",
      "Batch loss:  0.28024303913116455\n",
      "Batch loss:  0.26180851459503174\n",
      "Batch loss:  0.14526240527629852\n",
      "Batch loss:  0.32289057970046997\n",
      "Batch loss:  0.5720879435539246\n",
      "Batch loss:  0.30849215388298035\n",
      "Batch loss:  0.4352581799030304\n",
      "Batch loss:  0.26380425691604614\n",
      "Batch loss:  0.24135826528072357\n",
      "Batch loss:  0.33930251002311707\n",
      "Batch loss:  0.3244135081768036\n",
      "Batch loss:  0.2962524890899658\n",
      "Batch loss:  0.2848260998725891\n",
      "Batch loss:  0.3635651767253876\n",
      "Batch loss:  0.2110385298728943\n",
      "Batch loss:  0.3029479682445526\n",
      "Batch loss:  0.2715427875518799\n",
      "Batch loss:  0.20975419878959656\n",
      "Batch loss:  0.3082831799983978\n",
      "Batch loss:  0.212463840842247\n",
      "Batch loss:  0.3106880486011505\n",
      "Batch loss:  0.1537920981645584\n",
      "Batch loss:  0.4076468050479889\n",
      "Batch loss:  0.3020043969154358\n",
      "Batch loss:  0.22696298360824585\n",
      "Batch loss:  0.529221773147583\n",
      "Batch loss:  0.5498390793800354\n",
      "Batch loss:  0.3375975489616394\n",
      "Batch loss:  0.3501061797142029\n",
      "Batch loss:  0.2984771132469177\n",
      "Batch loss:  0.21687009930610657\n",
      "Batch loss:  0.49225932359695435\n",
      "Batch loss:  0.27340736985206604\n",
      "Batch loss:  0.24699455499649048\n",
      "Batch loss:  0.36714139580726624\n",
      "Batch loss:  0.3317990303039551\n",
      "Batch loss:  0.19345390796661377\n",
      "Batch loss:  0.19986046850681305\n",
      "Batch loss:  0.2148134708404541\n",
      "Batch loss:  0.3773248791694641\n",
      "Batch loss:  0.19913162291049957\n",
      "Batch loss:  0.26688456535339355\n",
      "Batch loss:  0.2657385468482971\n",
      "Batch loss:  0.20097926259040833\n",
      "Batch loss:  0.162147656083107\n",
      "Batch loss:  0.15623779594898224\n",
      "Batch loss:  0.20175524055957794\n",
      "Batch loss:  0.46557381749153137\n",
      "Batch loss:  0.356573224067688\n",
      "Batch loss:  0.23324106633663177\n",
      "Batch loss:  0.3468441963195801\n",
      "Batch loss:  0.3556487560272217\n",
      "Batch loss:  0.22963668406009674\n",
      "Batch loss:  0.23557397723197937\n",
      "Batch loss:  0.12411416321992874\n",
      "Batch loss:  0.24821783602237701\n",
      "Batch loss:  0.28035828471183777\n",
      "Batch loss:  0.27280083298683167\n",
      "Batch loss:  0.3034724295139313\n",
      "Batch loss:  0.4857109785079956\n",
      "Batch loss:  0.40609055757522583\n",
      "Batch loss:  0.4343094527721405\n",
      "Batch loss:  0.3718465566635132\n",
      "Batch loss:  0.3800928294658661\n",
      "Batch loss:  0.32753685116767883\n",
      "Batch loss:  0.4737236201763153\n",
      "Batch loss:  0.2630341947078705\n",
      "Batch loss:  0.16854150593280792\n",
      "Batch loss:  0.37700706720352173\n",
      "Batch loss:  0.24404197931289673\n",
      "Batch loss:  0.30112534761428833\n",
      "Batch loss:  0.364448606967926\n",
      "Batch loss:  0.23266613483428955\n",
      "Batch loss:  0.20783907175064087\n",
      "Batch loss:  0.3700908124446869\n",
      "Batch loss:  0.3253365755081177\n",
      "Batch loss:  0.3613157868385315\n",
      "Batch loss:  0.23002289235591888\n",
      "Batch loss:  0.38386306166648865\n",
      "Batch loss:  0.29883572459220886\n",
      "Batch loss:  0.22396664321422577\n",
      "Batch loss:  0.23567599058151245\n",
      "Batch loss:  0.2151072472333908\n",
      "Batch loss:  0.28192776441574097\n",
      "Batch loss:  0.26186642050743103\n",
      "Batch loss:  0.215239480137825\n",
      "Current average loss: 0.4961653830521108\n",
      "Batch loss:  0.35701391100883484\n",
      "Batch loss:  0.43560490012168884\n",
      "Batch loss:  0.3739931583404541\n",
      "Batch loss:  0.32512056827545166\n",
      "Batch loss:  0.2572942078113556\n",
      "Batch loss:  0.17246030271053314\n",
      "Batch loss:  0.24924471974372864\n",
      "Batch loss:  0.28299811482429504\n",
      "Batch loss:  0.1994786560535431\n",
      "Batch loss:  0.24976634979248047\n",
      "Batch loss:  0.33456355333328247\n",
      "Batch loss:  0.3539566993713379\n",
      "Batch loss:  0.3535148501396179\n",
      "Batch loss:  0.2747592031955719\n",
      "Batch loss:  0.2583424150943756\n",
      "Batch loss:  0.18402239680290222\n",
      "Batch loss:  0.24809958040714264\n",
      "Batch loss:  0.40866953134536743\n",
      "Batch loss:  0.41173556447029114\n",
      "Batch loss:  0.20164333283901215\n",
      "Batch loss:  0.37815213203430176\n",
      "Batch loss:  0.33560895919799805\n",
      "Batch loss:  0.3162297308444977\n",
      "Batch loss:  0.2847493886947632\n",
      "Batch loss:  0.2713911533355713\n",
      "Batch loss:  0.24871300160884857\n",
      "Batch loss:  0.21445275843143463\n",
      "Batch loss:  0.43570929765701294\n",
      "Batch loss:  0.26635465025901794\n",
      "Batch loss:  0.19536250829696655\n",
      "Batch loss:  0.17999808490276337\n",
      "Batch loss:  0.18965505063533783\n",
      "Batch loss:  0.22798921167850494\n",
      "Batch loss:  0.18296293914318085\n",
      "Batch loss:  0.5557383894920349\n",
      "Batch loss:  0.3300766944885254\n",
      "Batch loss:  0.2328006625175476\n",
      "Batch loss:  0.2390146702528\n",
      "Batch loss:  0.18881265819072723\n",
      "Batch loss:  0.20737341046333313\n",
      "Batch loss:  0.19202183187007904\n",
      "Batch loss:  0.36030784249305725\n",
      "Batch loss:  0.3558160662651062\n",
      "Batch loss:  0.3679185211658478\n",
      "Batch loss:  0.44650208950042725\n",
      "Batch loss:  0.26949065923690796\n",
      "Batch loss:  0.4562299847602844\n",
      "Batch loss:  0.23733839392662048\n",
      "Batch loss:  0.2757704555988312\n",
      "Batch loss:  0.13537149131298065\n",
      "Batch loss:  0.3438916802406311\n",
      "Batch loss:  0.5191858410835266\n",
      "Batch loss:  0.688740074634552\n",
      "Batch loss:  0.20528826117515564\n",
      "Batch loss:  0.3102928400039673\n",
      "Batch loss:  0.26226702332496643\n",
      "Batch loss:  0.2522170841693878\n",
      "Batch loss:  0.25628048181533813\n",
      "Batch loss:  0.18081936240196228\n",
      "Batch loss:  0.3520539700984955\n",
      "Batch loss:  0.25676625967025757\n",
      "Batch loss:  0.23618823289871216\n",
      "Batch loss:  0.3180331587791443\n",
      "Batch loss:  0.3378358781337738\n",
      "Batch loss:  0.2980820834636688\n",
      "Batch loss:  0.2721291184425354\n",
      "Batch loss:  0.37626221776008606\n",
      "Batch loss:  0.3506517708301544\n",
      "Batch loss:  0.23189818859100342\n",
      "Batch loss:  0.23551151156425476\n",
      "Batch loss:  0.3524405062198639\n",
      "Batch loss:  0.3487640619277954\n",
      "Batch loss:  0.16352681815624237\n",
      "Batch loss:  0.22350560128688812\n",
      "Batch loss:  0.28000736236572266\n",
      "Batch loss:  0.2732123136520386\n",
      "Batch loss:  0.2876638174057007\n",
      "Batch loss:  0.3238692879676819\n",
      "Batch loss:  0.19231919944286346\n",
      "Batch loss:  0.29225870966911316\n",
      "Batch loss:  0.4963512420654297\n",
      "Batch loss:  0.452421635389328\n",
      "Batch loss:  0.23271337151527405\n",
      "Batch loss:  0.3341996371746063\n",
      "Batch loss:  0.28839898109436035\n",
      "Batch loss:  0.23396436870098114\n",
      "Batch loss:  0.23386119306087494\n",
      "Batch loss:  0.27587658166885376\n",
      "Batch loss:  0.20445670187473297\n",
      "Batch loss:  0.23420915007591248\n",
      "Batch loss:  0.1569546014070511\n",
      "Batch loss:  0.24019601941108704\n",
      "Batch loss:  0.22301992774009705\n",
      "Batch loss:  0.13502083718776703\n",
      "Batch loss:  0.36696505546569824\n",
      "Batch loss:  0.22323882579803467\n",
      "Batch loss:  0.16739735007286072\n",
      "Batch loss:  0.33538347482681274\n",
      "Batch loss:  0.22598780691623688\n",
      "Batch loss:  0.2801326811313629\n",
      "Current average loss: 0.49282017559500363\n",
      "Batch loss:  0.2425493448972702\n",
      "Batch loss:  0.21747584640979767\n",
      "Batch loss:  0.18405288457870483\n",
      "Batch loss:  0.2180231362581253\n",
      "Batch loss:  0.25133752822875977\n",
      "Batch loss:  0.3016054928302765\n",
      "Batch loss:  0.2627483010292053\n",
      "Batch loss:  0.3778277337551117\n",
      "Batch loss:  0.2203666865825653\n",
      "Batch loss:  0.33103540539741516\n",
      "Batch loss:  0.23679248988628387\n",
      "Batch loss:  0.24106717109680176\n",
      "Batch loss:  0.24914392828941345\n",
      "Batch loss:  0.34014278650283813\n",
      "Batch loss:  0.4471392035484314\n",
      "Batch loss:  0.49924805760383606\n",
      "Batch loss:  0.21655510365962982\n",
      "Batch loss:  0.2830014228820801\n",
      "Batch loss:  0.21110166609287262\n",
      "Batch loss:  0.29810020327568054\n",
      "Batch loss:  0.2571225166320801\n",
      "Batch loss:  0.22712503373622894\n",
      "Batch loss:  0.2816791832447052\n",
      "Batch loss:  0.29455000162124634\n",
      "Batch loss:  0.21928158402442932\n",
      "Batch loss:  0.23805959522724152\n",
      "Batch loss:  0.3789374530315399\n",
      "Batch loss:  0.1966060847043991\n",
      "Batch loss:  0.18627963960170746\n",
      "Batch loss:  0.16551953554153442\n",
      "Batch loss:  0.7086828351020813\n",
      "Batch loss:  0.29123160243034363\n",
      "Batch loss:  0.196687251329422\n",
      "Batch loss:  0.33978334069252014\n",
      "Batch loss:  0.20619960129261017\n",
      "Batch loss:  0.21536646783351898\n",
      "Batch loss:  0.4497198760509491\n",
      "Batch loss:  0.3747049570083618\n",
      "Batch loss:  0.19581305980682373\n",
      "Batch loss:  0.16465190052986145\n",
      "Batch loss:  0.2996112108230591\n",
      "Batch loss:  0.2545072138309479\n",
      "Batch loss:  0.4690340757369995\n",
      "Batch loss:  0.26222923398017883\n",
      "Batch loss:  0.21886900067329407\n",
      "Batch loss:  0.4885750710964203\n",
      "Batch loss:  0.537900447845459\n",
      "Batch loss:  0.2108575701713562\n",
      "Batch loss:  0.2883960008621216\n",
      "Batch loss:  0.1419426053762436\n",
      "Batch loss:  0.5644793510437012\n",
      "Batch loss:  0.1531209945678711\n",
      "Batch loss:  0.2255394607782364\n",
      "Batch loss:  0.28182217478752136\n",
      "Batch loss:  0.16193582117557526\n",
      "Batch loss:  0.2802369296550751\n",
      "Batch loss:  0.3061002194881439\n",
      "Batch loss:  0.2551311254501343\n",
      "Batch loss:  0.20767955482006073\n",
      "Batch loss:  0.42989981174468994\n",
      "Batch loss:  0.22633413970470428\n",
      "Batch loss:  0.16353769600391388\n",
      "Batch loss:  0.3435739576816559\n",
      "Batch loss:  0.2246626615524292\n",
      "Batch loss:  0.45234987139701843\n",
      "Batch loss:  0.38778847455978394\n",
      "Batch loss:  0.3489331007003784\n",
      "Batch loss:  0.3835224509239197\n",
      "Batch loss:  0.3479817509651184\n",
      "Batch loss:  0.1695227175951004\n",
      "Batch loss:  0.2518107295036316\n",
      "Batch loss:  0.43948668241500854\n",
      "Batch loss:  0.17588959634304047\n",
      "Batch loss:  0.26392269134521484\n",
      "Batch loss:  0.20910777151584625\n",
      "Batch loss:  0.32327741384506226\n",
      "Batch loss:  0.39669427275657654\n",
      "Batch loss:  0.21444620192050934\n",
      "Batch loss:  0.36292749643325806\n",
      "Batch loss:  0.3934219479560852\n",
      "Batch loss:  0.2325439304113388\n",
      "Batch loss:  0.38133642077445984\n",
      "Batch loss:  0.5264685750007629\n",
      "Batch loss:  0.22838138043880463\n",
      "Batch loss:  0.3033854365348816\n",
      "Batch loss:  0.21393975615501404\n",
      "Batch loss:  0.2876889407634735\n",
      "Batch loss:  0.30202794075012207\n",
      "Batch loss:  0.3513263165950775\n",
      "Batch loss:  0.2696443498134613\n",
      "Batch loss:  0.5119456648826599\n",
      "Batch loss:  0.21757161617279053\n",
      "Batch loss:  0.232446551322937\n",
      "Batch loss:  0.2308156043291092\n",
      "Batch loss:  0.3137534260749817\n",
      "Batch loss:  0.4558160603046417\n",
      "Batch loss:  0.28156670928001404\n",
      "Batch loss:  0.1478637158870697\n",
      "Batch loss:  0.15059442818164825\n",
      "Batch loss:  0.4709598124027252\n",
      "Current average loss: 0.4896718554058031\n",
      "Batch loss:  0.40830427408218384\n",
      "Batch loss:  0.4757843613624573\n",
      "Batch loss:  0.2594241797924042\n",
      "Batch loss:  0.25818243622779846\n",
      "Batch loss:  0.46503251791000366\n",
      "Batch loss:  0.2592751979827881\n",
      "Batch loss:  0.23655088245868683\n",
      "Batch loss:  0.29305437207221985\n",
      "Batch loss:  0.1687237173318863\n",
      "Batch loss:  0.27817660570144653\n",
      "Batch loss:  0.33389613032341003\n",
      "Batch loss:  0.15470360219478607\n",
      "Batch loss:  0.29250434041023254\n",
      "Batch loss:  0.32028332352638245\n",
      "Batch loss:  0.3408062756061554\n",
      "Batch loss:  0.25416329503059387\n",
      "Batch loss:  0.2652815878391266\n",
      "Batch loss:  0.332587331533432\n",
      "Batch loss:  0.29564985632896423\n",
      "Batch loss:  0.24167035520076752\n",
      "Batch loss:  0.34624356031417847\n",
      "Batch loss:  0.22869959473609924\n",
      "Batch loss:  0.24390146136283875\n",
      "Batch loss:  0.3942420184612274\n",
      "Batch loss:  0.19748824834823608\n",
      "Batch loss:  0.25351813435554504\n",
      "Batch loss:  0.30801060795783997\n",
      "Batch loss:  0.37275561690330505\n",
      "Batch loss:  0.24288971722126007\n",
      "Batch loss:  0.3025906980037689\n",
      "Batch loss:  0.48539960384368896\n",
      "Batch loss:  0.1578238159418106\n",
      "Batch loss:  0.408331960439682\n",
      "Batch loss:  0.2192557007074356\n",
      "Batch loss:  0.3871298134326935\n",
      "Batch loss:  0.24793146550655365\n",
      "Batch loss:  0.18491211533546448\n",
      "Batch loss:  0.18377460539340973\n",
      "Batch loss:  0.11501957476139069\n",
      "Batch loss:  0.2463366687297821\n",
      "Batch loss:  0.15073907375335693\n",
      "Batch loss:  0.2293892502784729\n",
      "Batch loss:  0.23211832344532013\n",
      "Batch loss:  0.2635272145271301\n",
      "Batch loss:  0.2812195420265198\n",
      "Batch loss:  0.22645527124404907\n",
      "Batch loss:  0.322514146566391\n",
      "Batch loss:  0.3597610294818878\n",
      "Batch loss:  0.36147749423980713\n",
      "Batch loss:  0.16520194709300995\n",
      "Batch loss:  0.24819037318229675\n",
      "Batch loss:  0.18415945768356323\n",
      "Batch loss:  0.3582535684108734\n",
      "Batch loss:  0.13681703805923462\n",
      "Batch loss:  0.3372161388397217\n",
      "Batch loss:  0.298566073179245\n",
      "Batch loss:  0.15474115312099457\n",
      "Batch loss:  0.23999065160751343\n",
      "Batch loss:  0.24697573482990265\n",
      "Batch loss:  0.2119850218296051\n",
      "Batch loss:  0.4471648037433624\n",
      "Batch loss:  0.40496954321861267\n",
      "Batch loss:  0.4680604040622711\n",
      "Batch loss:  0.3199811279773712\n",
      "Batch loss:  0.5661182999610901\n",
      "Batch loss:  0.3164886236190796\n",
      "Batch loss:  0.4432302415370941\n",
      "Batch loss:  0.3294588327407837\n",
      "Batch loss:  0.23700350522994995\n",
      "Batch loss:  0.28759709000587463\n",
      "Batch loss:  0.30954381823539734\n",
      "Batch loss:  0.16872389614582062\n",
      "Batch loss:  0.4469352662563324\n",
      "Batch loss:  0.19312182068824768\n",
      "Batch loss:  0.27518948912620544\n",
      "Batch loss:  0.12514205276966095\n",
      "Batch loss:  0.20548751950263977\n",
      "Batch loss:  0.2607680857181549\n",
      "Batch loss:  0.5298027992248535\n",
      "Batch loss:  0.24924057722091675\n",
      "Batch loss:  0.26899808645248413\n",
      "Batch loss:  0.20699255168437958\n",
      "Batch loss:  0.31406742334365845\n",
      "Batch loss:  0.2742120623588562\n",
      "Batch loss:  0.3470255434513092\n",
      "Batch loss:  0.2980838716030121\n",
      "Batch loss:  0.24091464281082153\n",
      "Batch loss:  0.3272556662559509\n",
      "Batch loss:  0.17590712010860443\n",
      "Batch loss:  0.2767876088619232\n",
      "Batch loss:  0.2567377984523773\n",
      "Batch loss:  0.4564271569252014\n",
      "Batch loss:  0.24295946955680847\n",
      "Batch loss:  0.2307962030172348\n",
      "Batch loss:  0.18567056953907013\n",
      "Batch loss:  0.25068414211273193\n",
      "Batch loss:  0.3183611333370209\n",
      "Batch loss:  0.3330298662185669\n",
      "Batch loss:  0.2332904040813446\n",
      "Batch loss:  0.2866821587085724\n",
      "Current average loss: 0.4864866240143571\n",
      "Batch loss:  0.26479050517082214\n",
      "Batch loss:  0.26300033926963806\n",
      "Batch loss:  0.22500945627689362\n",
      "Batch loss:  0.2513391375541687\n",
      "Batch loss:  0.2585051357746124\n",
      "Batch loss:  0.20688395202159882\n",
      "Batch loss:  0.3238070011138916\n",
      "Batch loss:  0.40345486998558044\n",
      "Batch loss:  0.3874025344848633\n",
      "Batch loss:  0.17366202175617218\n",
      "Batch loss:  0.29145216941833496\n",
      "Batch loss:  0.19626638293266296\n",
      "Batch loss:  0.1644192785024643\n",
      "Batch loss:  0.13100209832191467\n",
      "Batch loss:  0.16261026263237\n",
      "Batch loss:  0.2562378942966461\n",
      "Batch loss:  0.42024552822113037\n",
      "Batch loss:  0.24271845817565918\n",
      "Batch loss:  0.2855367958545685\n",
      "Batch loss:  0.1942082643508911\n",
      "Batch loss:  0.3053850829601288\n",
      "Batch loss:  0.3263428211212158\n",
      "Batch loss:  0.18945486843585968\n",
      "Batch loss:  0.2761636972427368\n",
      "Batch loss:  0.5496821403503418\n",
      "Batch loss:  0.2762133479118347\n",
      "Batch loss:  0.8764680624008179\n",
      "Batch loss:  0.12964202463626862\n",
      "Batch loss:  0.16741445660591125\n",
      "Batch loss:  0.3312477767467499\n",
      "Batch loss:  0.37466853857040405\n",
      "Batch loss:  0.3565422296524048\n",
      "Batch loss:  0.219402015209198\n",
      "Batch loss:  0.21494776010513306\n",
      "Batch loss:  0.3359386622905731\n",
      "Batch loss:  0.5194226503372192\n",
      "Batch loss:  0.23121793568134308\n",
      "Batch loss:  0.28726425766944885\n",
      "Batch loss:  0.2736797332763672\n",
      "Batch loss:  0.17510350048542023\n",
      "Batch loss:  0.2371760904788971\n",
      "Batch loss:  0.3277473747730255\n",
      "Batch loss:  0.757404088973999\n",
      "Batch loss:  0.15675045549869537\n",
      "Batch loss:  0.4320015013217926\n",
      "Batch loss:  0.313809335231781\n",
      "Batch loss:  0.3287944197654724\n",
      "Batch loss:  0.3112400770187378\n",
      "Batch loss:  0.13902999460697174\n",
      "Batch loss:  0.39859145879745483\n",
      "Batch loss:  0.4193647801876068\n",
      "Batch loss:  0.24320779740810394\n",
      "Batch loss:  0.27740535140037537\n",
      "Batch loss:  0.2574668526649475\n",
      "Batch loss:  0.25156375765800476\n",
      "Batch loss:  0.3766785264015198\n",
      "Batch loss:  0.2720935046672821\n",
      "Batch loss:  0.22109463810920715\n",
      "Batch loss:  0.28396347165107727\n",
      "Batch loss:  0.1626296490430832\n",
      "Batch loss:  0.1414773315191269\n",
      "Batch loss:  0.21812613308429718\n",
      "Batch loss:  0.35241466760635376\n",
      "Batch loss:  0.3194815516471863\n",
      "Batch loss:  0.18294352293014526\n",
      "Batch loss:  0.2589126229286194\n",
      "Batch loss:  0.566806435585022\n",
      "Batch loss:  0.3184094727039337\n",
      "Batch loss:  0.2492368072271347\n",
      "Batch loss:  0.24896050989627838\n",
      "Batch loss:  0.33816397190093994\n",
      "Batch loss:  0.2827569544315338\n",
      "Batch loss:  0.2610691785812378\n",
      "Batch loss:  0.3218587338924408\n",
      "Batch loss:  0.6164994835853577\n",
      "Batch loss:  0.260080486536026\n",
      "Batch loss:  0.2099210023880005\n",
      "Batch loss:  0.26056262850761414\n",
      "Batch loss:  0.26842230558395386\n",
      "Batch loss:  0.19282598793506622\n",
      "Batch loss:  0.1961470991373062\n",
      "Batch loss:  0.17645098268985748\n",
      "Batch loss:  0.3764519691467285\n",
      "Batch loss:  0.35032328963279724\n",
      "Batch loss:  0.6115986108779907\n",
      "Batch loss:  0.18505330383777618\n",
      "Batch loss:  0.17315450310707092\n",
      "Batch loss:  0.3536930978298187\n",
      "Batch loss:  0.2545109987258911\n",
      "Batch loss:  0.13132447004318237\n",
      "Batch loss:  0.1597212702035904\n",
      "Batch loss:  0.2386985421180725\n",
      "Batch loss:  0.14218799769878387\n",
      "Batch loss:  0.22640830278396606\n",
      "Batch loss:  0.24826647341251373\n",
      "Batch loss:  0.27942898869514465\n",
      "Batch loss:  0.18657617270946503\n",
      "Batch loss:  0.4684276282787323\n",
      "Batch loss:  0.42764389514923096\n",
      "Batch loss:  0.18275193870067596\n",
      "Current average loss: 0.48345260673936485\n",
      "Batch loss:  0.3523237109184265\n",
      "Batch loss:  0.2872464954853058\n",
      "Batch loss:  0.1208011731505394\n",
      "Batch loss:  0.4012634754180908\n",
      "Batch loss:  0.3090709447860718\n",
      "Batch loss:  0.4597301483154297\n",
      "Batch loss:  0.1052432581782341\n",
      "Batch loss:  0.25281456112861633\n",
      "Batch loss:  0.2484251707792282\n",
      "Batch loss:  0.26107949018478394\n",
      "Batch loss:  0.25487902760505676\n",
      "Batch loss:  0.24454039335250854\n",
      "Batch loss:  0.2352556586265564\n",
      "Batch loss:  0.5874656438827515\n",
      "Batch loss:  0.2555794417858124\n",
      "Batch loss:  0.23851469159126282\n",
      "Batch loss:  0.2756849527359009\n",
      "Batch loss:  0.25646278262138367\n",
      "Batch loss:  0.36085987091064453\n",
      "Batch loss:  0.4089973270893097\n",
      "Batch loss:  0.18895673751831055\n",
      "Batch loss:  0.2923169434070587\n",
      "Batch loss:  0.22954599559307098\n",
      "Batch loss:  0.2968018352985382\n",
      "Batch loss:  0.23991772532463074\n",
      "Batch loss:  0.3267609477043152\n",
      "Batch loss:  0.4067787528038025\n",
      "Batch loss:  0.36370649933815\n",
      "Batch loss:  0.3091976046562195\n",
      "Batch loss:  0.23059391975402832\n",
      "Batch loss:  0.20359200239181519\n",
      "Batch loss:  0.4158538579940796\n",
      "Batch loss:  0.4112687110900879\n",
      "Batch loss:  0.12004353106021881\n",
      "Batch loss:  0.2556299567222595\n",
      "Batch loss:  0.339668333530426\n",
      "Batch loss:  0.13360902667045593\n",
      "Batch loss:  0.24541352689266205\n",
      "Batch loss:  0.3537900149822235\n",
      "Batch loss:  0.2233034074306488\n",
      "Batch loss:  0.258975088596344\n",
      "Batch loss:  0.25791147351264954\n",
      "Batch loss:  0.26833078265190125\n",
      "Batch loss:  0.26764142513275146\n",
      "Batch loss:  0.28619179129600525\n",
      "Batch loss:  0.5773116946220398\n",
      "Batch loss:  0.17348048090934753\n",
      "Batch loss:  0.3177719712257385\n",
      "Batch loss:  0.37517234683036804\n",
      "Batch loss:  0.32682037353515625\n",
      "Batch loss:  0.23513147234916687\n",
      "Batch loss:  0.17439962923526764\n",
      "Batch loss:  0.18875408172607422\n",
      "Batch loss:  0.2879338264465332\n",
      "Batch loss:  0.3583492338657379\n",
      "Batch loss:  0.2469439059495926\n",
      "Batch loss:  0.18349316716194153\n",
      "Batch loss:  0.23394596576690674\n",
      "Batch loss:  0.3040217161178589\n",
      "Batch loss:  0.4420104920864105\n",
      "Batch loss:  0.25815677642822266\n",
      "Batch loss:  0.25500643253326416\n",
      "Batch loss:  0.4696958363056183\n",
      "Batch loss:  0.20077580213546753\n",
      "Batch loss:  0.4999915361404419\n",
      "Batch loss:  0.13832828402519226\n",
      "Batch loss:  0.34979283809661865\n",
      "Batch loss:  0.25314566493034363\n",
      "Batch loss:  0.21189384162425995\n",
      "Batch loss:  0.5188175439834595\n",
      "Batch loss:  0.25199010968208313\n",
      "Batch loss:  0.27876555919647217\n",
      "Batch loss:  0.26244157552719116\n",
      "Batch loss:  0.36030328273773193\n",
      "Batch loss:  0.32521581649780273\n",
      "Batch loss:  0.16601470112800598\n",
      "Batch loss:  0.20278041064739227\n",
      "Batch loss:  0.39949601888656616\n",
      "Batch loss:  0.2045084685087204\n",
      "Batch loss:  0.1562943011522293\n",
      "Batch loss:  0.38115593791007996\n",
      "Batch loss:  0.32872140407562256\n",
      "Batch loss:  0.19340571761131287\n",
      "Batch loss:  0.15465109050273895\n",
      "Batch loss:  0.35120776295661926\n",
      "Batch loss:  0.28621017932891846\n",
      "Batch loss:  0.2888886630535126\n",
      "Batch loss:  0.56755131483078\n",
      "Batch loss:  0.2022070437669754\n",
      "Batch loss:  0.30276355147361755\n",
      "Batch loss:  0.21704937517642975\n",
      "Batch loss:  0.15562830865383148\n",
      "Batch loss:  0.299376517534256\n",
      "Batch loss:  0.3516761362552643\n",
      "Batch loss:  0.2534251809120178\n",
      "Batch loss:  0.1913561373949051\n",
      "Batch loss:  0.48458418250083923\n",
      "Batch loss:  0.37262630462646484\n",
      "Batch loss:  0.29159554839134216\n",
      "Batch loss:  0.19750358164310455\n",
      "Current average loss: 0.4805071918829122\n",
      "Batch loss:  0.28757601976394653\n",
      "Batch loss:  0.3661579489707947\n",
      "Batch loss:  0.2150518000125885\n",
      "Batch loss:  0.3058416545391083\n",
      "Batch loss:  0.2654191553592682\n",
      "Batch loss:  0.21496686339378357\n",
      "Batch loss:  0.5050458312034607\n",
      "Batch loss:  0.15643981099128723\n",
      "Batch loss:  0.4451431334018707\n",
      "Batch loss:  0.1605917364358902\n",
      "Batch loss:  0.23580706119537354\n",
      "Batch loss:  0.3819558322429657\n",
      "Batch loss:  0.6148272752761841\n",
      "Batch loss:  0.32092487812042236\n",
      "Batch loss:  0.45163044333457947\n",
      "Batch loss:  0.21461139619350433\n",
      "Batch loss:  0.21561679244041443\n",
      "Batch loss:  0.16018235683441162\n",
      "Batch loss:  0.3566702604293823\n",
      "Batch loss:  0.30988809466362\n",
      "Batch loss:  0.2759797275066376\n",
      "Batch loss:  0.24834130704402924\n",
      "Batch loss:  0.27449795603752136\n",
      "Batch loss:  0.2075851857662201\n",
      "Batch loss:  0.4451214075088501\n",
      "Batch loss:  0.14922302961349487\n",
      "Batch loss:  0.24703407287597656\n",
      "Batch loss:  0.32601165771484375\n",
      "Batch loss:  0.15761339664459229\n",
      "Batch loss:  0.2842831313610077\n",
      "Batch loss:  0.28669413924217224\n",
      "Batch loss:  0.1622024029493332\n",
      "Batch loss:  0.13399717211723328\n",
      "Batch loss:  0.24448475241661072\n",
      "Batch loss:  0.301248699426651\n",
      "Batch loss:  0.19871015846729279\n",
      "Batch loss:  0.24322423338890076\n",
      "Batch loss:  0.23766006529331207\n",
      "Batch loss:  0.2188262939453125\n",
      "Batch loss:  0.36599206924438477\n",
      "Batch loss:  0.2995014786720276\n",
      "Batch loss:  0.19972044229507446\n",
      "Batch loss:  0.29884427785873413\n",
      "Batch loss:  0.19110873341560364\n",
      "Batch loss:  0.22048577666282654\n",
      "Batch loss:  0.3950897753238678\n",
      "Batch loss:  0.352302610874176\n",
      "Batch loss:  0.25887373089790344\n",
      "Batch loss:  0.23995648324489594\n",
      "Batch loss:  0.20380429923534393\n",
      "Batch loss:  0.19000937044620514\n",
      "Batch loss:  0.29665306210517883\n",
      "Batch loss:  0.35269802808761597\n",
      "Batch loss:  0.3570169508457184\n",
      "Batch loss:  0.25772029161453247\n",
      "Batch loss:  0.36615481972694397\n",
      "Batch loss:  0.1974237710237503\n",
      "Batch loss:  0.27721357345581055\n",
      "Batch loss:  0.2883681058883667\n",
      "Batch loss:  0.15855902433395386\n",
      "Batch loss:  0.0998757928609848\n",
      "Batch loss:  0.15570367872714996\n",
      "Batch loss:  0.23192009329795837\n",
      "Batch loss:  0.2580204904079437\n",
      "Batch loss:  0.2004486471414566\n",
      "Batch loss:  0.3069725036621094\n",
      "Batch loss:  0.2848224937915802\n",
      "Batch loss:  0.11447867751121521\n",
      "Batch loss:  0.2333952933549881\n",
      "Batch loss:  0.2877424955368042\n",
      "Batch loss:  0.2646278440952301\n",
      "Batch loss:  0.19927513599395752\n",
      "Batch loss:  0.225607767701149\n",
      "Batch loss:  0.3046518564224243\n",
      "Batch loss:  0.4710741341114044\n",
      "Batch loss:  0.2162701040506363\n",
      "Batch loss:  0.36330893635749817\n",
      "Batch loss:  0.34434038400650024\n",
      "Batch loss:  0.4777200222015381\n",
      "Batch loss:  0.3196774125099182\n",
      "Batch loss:  0.3752216398715973\n",
      "Batch loss:  0.18085606396198273\n",
      "Batch loss:  0.2642545998096466\n",
      "Batch loss:  0.2203107327222824\n",
      "Batch loss:  0.19448792934417725\n",
      "Batch loss:  0.17838899791240692\n",
      "Batch loss:  0.33275872468948364\n",
      "Batch loss:  0.4518316984176636\n",
      "Batch loss:  0.15125180780887604\n",
      "Batch loss:  0.30011624097824097\n",
      "Batch loss:  0.3439023494720459\n",
      "Batch loss:  0.2675028145313263\n",
      "Batch loss:  0.27744948863983154\n",
      "Batch loss:  0.2855874001979828\n",
      "Batch loss:  0.3282676935195923\n",
      "Batch loss:  0.29847994446754456\n",
      "Batch loss:  0.40141841769218445\n",
      "Batch loss:  0.2569153904914856\n",
      "Batch loss:  0.42551329731941223\n",
      "Batch loss:  0.2822454571723938\n",
      "Current average loss: 0.47748086134625306\n",
      "Batch loss:  0.25539451837539673\n",
      "Batch loss:  0.43392428755760193\n",
      "Batch loss:  0.22691456973552704\n",
      "Batch loss:  0.30716606974601746\n",
      "Batch loss:  0.28126949071884155\n",
      "Batch loss:  0.2392139434814453\n",
      "Batch loss:  0.25877001881599426\n",
      "Batch loss:  0.26926398277282715\n",
      "Batch loss:  0.3976956903934479\n",
      "Batch loss:  0.31720659136772156\n",
      "Batch loss:  0.2733573913574219\n",
      "Batch loss:  0.3437245786190033\n",
      "Batch loss:  0.24230584502220154\n",
      "Batch loss:  0.25472602248191833\n",
      "Batch loss:  0.21181106567382812\n",
      "Batch loss:  0.29665738344192505\n",
      "Batch loss:  0.28600412607192993\n",
      "Batch loss:  0.1786910593509674\n",
      "Batch loss:  0.24035370349884033\n",
      "Batch loss:  0.3124902546405792\n",
      "Batch loss:  0.22686032950878143\n",
      "Batch loss:  0.24992798268795013\n",
      "Batch loss:  0.2673913538455963\n",
      "Batch loss:  0.3033991754055023\n",
      "Batch loss:  0.24016208946704865\n",
      "Batch loss:  0.20467554032802582\n",
      "Batch loss:  0.2589055299758911\n",
      "Batch loss:  0.32192596793174744\n",
      "Batch loss:  0.18270117044448853\n",
      "Batch loss:  0.21897804737091064\n",
      "Batch loss:  0.14284516870975494\n",
      "Batch loss:  0.4329206943511963\n",
      "Batch loss:  0.43469706177711487\n",
      "Batch loss:  0.17057152092456818\n",
      "Batch loss:  0.2848738431930542\n",
      "Batch loss:  0.3786846697330475\n",
      "Batch loss:  0.32833534479141235\n",
      "Batch loss:  0.29795482754707336\n",
      "Batch loss:  0.20733164250850677\n",
      "Batch loss:  0.4063514769077301\n",
      "Batch loss:  0.3039589822292328\n",
      "Batch loss:  0.2941666543483734\n",
      "Batch loss:  0.21183274686336517\n",
      "Batch loss:  0.250468909740448\n",
      "Batch loss:  0.2077270746231079\n",
      "Batch loss:  0.3597660958766937\n",
      "Batch loss:  0.18170645833015442\n",
      "Batch loss:  0.13618528842926025\n",
      "Batch loss:  0.24241523444652557\n",
      "Batch loss:  0.40217098593711853\n",
      "Batch loss:  0.37669408321380615\n",
      "Batch loss:  0.19837817549705505\n",
      "Batch loss:  0.17153090238571167\n",
      "Batch loss:  0.33610832691192627\n",
      "Batch loss:  0.28038448095321655\n",
      "Batch loss:  0.3760753571987152\n",
      "Batch loss:  0.41655057668685913\n",
      "Batch loss:  0.28959399461746216\n",
      "Batch loss:  0.23012012243270874\n",
      "Batch loss:  0.3311176598072052\n",
      "Batch loss:  0.22367922961711884\n",
      "Batch loss:  0.15738007426261902\n",
      "Batch loss:  0.2211790680885315\n",
      "Batch loss:  0.2186678797006607\n",
      "Batch loss:  0.27459433674812317\n",
      "Batch loss:  0.301475465297699\n",
      "Batch loss:  0.19009922444820404\n",
      "Batch loss:  0.16951680183410645\n",
      "Batch loss:  0.24895431101322174\n",
      "Batch loss:  0.17523591220378876\n",
      "Batch loss:  0.34025460481643677\n",
      "Batch loss:  0.2511204183101654\n",
      "Batch loss:  0.3929595947265625\n",
      "Batch loss:  0.306188702583313\n",
      "Batch loss:  0.3085457682609558\n",
      "Batch loss:  0.2774147391319275\n",
      "Batch loss:  0.20386308431625366\n",
      "Batch loss:  0.3894927203655243\n",
      "Batch loss:  0.20365466177463531\n",
      "Batch loss:  0.18813495337963104\n",
      "Batch loss:  0.2488533854484558\n",
      "Batch loss:  0.32900121808052063\n",
      "Batch loss:  0.23915857076644897\n",
      "Batch loss:  0.250774621963501\n",
      "Batch loss:  0.34252721071243286\n",
      "Batch loss:  0.34836718440055847\n",
      "Batch loss:  0.44975799322128296\n",
      "Batch loss:  0.3234681487083435\n",
      "Batch loss:  0.20587579905986786\n",
      "Batch loss:  0.21789158880710602\n",
      "Batch loss:  0.15981584787368774\n",
      "Batch loss:  0.2343571037054062\n",
      "Batch loss:  0.3222908079624176\n",
      "Batch loss:  0.2852197587490082\n",
      "Batch loss:  0.31601056456565857\n",
      "Batch loss:  0.27495306730270386\n",
      "Batch loss:  0.232220858335495\n",
      "Batch loss:  0.20611970126628876\n",
      "Batch loss:  0.19379980862140656\n",
      "Batch loss:  0.5384052991867065\n",
      "Current average loss: 0.47451020704483465\n",
      "Batch loss:  0.3141157329082489\n",
      "Batch loss:  0.3290078639984131\n",
      "Batch loss:  0.1854921132326126\n",
      "Batch loss:  0.300028532743454\n",
      "Batch loss:  0.13650618493556976\n",
      "Batch loss:  0.34002718329429626\n",
      "Batch loss:  0.24169635772705078\n",
      "Batch loss:  0.16513028740882874\n",
      "Batch loss:  0.16159524023532867\n",
      "Batch loss:  0.312986820936203\n",
      "Batch loss:  0.15691924095153809\n",
      "Batch loss:  0.24909304082393646\n",
      "Batch loss:  0.2070753127336502\n",
      "Batch loss:  0.24914348125457764\n",
      "Batch loss:  0.17654959857463837\n",
      "Batch loss:  0.42804059386253357\n",
      "Batch loss:  0.33839091658592224\n",
      "Batch loss:  0.36287790536880493\n",
      "Batch loss:  0.2520386874675751\n",
      "Batch loss:  0.3403885066509247\n",
      "Batch loss:  0.263970285654068\n",
      "Batch loss:  0.974992573261261\n",
      "Batch loss:  0.22222596406936646\n",
      "Batch loss:  0.14604803919792175\n",
      "Batch loss:  0.2000514566898346\n",
      "Batch loss:  0.2655269503593445\n",
      "Batch loss:  0.2955239415168762\n",
      "Batch loss:  0.2863940894603729\n",
      "Batch loss:  0.37078970670700073\n",
      "Batch loss:  0.15185660123825073\n",
      "Batch loss:  0.1511373370885849\n",
      "Batch loss:  0.2184373438358307\n",
      "Batch loss:  0.1855955272912979\n",
      "Batch loss:  0.22977754473686218\n",
      "Batch loss:  0.41755053400993347\n",
      "Batch loss:  0.30096548795700073\n",
      "Batch loss:  0.24115772545337677\n",
      "Batch loss:  0.33198249340057373\n",
      "Batch loss:  0.1450531780719757\n",
      "Batch loss:  0.4347279369831085\n",
      "Batch loss:  0.5107241272926331\n",
      "Batch loss:  0.19474132359027863\n",
      "Batch loss:  0.17665967345237732\n",
      "Batch loss:  0.29695743322372437\n",
      "Batch loss:  0.16132694482803345\n",
      "Batch loss:  0.27377641201019287\n",
      "Batch loss:  0.2595404386520386\n",
      "Batch loss:  0.2357056885957718\n",
      "Batch loss:  0.4814220368862152\n",
      "Batch loss:  0.3247782588005066\n",
      "Batch loss:  0.14833277463912964\n",
      "Batch loss:  0.2808287441730499\n",
      "Batch loss:  0.29488861560821533\n",
      "Batch loss:  0.24893946945667267\n",
      "Batch loss:  0.23670852184295654\n",
      "Batch loss:  0.26942023634910583\n",
      "Batch loss:  0.4678017795085907\n",
      "Batch loss:  0.31929075717926025\n",
      "Batch loss:  0.19082625210285187\n",
      "Batch loss:  0.26948827505111694\n",
      "Batch loss:  0.2423604428768158\n",
      "Batch loss:  0.5163747072219849\n",
      "Batch loss:  0.2551811635494232\n",
      "Batch loss:  0.19647866487503052\n",
      "Batch loss:  0.6092174053192139\n",
      "Batch loss:  0.24842792749404907\n",
      "Batch loss:  0.3855193555355072\n",
      "Batch loss:  0.280544638633728\n",
      "Batch loss:  0.3253149092197418\n",
      "Batch loss:  0.29288095235824585\n",
      "Batch loss:  0.47014424204826355\n",
      "Batch loss:  0.23638133704662323\n",
      "Batch loss:  0.3771863281726837\n",
      "Batch loss:  0.26662322878837585\n",
      "Batch loss:  0.190787672996521\n",
      "Batch loss:  0.27397897839546204\n",
      "Batch loss:  0.2727811634540558\n",
      "Batch loss:  0.11049119383096695\n",
      "Batch loss:  0.18217229843139648\n",
      "Batch loss:  0.29511356353759766\n",
      "Batch loss:  0.28417128324508667\n",
      "Batch loss:  0.652643084526062\n",
      "Batch loss:  0.18418511748313904\n",
      "Batch loss:  0.3143198490142822\n",
      "Batch loss:  0.33876731991767883\n",
      "Batch loss:  0.42482051253318787\n",
      "Batch loss:  0.25824859738349915\n",
      "Batch loss:  0.19974805414676666\n",
      "Batch loss:  0.38220179080963135\n",
      "Batch loss:  0.2605038285255432\n",
      "Batch loss:  0.4258810579776764\n",
      "Batch loss:  0.3707854151725769\n",
      "Batch loss:  0.2834623157978058\n",
      "Batch loss:  0.2190447300672531\n",
      "Batch loss:  0.18000724911689758\n",
      "Batch loss:  0.15327131748199463\n",
      "Batch loss:  0.2750023305416107\n",
      "Batch loss:  0.226288303732872\n",
      "Batch loss:  0.3074599504470825\n",
      "Batch loss:  0.16472017765045166\n",
      "Current average loss: 0.4717867608539627\n",
      "Batch loss:  0.31874001026153564\n",
      "Batch loss:  0.3307075500488281\n",
      "Batch loss:  0.3184489607810974\n",
      "Batch loss:  0.2903168797492981\n",
      "Batch loss:  0.43028584122657776\n",
      "Batch loss:  0.28819912672042847\n",
      "Batch loss:  0.2194732427597046\n",
      "Batch loss:  0.37397077679634094\n",
      "Batch loss:  0.5945447087287903\n",
      "Batch loss:  0.5357683300971985\n",
      "Batch loss:  0.38721373677253723\n",
      "Batch loss:  0.38257917761802673\n",
      "Batch loss:  0.2548291087150574\n",
      "Batch loss:  0.22174867987632751\n",
      "Batch loss:  0.3603611886501312\n",
      "Batch loss:  0.2779597043991089\n",
      "Batch loss:  0.29693594574928284\n",
      "Batch loss:  0.41701704263687134\n",
      "Batch loss:  0.2690508961677551\n",
      "Batch loss:  0.5504850745201111\n",
      "Batch loss:  0.21500420570373535\n",
      "Batch loss:  0.23590269684791565\n",
      "Batch loss:  0.17688849568367004\n",
      "Batch loss:  0.27679458260536194\n",
      "Batch loss:  0.38997623324394226\n",
      "Batch loss:  0.3046627342700958\n",
      "Batch loss:  0.291697233915329\n",
      "Batch loss:  0.25939029455184937\n",
      "Batch loss:  0.4253480136394501\n",
      "Batch loss:  0.24741403758525848\n",
      "Batch loss:  0.2551991641521454\n",
      "Batch loss:  0.2913573384284973\n",
      "Batch loss:  0.20193448662757874\n",
      "Batch loss:  0.35240283608436584\n",
      "Batch loss:  0.188524529337883\n",
      "Batch loss:  0.1634683459997177\n",
      "Batch loss:  0.42713019251823425\n",
      "Batch loss:  0.4577329456806183\n",
      "Batch loss:  0.17538964748382568\n",
      "Batch loss:  0.27275511622428894\n",
      "Batch loss:  0.16596625745296478\n",
      "Batch loss:  0.1437082588672638\n",
      "Batch loss:  0.34284332394599915\n",
      "Batch loss:  0.29094940423965454\n",
      "Batch loss:  0.18329857289791107\n",
      "Batch loss:  0.44668495655059814\n",
      "Batch loss:  0.27705276012420654\n",
      "Batch loss:  0.22780367732048035\n",
      "Batch loss:  0.27588918805122375\n",
      "Batch loss:  0.5757732391357422\n",
      "Batch loss:  0.11674466729164124\n",
      "Batch loss:  0.32970309257507324\n",
      "Batch loss:  0.25955870747566223\n",
      "Batch loss:  0.23805280029773712\n",
      "Batch loss:  0.2712355852127075\n",
      "Batch loss:  0.24657242000102997\n",
      "Batch loss:  0.23544223606586456\n",
      "Batch loss:  0.16993936896324158\n",
      "Batch loss:  0.18612566590309143\n",
      "Batch loss:  0.38614505529403687\n",
      "Batch loss:  0.3018297851085663\n",
      "Batch loss:  0.38975340127944946\n",
      "Batch loss:  0.20113655924797058\n",
      "Batch loss:  0.5169445872306824\n",
      "Batch loss:  0.3155863881111145\n",
      "Batch loss:  0.2912682592868805\n",
      "Batch loss:  0.41573426127433777\n",
      "Batch loss:  0.27141639590263367\n",
      "Batch loss:  0.15967004001140594\n",
      "Batch loss:  0.22936275601387024\n",
      "Batch loss:  0.4114057123661041\n",
      "Batch loss:  0.10467085987329483\n",
      "Batch loss:  0.2780176103115082\n",
      "Batch loss:  0.3247676491737366\n",
      "Batch loss:  0.3299475610256195\n",
      "Batch loss:  0.3841507136821747\n",
      "Batch loss:  0.35757195949554443\n",
      "Batch loss:  0.14965884387493134\n",
      "Batch loss:  0.31344330310821533\n",
      "Batch loss:  0.26617923378944397\n",
      "Batch loss:  0.3865160644054413\n",
      "Batch loss:  0.3532194197177887\n",
      "Batch loss:  0.3375709652900696\n",
      "Batch loss:  0.2333754450082779\n",
      "Batch loss:  0.2293015718460083\n",
      "Batch loss:  0.36996087431907654\n",
      "Batch loss:  0.16763311624526978\n",
      "Batch loss:  0.2250189483165741\n",
      "Batch loss:  0.1727374643087387\n",
      "Batch loss:  0.2083316594362259\n",
      "Batch loss:  0.19835641980171204\n",
      "Batch loss:  0.1318124532699585\n",
      "Batch loss:  0.3536679744720459\n",
      "Batch loss:  0.30339375138282776\n",
      "Batch loss:  0.14023511111736298\n",
      "Batch loss:  0.3086100220680237\n",
      "Batch loss:  0.2289666384458542\n",
      "Batch loss:  0.3621649742126465\n",
      "Batch loss:  0.2763005197048187\n",
      "Batch loss:  0.45283132791519165\n",
      "Current average loss: 0.46926868384590387\n",
      "Batch loss:  0.3832493722438812\n",
      "Batch loss:  0.34444764256477356\n",
      "Batch loss:  0.2844330966472626\n",
      "Batch loss:  0.20477038621902466\n",
      "Batch loss:  0.3528052270412445\n",
      "Batch loss:  0.35583487153053284\n",
      "Batch loss:  0.2489067018032074\n",
      "Batch loss:  0.1185230016708374\n",
      "Batch loss:  0.3341103494167328\n",
      "Batch loss:  0.1932593733072281\n",
      "Batch loss:  0.13717132806777954\n",
      "Batch loss:  0.45143893361091614\n",
      "Batch loss:  0.27451300621032715\n",
      "Batch loss:  0.32550713419914246\n",
      "Batch loss:  0.18340303003787994\n",
      "Batch loss:  0.18079926073551178\n",
      "Batch loss:  0.42656901478767395\n",
      "Batch loss:  0.1985342800617218\n",
      "Batch loss:  0.21189163625240326\n",
      "Batch loss:  0.17195871472358704\n",
      "Batch loss:  0.409102201461792\n",
      "Batch loss:  0.21916261315345764\n",
      "Batch loss:  0.2660250961780548\n",
      "Batch loss:  0.34414565563201904\n",
      "Batch loss:  0.2604265511035919\n",
      "Batch loss:  0.16859398782253265\n",
      "Batch loss:  0.1879895031452179\n",
      "Batch loss:  0.20797568559646606\n",
      "Batch loss:  0.37448999285697937\n",
      "Batch loss:  0.1588899940252304\n",
      "Batch loss:  0.3789357542991638\n",
      "Batch loss:  0.28371462225914\n",
      "Batch loss:  0.19948703050613403\n",
      "Batch loss:  0.2329428493976593\n",
      "Batch loss:  0.2795301377773285\n",
      "Batch loss:  0.22462914884090424\n",
      "Batch loss:  0.26269370317459106\n",
      "Batch loss:  0.4390547573566437\n",
      "Batch loss:  0.26526322960853577\n",
      "Batch loss:  0.1900082230567932\n",
      "Batch loss:  0.18168888986110687\n",
      "Batch loss:  0.30447688698768616\n",
      "Batch loss:  0.3519417345523834\n",
      "Batch loss:  0.36035624146461487\n",
      "Batch loss:  0.4953719675540924\n",
      "Batch loss:  0.19610843062400818\n",
      "Batch loss:  0.2875603437423706\n",
      "Batch loss:  0.3000742495059967\n",
      "Batch loss:  0.17216019332408905\n",
      "Batch loss:  0.4572522044181824\n",
      "Batch loss:  0.6259387731552124\n",
      "Batch loss:  0.5147165060043335\n",
      "Batch loss:  0.20013602077960968\n",
      "Batch loss:  0.32443463802337646\n",
      "Batch loss:  0.5551289916038513\n",
      "Batch loss:  0.44229939579963684\n",
      "Batch loss:  0.23020240664482117\n",
      "Batch loss:  0.32286134362220764\n",
      "Batch loss:  0.3239988386631012\n",
      "Batch loss:  0.28334668278694153\n",
      "Batch loss:  0.3871854543685913\n",
      "Batch loss:  0.13530297577381134\n",
      "Batch loss:  0.17533905804157257\n",
      "Batch loss:  0.16483232378959656\n",
      "Batch loss:  0.34989312291145325\n",
      "Batch loss:  0.36974620819091797\n",
      "Batch loss:  0.15901106595993042\n",
      "Batch loss:  0.2458953857421875\n",
      "Batch loss:  0.36692842841148376\n",
      "Batch loss:  0.26568013429641724\n",
      "Batch loss:  0.3282707929611206\n",
      "Batch loss:  0.4016241133213043\n",
      "Batch loss:  0.17882345616817474\n",
      "Batch loss:  0.15580898523330688\n",
      "Batch loss:  0.3059333562850952\n",
      "Batch loss:  0.31517642736434937\n",
      "Batch loss:  0.2169753909111023\n",
      "Batch loss:  0.40080690383911133\n",
      "Batch loss:  0.18479309976100922\n",
      "Batch loss:  0.16738763451576233\n",
      "Batch loss:  0.4257465898990631\n",
      "Batch loss:  0.16228865087032318\n",
      "Batch loss:  0.20052577555179596\n",
      "Batch loss:  0.2892061471939087\n",
      "Batch loss:  0.35719388723373413\n",
      "Batch loss:  0.3432042598724365\n",
      "Batch loss:  0.18195557594299316\n",
      "Batch loss:  0.20528237521648407\n",
      "Batch loss:  0.1931641548871994\n",
      "Batch loss:  0.2725498676300049\n",
      "Batch loss:  0.3452524244785309\n",
      "Batch loss:  0.27876976132392883\n",
      "Batch loss:  0.2826572060585022\n",
      "Batch loss:  0.2846970558166504\n",
      "Batch loss:  0.31323856115341187\n",
      "Batch loss:  0.2884635329246521\n",
      "Batch loss:  0.19054779410362244\n",
      "Batch loss:  0.3566478192806244\n",
      "Batch loss:  0.2606596350669861\n",
      "Batch loss:  0.3342183828353882\n",
      "Current average loss: 0.46667469106011283\n",
      "Batch loss:  0.2329297810792923\n",
      "Batch loss:  0.33533573150634766\n",
      "Batch loss:  0.17671126127243042\n",
      "Batch loss:  0.25674545764923096\n",
      "Batch loss:  0.20442253351211548\n",
      "Batch loss:  0.2314806580543518\n",
      "Batch loss:  0.2360975444316864\n",
      "Batch loss:  0.19415539503097534\n",
      "Batch loss:  0.19848531484603882\n",
      "Batch loss:  0.18085958063602448\n",
      "Batch loss:  0.1910860687494278\n",
      "Batch loss:  0.12453761696815491\n",
      "Batch loss:  0.4088350236415863\n",
      "Batch loss:  0.14250227808952332\n",
      "Batch loss:  0.4405513107776642\n",
      "Batch loss:  0.36627671122550964\n",
      "Batch loss:  0.13237278163433075\n",
      "Batch loss:  0.1158110722899437\n",
      "Batch loss:  0.4967586100101471\n",
      "Batch loss:  0.32769739627838135\n",
      "Batch loss:  0.19699355959892273\n",
      "Batch loss:  0.20229454338550568\n",
      "Batch loss:  0.18349356949329376\n",
      "Batch loss:  0.3168671727180481\n",
      "Batch loss:  0.43347737193107605\n",
      "Batch loss:  0.3872789144515991\n",
      "Batch loss:  0.5759199261665344\n",
      "Batch loss:  0.5067468881607056\n",
      "Batch loss:  0.25193101167678833\n",
      "Batch loss:  0.3204543888568878\n",
      "Batch loss:  0.47641366720199585\n",
      "Batch loss:  0.4169897139072418\n",
      "Batch loss:  0.5029328465461731\n",
      "Batch loss:  0.19331610202789307\n",
      "Batch loss:  0.34996408224105835\n",
      "Batch loss:  0.15845823287963867\n",
      "Batch loss:  0.28594863414764404\n",
      "Batch loss:  0.16978248953819275\n",
      "Batch loss:  0.34090951085090637\n",
      "Batch loss:  0.2400164157152176\n",
      "Batch loss:  0.23026755452156067\n",
      "Batch loss:  0.2383350133895874\n",
      "Batch loss:  0.2574910521507263\n",
      "Batch loss:  0.251067191362381\n",
      "Batch loss:  0.1635112464427948\n",
      "Batch loss:  0.31988123059272766\n",
      "Batch loss:  0.35159778594970703\n",
      "Batch loss:  0.34443992376327515\n",
      "Batch loss:  0.27213767170906067\n",
      "Batch loss:  0.5262799263000488\n",
      "Batch loss:  0.32749804854393005\n",
      "Batch loss:  0.12762227654457092\n",
      "Batch loss:  0.40531405806541443\n",
      "Batch loss:  0.22098256647586823\n",
      "Batch loss:  0.23576684296131134\n",
      "Batch loss:  0.2805040180683136\n",
      "Batch loss:  0.26729461550712585\n",
      "Batch loss:  0.3866949677467346\n",
      "Batch loss:  0.3789300322532654\n",
      "Batch loss:  0.1268080770969391\n",
      "Batch loss:  0.331910640001297\n",
      "Batch loss:  0.2590172588825226\n",
      "Batch loss:  0.3769845962524414\n",
      "Batch loss:  0.22751745581626892\n",
      "Batch loss:  0.31357064843177795\n",
      "Batch loss:  0.11256762593984604\n",
      "Batch loss:  0.32020941376686096\n",
      "Batch loss:  0.4215850234031677\n",
      "Batch loss:  0.1844712793827057\n",
      "Batch loss:  0.3275087773799896\n",
      "Batch loss:  0.24956022202968597\n",
      "Batch loss:  0.23267531394958496\n",
      "Batch loss:  0.2696395218372345\n",
      "Batch loss:  0.49213066697120667\n",
      "Batch loss:  0.2653435468673706\n",
      "Batch loss:  0.31568241119384766\n",
      "Batch loss:  0.32950183749198914\n",
      "Batch loss:  0.1574714332818985\n",
      "Batch loss:  0.216896653175354\n",
      "Batch loss:  0.2686383128166199\n",
      "Batch loss:  0.33240431547164917\n",
      "Batch loss:  0.2895166575908661\n",
      "Batch loss:  0.2391388863325119\n",
      "Batch loss:  0.19222678244113922\n",
      "Batch loss:  0.2124258130788803\n",
      "Batch loss:  0.25810903310775757\n",
      "Batch loss:  0.2871420979499817\n",
      "Batch loss:  0.19234681129455566\n",
      "Batch loss:  0.2544667720794678\n",
      "Batch loss:  0.14312563836574554\n",
      "Batch loss:  0.35199856758117676\n",
      "Batch loss:  0.256429523229599\n",
      "Batch loss:  0.2142133265733719\n",
      "Batch loss:  0.26623183488845825\n",
      "Batch loss:  0.3087179362773895\n",
      "Batch loss:  0.3657715916633606\n",
      "Batch loss:  0.40048661828041077\n",
      "Batch loss:  0.21962271630764008\n",
      "Batch loss:  0.37954673171043396\n",
      "Batch loss:  0.5471278429031372\n",
      "Current average loss: 0.4641656962397562\n",
      "Batch loss:  0.3294646143913269\n",
      "Batch loss:  0.246256023645401\n",
      "Batch loss:  0.24323047697544098\n",
      "Batch loss:  0.17753101885318756\n",
      "Batch loss:  0.3556859493255615\n",
      "Batch loss:  0.4662359058856964\n",
      "Batch loss:  0.19201244413852692\n",
      "Batch loss:  0.33920034766197205\n",
      "Batch loss:  0.5064918994903564\n",
      "Batch loss:  0.24100114405155182\n",
      "Batch loss:  0.34836912155151367\n",
      "Batch loss:  0.3098912835121155\n",
      "Batch loss:  0.3215034604072571\n",
      "Batch loss:  0.3765844404697418\n",
      "Batch loss:  0.19532598555088043\n",
      "Batch loss:  0.2223028689622879\n",
      "Batch loss:  0.30079755187034607\n",
      "Batch loss:  0.1899288445711136\n",
      "Batch loss:  0.23813067376613617\n",
      "Batch loss:  0.3827785551548004\n",
      "Batch loss:  0.42789024114608765\n",
      "Batch loss:  0.3081337511539459\n",
      "Batch loss:  0.18357639014720917\n",
      "Batch loss:  0.3159496784210205\n",
      "Batch loss:  0.17871928215026855\n",
      "Batch loss:  0.29340726137161255\n",
      "Batch loss:  0.2189188003540039\n",
      "Batch loss:  0.32061389088630676\n",
      "Batch loss:  0.26134952902793884\n",
      "Batch loss:  0.22191868722438812\n",
      "Batch loss:  0.44264721870422363\n",
      "Batch loss:  0.3277772068977356\n",
      "Batch loss:  0.3527202308177948\n",
      "Batch loss:  0.22827619314193726\n",
      "Batch loss:  0.2684216797351837\n",
      "Batch loss:  0.2903349697589874\n",
      "Batch loss:  0.21962593495845795\n",
      "Batch loss:  0.17499110102653503\n",
      "Batch loss:  0.10134897381067276\n",
      "Batch loss:  0.27012959122657776\n",
      "Batch loss:  0.21466727554798126\n",
      "Batch loss:  0.30320608615875244\n",
      "Batch loss:  0.3681502640247345\n",
      "Batch loss:  0.1854253113269806\n",
      "Batch loss:  0.13798551261425018\n",
      "Batch loss:  0.29544398188591003\n",
      "Batch loss:  0.21581538021564484\n",
      "Batch loss:  0.22894498705863953\n",
      "Batch loss:  0.3301776051521301\n",
      "Batch loss:  0.19285957515239716\n",
      "Batch loss:  0.5280489921569824\n",
      "Batch loss:  0.17048509418964386\n",
      "Batch loss:  0.20332716405391693\n",
      "Batch loss:  0.2695888578891754\n",
      "Batch loss:  0.27361607551574707\n",
      "Batch loss:  0.16289930045604706\n",
      "Batch loss:  0.38560232520103455\n",
      "Batch loss:  0.48156461119651794\n",
      "Batch loss:  0.1705435812473297\n",
      "Batch loss:  0.2714858651161194\n",
      "Batch loss:  0.30425259470939636\n",
      "Batch loss:  0.327010840177536\n",
      "Batch loss:  0.2544930577278137\n",
      "Batch loss:  0.17078658938407898\n",
      "Batch loss:  0.29135698080062866\n",
      "Batch loss:  0.5369480848312378\n",
      "Batch loss:  0.24929821491241455\n",
      "Batch loss:  0.2256190925836563\n",
      "Batch loss:  0.153973788022995\n",
      "Batch loss:  0.3197542726993561\n",
      "Batch loss:  0.22633469104766846\n",
      "Batch loss:  0.25814107060432434\n",
      "Batch loss:  0.2590979039669037\n",
      "Batch loss:  0.45872998237609863\n",
      "Batch loss:  0.12382496893405914\n",
      "Batch loss:  0.21853038668632507\n",
      "Batch loss:  0.44022440910339355\n",
      "Batch loss:  0.18979822099208832\n",
      "Batch loss:  0.18619632720947266\n",
      "Batch loss:  0.3098851144313812\n",
      "Batch loss:  0.24120698869228363\n",
      "Batch loss:  0.27347105741500854\n",
      "Batch loss:  0.26030784845352173\n",
      "Batch loss:  0.25032761693000793\n",
      "Batch loss:  0.27779433131217957\n",
      "Batch loss:  0.18212559819221497\n",
      "Batch loss:  0.26783981919288635\n",
      "Batch loss:  0.18633200228214264\n",
      "Batch loss:  0.16876475512981415\n",
      "Batch loss:  0.19647139310836792\n",
      "Batch loss:  0.28378766775131226\n",
      "Batch loss:  0.42027971148490906\n",
      "Batch loss:  0.22531099617481232\n",
      "Batch loss:  0.2332519292831421\n",
      "Batch loss:  0.1870846450328827\n",
      "Batch loss:  0.14527486264705658\n",
      "Batch loss:  0.1890258640050888\n",
      "Batch loss:  0.19624640047550201\n",
      "Batch loss:  0.19859562814235687\n",
      "Batch loss:  0.464051753282547\n",
      "Current average loss: 0.46152421547062444\n",
      "Batch loss:  0.1772775799036026\n",
      "Batch loss:  0.1683066338300705\n",
      "Batch loss:  0.15525533258914948\n",
      "Batch loss:  0.2818288207054138\n",
      "Batch loss:  0.24453872442245483\n",
      "Batch loss:  0.34815359115600586\n",
      "Batch loss:  0.14578048884868622\n",
      "Batch loss:  0.16104359924793243\n",
      "Batch loss:  0.24788536131381989\n",
      "Batch loss:  0.29626935720443726\n",
      "Batch loss:  0.1784287542104721\n",
      "Batch loss:  0.1582510620355606\n",
      "Batch loss:  0.19766443967819214\n",
      "Batch loss:  0.3267233967781067\n",
      "Batch loss:  0.24981458485126495\n",
      "Batch loss:  0.30130383372306824\n",
      "Batch loss:  0.16390550136566162\n",
      "Batch loss:  0.15642733871936798\n",
      "Batch loss:  0.12622159719467163\n",
      "Batch loss:  0.16931599378585815\n",
      "Batch loss:  0.2653656005859375\n",
      "Batch loss:  0.2921922504901886\n",
      "Batch loss:  0.3409130573272705\n",
      "Batch loss:  0.13924045860767365\n",
      "Batch loss:  0.18443633615970612\n",
      "Batch loss:  0.3376724123954773\n",
      "Batch loss:  0.19919881224632263\n",
      "Batch loss:  0.19567899405956268\n",
      "Batch loss:  0.2441663295030594\n",
      "Batch loss:  0.2504754662513733\n",
      "Batch loss:  0.17119894921779633\n",
      "Batch loss:  0.2130262702703476\n",
      "Batch loss:  0.2622066140174866\n",
      "Batch loss:  0.14561428129673004\n",
      "Batch loss:  0.1721089482307434\n",
      "Batch loss:  0.3663502037525177\n",
      "Batch loss:  0.2650746703147888\n",
      "Batch loss:  0.24490201473236084\n",
      "Batch loss:  0.18634894490242004\n",
      "Batch loss:  0.27435192465782166\n",
      "Batch loss:  0.163546621799469\n",
      "Batch loss:  0.09164270758628845\n",
      "Batch loss:  0.1531318724155426\n",
      "Batch loss:  0.22878973186016083\n",
      "Batch loss:  0.23992185294628143\n",
      "Batch loss:  0.3133111298084259\n",
      "Batch loss:  0.17708785831928253\n",
      "Batch loss:  0.19189755618572235\n",
      "Batch loss:  0.28310874104499817\n",
      "Batch loss:  0.23870037496089935\n",
      "Batch loss:  0.25562238693237305\n",
      "Batch loss:  0.22559231519699097\n",
      "Batch loss:  0.5198783278465271\n",
      "Batch loss:  0.2227998524904251\n",
      "Batch loss:  0.25937485694885254\n",
      "Batch loss:  0.4367654025554657\n",
      "Batch loss:  0.22773674130439758\n",
      "Batch loss:  0.28266435861587524\n",
      "Batch loss:  0.32768890261650085\n",
      "Batch loss:  0.25841224193573\n",
      "Batch loss:  0.27799415588378906\n",
      "Batch loss:  0.11381792277097702\n",
      "Batch loss:  0.18021953105926514\n",
      "Batch loss:  0.31798937916755676\n",
      "Batch loss:  0.2643795907497406\n",
      "Batch loss:  0.40895095467567444\n",
      "Batch loss:  0.13869608938694\n",
      "Batch loss:  0.2825256884098053\n",
      "Batch loss:  0.26646146178245544\n",
      "Batch loss:  0.29319027066230774\n",
      "Batch loss:  0.14549632370471954\n",
      "Batch loss:  0.2693377137184143\n",
      "Batch loss:  0.19347462058067322\n",
      "Batch loss:  0.25821220874786377\n",
      "Batch loss:  0.1951802670955658\n",
      "Batch loss:  0.4010336399078369\n",
      "Batch loss:  0.2349315732717514\n",
      "Batch loss:  0.22310221195220947\n",
      "Batch loss:  0.154318705201149\n",
      "Batch loss:  0.16624900698661804\n",
      "Batch loss:  0.15340372920036316\n",
      "Batch loss:  0.18305212259292603\n",
      "Batch loss:  0.28026050329208374\n",
      "Batch loss:  0.538241446018219\n",
      "Batch loss:  0.350015789270401\n",
      "Batch loss:  0.20752494037151337\n",
      "Batch loss:  0.1652836948633194\n",
      "Batch loss:  0.15528926253318787\n",
      "Batch loss:  0.33059993386268616\n",
      "Batch loss:  0.2526857256889343\n",
      "Batch loss:  0.24378694593906403\n",
      "Batch loss:  0.20861674845218658\n",
      "Batch loss:  0.2628430724143982\n",
      "Batch loss:  0.20432062447071075\n",
      "Batch loss:  0.23354339599609375\n",
      "Batch loss:  0.45932650566101074\n",
      "Batch loss:  0.21869927644729614\n",
      "Batch loss:  0.17099367082118988\n",
      "Batch loss:  0.29603320360183716\n",
      "Batch loss:  0.42935216426849365\n",
      "Current average loss: 0.4585624001603113\n",
      "Batch loss:  0.12681500613689423\n",
      "Batch loss:  0.2536472976207733\n",
      "Batch loss:  0.3019252419471741\n",
      "Batch loss:  0.21509070694446564\n",
      "Batch loss:  0.22503772377967834\n",
      "Batch loss:  0.3804754316806793\n",
      "Batch loss:  0.28712233901023865\n",
      "Batch loss:  0.37623536586761475\n",
      "Batch loss:  0.33644577860832214\n",
      "Batch loss:  0.3758130669593811\n",
      "Batch loss:  0.3750916123390198\n",
      "Batch loss:  0.22674214839935303\n",
      "Batch loss:  0.2068966031074524\n",
      "Batch loss:  0.29577523469924927\n",
      "Batch loss:  0.4028419256210327\n",
      "Batch loss:  0.24560128152370453\n",
      "Batch loss:  0.41829901933670044\n",
      "Batch loss:  0.2258058339357376\n",
      "Batch loss:  0.32216185331344604\n",
      "Batch loss:  0.1778394877910614\n",
      "Batch loss:  0.12233659625053406\n",
      "Batch loss:  0.20562219619750977\n",
      "Batch loss:  0.3730364441871643\n",
      "Batch loss:  0.1713206022977829\n",
      "Batch loss:  0.4643261432647705\n",
      "Batch loss:  0.317099004983902\n",
      "Batch loss:  0.45458030700683594\n",
      "Batch loss:  0.38258716464042664\n",
      "Batch loss:  0.5037277936935425\n",
      "Batch loss:  0.3037126660346985\n",
      "Batch loss:  0.2809825539588928\n",
      "Batch loss:  0.27082887291908264\n",
      "Batch loss:  0.22430884838104248\n",
      "Batch loss:  0.3340294361114502\n",
      "Batch loss:  0.16475588083267212\n",
      "Batch loss:  0.2639122009277344\n",
      "Batch loss:  0.23764371871948242\n",
      "Batch loss:  0.23762467503547668\n",
      "Batch loss:  0.32018956542015076\n",
      "Batch loss:  0.1614370495080948\n",
      "Batch loss:  0.2139555811882019\n",
      "Batch loss:  0.33349207043647766\n",
      "Batch loss:  0.2784962058067322\n",
      "Batch loss:  0.20142582058906555\n",
      "Batch loss:  0.3230670392513275\n",
      "Batch loss:  0.2526918351650238\n",
      "Batch loss:  0.24788999557495117\n",
      "Batch loss:  0.25139081478118896\n",
      "Batch loss:  0.280535489320755\n",
      "Batch loss:  0.1955285370349884\n",
      "Batch loss:  0.3152434527873993\n",
      "Batch loss:  0.18595528602600098\n",
      "Batch loss:  0.2895282804965973\n",
      "Batch loss:  0.24454335868358612\n",
      "Batch loss:  0.1181151419878006\n",
      "Batch loss:  0.2570074200630188\n",
      "Batch loss:  0.25958311557769775\n",
      "Batch loss:  0.27605006098747253\n",
      "Batch loss:  0.3365926444530487\n",
      "Batch loss:  0.4089955985546112\n",
      "Batch loss:  0.3315393030643463\n",
      "Batch loss:  0.34817564487457275\n",
      "Batch loss:  0.20330575108528137\n",
      "Batch loss:  0.43234783411026\n",
      "Batch loss:  0.18621115386486053\n",
      "Batch loss:  0.2905670702457428\n",
      "Batch loss:  0.49294644594192505\n",
      "Batch loss:  0.27231651544570923\n",
      "Batch loss:  0.18662287294864655\n",
      "Batch loss:  0.20419929921627045\n",
      "Batch loss:  0.44087618589401245\n",
      "Batch loss:  0.1460147351026535\n",
      "Batch loss:  0.14576828479766846\n",
      "Batch loss:  0.25558480620384216\n",
      "Batch loss:  0.21917086839675903\n",
      "Batch loss:  0.30575019121170044\n",
      "Batch loss:  0.1955588161945343\n",
      "Batch loss:  0.31653404235839844\n",
      "Batch loss:  0.3045874238014221\n",
      "Batch loss:  0.24126318097114563\n",
      "Batch loss:  0.26593896746635437\n",
      "Batch loss:  0.49995261430740356\n",
      "Batch loss:  0.2018635869026184\n",
      "Batch loss:  0.1874825358390808\n",
      "Batch loss:  0.2982522249221802\n",
      "Batch loss:  0.29114603996276855\n",
      "Batch loss:  0.4912206828594208\n",
      "Batch loss:  0.1313585788011551\n",
      "Batch loss:  0.1974332630634308\n",
      "Batch loss:  0.3314913809299469\n",
      "Batch loss:  0.17292048037052155\n",
      "Batch loss:  0.28306353092193604\n",
      "Batch loss:  0.32730522751808167\n",
      "Batch loss:  0.30156823992729187\n",
      "Batch loss:  0.18484488129615784\n",
      "Batch loss:  0.19720807671546936\n",
      "Batch loss:  0.30304422974586487\n",
      "Batch loss:  0.19882217049598694\n",
      "Batch loss:  0.17823347449302673\n",
      "Batch loss:  0.40559497475624084\n",
      "Current average loss: 0.4561562753767828\n",
      "Batch loss:  0.3045749366283417\n",
      "Batch loss:  0.20771358907222748\n",
      "Batch loss:  0.1704031527042389\n",
      "Batch loss:  0.5090304613113403\n",
      "Batch loss:  0.18092145025730133\n",
      "Batch loss:  0.1126423180103302\n",
      "Batch loss:  0.28328022360801697\n",
      "Batch loss:  0.164607435464859\n",
      "Batch loss:  0.1569455862045288\n",
      "Batch loss:  0.418654203414917\n",
      "Batch loss:  0.17448961734771729\n",
      "Batch loss:  0.28031647205352783\n",
      "Batch loss:  0.3251747786998749\n",
      "Batch loss:  0.30796852707862854\n",
      "Batch loss:  0.4205637276172638\n",
      "Batch loss:  0.15964274108409882\n",
      "Batch loss:  0.20168593525886536\n",
      "Batch loss:  0.308790922164917\n",
      "Batch loss:  0.13236430287361145\n",
      "Batch loss:  0.2630217671394348\n",
      "Batch loss:  0.3203123211860657\n",
      "Batch loss:  0.29614901542663574\n",
      "Batch loss:  0.3636402189731598\n",
      "Batch loss:  0.2605058252811432\n",
      "Batch loss:  0.2433871030807495\n",
      "Batch loss:  0.3218529522418976\n",
      "Batch loss:  0.43189939856529236\n",
      "Batch loss:  0.3109731078147888\n",
      "Batch loss:  0.2816714644432068\n",
      "Batch loss:  0.16966162621974945\n",
      "Batch loss:  0.27452316880226135\n",
      "Batch loss:  0.2657668888568878\n",
      "Batch loss:  0.29534003138542175\n",
      "Batch loss:  0.3418922424316406\n",
      "Batch loss:  0.1707957535982132\n",
      "Batch loss:  0.2150537520647049\n",
      "Batch loss:  0.27832064032554626\n",
      "Batch loss:  0.15835803747177124\n",
      "Batch loss:  0.172064870595932\n",
      "Batch loss:  0.22001124918460846\n",
      "Batch loss:  0.28917810320854187\n",
      "Batch loss:  0.24732144176959991\n",
      "Batch loss:  0.1676558256149292\n",
      "Batch loss:  0.19629830121994019\n",
      "Batch loss:  0.25599509477615356\n",
      "Batch loss:  0.2964482307434082\n",
      "Batch loss:  0.18943338096141815\n",
      "Batch loss:  0.18458490073680878\n",
      "Batch loss:  0.261079877614975\n",
      "Batch loss:  0.23430921137332916\n",
      "Batch loss:  0.4280911684036255\n",
      "Batch loss:  0.2261013239622116\n",
      "Batch loss:  0.20362588763237\n",
      "Batch loss:  0.2584984302520752\n",
      "Batch loss:  0.11001462489366531\n",
      "Batch loss:  0.19172589480876923\n",
      "Batch loss:  0.1802971214056015\n",
      "Batch loss:  0.2211669534444809\n",
      "Batch loss:  0.20693662762641907\n",
      "Batch loss:  0.3483664393424988\n",
      "Batch loss:  0.2380381077528\n",
      "Batch loss:  0.24075199663639069\n",
      "Batch loss:  0.28180667757987976\n",
      "Batch loss:  0.19168280065059662\n",
      "Batch loss:  0.25724437832832336\n",
      "Batch loss:  0.2845212519168854\n",
      "Batch loss:  0.10247975587844849\n",
      "Batch loss:  0.18713246285915375\n",
      "Batch loss:  0.203026682138443\n",
      "Batch loss:  0.2150534838438034\n",
      "Batch loss:  0.4318528175354004\n",
      "Batch loss:  0.475005567073822\n",
      "Batch loss:  0.20161575078964233\n",
      "Batch loss:  0.1378125250339508\n",
      "Batch loss:  0.10237926244735718\n",
      "Batch loss:  0.2909030020236969\n",
      "Batch loss:  0.27794837951660156\n",
      "Batch loss:  0.3618757724761963\n",
      "Batch loss:  0.2697751224040985\n",
      "Batch loss:  0.3266693651676178\n",
      "Batch loss:  0.21670164167881012\n",
      "Batch loss:  0.3595986068248749\n",
      "Batch loss:  0.1318952441215515\n",
      "Batch loss:  0.22741515934467316\n",
      "Batch loss:  0.3621399402618408\n",
      "Batch loss:  0.29613661766052246\n",
      "Batch loss:  0.24146880209445953\n",
      "Batch loss:  0.330032080411911\n",
      "Batch loss:  0.25092828273773193\n",
      "Batch loss:  0.21890822052955627\n",
      "Batch loss:  0.2176690697669983\n",
      "Batch loss:  0.22017528116703033\n",
      "Batch loss:  0.25122278928756714\n",
      "Batch loss:  0.1275385618209839\n",
      "Batch loss:  0.1312384456396103\n",
      "Batch loss:  0.3576412498950958\n",
      "Batch loss:  0.42761921882629395\n",
      "Batch loss:  0.22665919363498688\n",
      "Batch loss:  0.266212522983551\n",
      "Batch loss:  0.2867943346500397\n",
      "Current average loss: 0.4534963679361102\n",
      "Batch loss:  0.2751885652542114\n",
      "Batch loss:  0.17499211430549622\n",
      "Batch loss:  0.515690267086029\n",
      "Batch loss:  0.22195182740688324\n",
      "Batch loss:  0.18174660205841064\n",
      "Batch loss:  0.1893191933631897\n",
      "Batch loss:  0.44555842876434326\n",
      "Batch loss:  0.471586138010025\n",
      "Batch loss:  0.170183002948761\n",
      "Batch loss:  0.2559610903263092\n",
      "Batch loss:  0.2631555497646332\n",
      "Batch loss:  0.35737812519073486\n",
      "Batch loss:  0.1855442076921463\n",
      "Batch loss:  0.1528896540403366\n",
      "Batch loss:  0.22223997116088867\n",
      "Batch loss:  0.25476956367492676\n",
      "Batch loss:  0.1932556927204132\n",
      "Batch loss:  0.11657164245843887\n",
      "Batch loss:  0.472758024930954\n",
      "Batch loss:  0.5396952629089355\n",
      "Batch loss:  0.23511561751365662\n",
      "Batch loss:  0.3151274025440216\n",
      "Batch loss:  0.1115693747997284\n",
      "Batch loss:  0.25558510422706604\n",
      "Batch loss:  0.40567290782928467\n",
      "Batch loss:  0.18065829575061798\n",
      "Batch loss:  0.2131231725215912\n",
      "Batch loss:  0.25539904832839966\n",
      "Batch loss:  0.25826767086982727\n",
      "Batch loss:  0.2406686544418335\n",
      "Batch loss:  0.281302273273468\n",
      "Batch loss:  0.32788601517677307\n",
      "Batch loss:  0.2222059965133667\n",
      "Batch loss:  0.20655812323093414\n",
      "Batch loss:  0.3314761221408844\n",
      "Batch loss:  0.32212522625923157\n",
      "Batch loss:  0.1426972895860672\n",
      "Batch loss:  0.2336001992225647\n",
      "Batch loss:  0.18964844942092896\n",
      "Batch loss:  0.3325687050819397\n",
      "Batch loss:  0.34345367550849915\n",
      "Batch loss:  0.11125887930393219\n",
      "Batch loss:  0.3040498197078705\n",
      "Batch loss:  0.2266809195280075\n",
      "Batch loss:  0.28499990701675415\n",
      "Batch loss:  0.32550710439682007\n",
      "Batch loss:  0.17236968874931335\n",
      "Batch loss:  0.572877049446106\n",
      "Batch loss:  0.155324324965477\n",
      "Batch loss:  0.2876240015029907\n",
      "Batch loss:  0.20031008124351501\n",
      "Batch loss:  0.4607366621494293\n",
      "Batch loss:  0.19789935648441315\n",
      "Batch loss:  0.2004038393497467\n",
      "Batch loss:  0.2883914113044739\n",
      "Batch loss:  0.18200840055942535\n",
      "Batch loss:  0.2313642054796219\n",
      "Batch loss:  0.2662372887134552\n",
      "Batch loss:  0.18313296139240265\n",
      "Batch loss:  0.33139747381210327\n",
      "Batch loss:  0.21617577970027924\n",
      "Batch loss:  0.4318912923336029\n",
      "Batch loss:  0.17076720297336578\n",
      "Batch loss:  0.19286811351776123\n",
      "Batch loss:  0.348524808883667\n",
      "Batch loss:  0.2828102707862854\n",
      "Batch loss:  0.20290507376194\n",
      "Batch loss:  0.3020230233669281\n",
      "Batch loss:  0.193626269698143\n",
      "Batch loss:  0.2585238814353943\n",
      "Batch loss:  0.18861009180545807\n",
      "Batch loss:  0.42874330282211304\n",
      "Batch loss:  0.2752062976360321\n",
      "Batch loss:  0.28184792399406433\n",
      "Batch loss:  0.2748536765575409\n",
      "Batch loss:  0.2544700503349304\n",
      "Batch loss:  0.17957955598831177\n",
      "Batch loss:  0.2509971261024475\n",
      "Batch loss:  0.35124823451042175\n",
      "Batch loss:  0.2126745581626892\n",
      "Batch loss:  0.21924588084220886\n",
      "Batch loss:  0.2656148374080658\n",
      "Batch loss:  0.3303544521331787\n",
      "Batch loss:  0.11992143094539642\n",
      "Batch loss:  0.5066961646080017\n",
      "Batch loss:  0.13585157692432404\n",
      "Batch loss:  0.27872949838638306\n",
      "Batch loss:  0.3407857120037079\n",
      "Batch loss:  0.5444239974021912\n",
      "Batch loss:  0.34355464577674866\n",
      "Batch loss:  0.21352256834506989\n",
      "Batch loss:  0.14902813732624054\n",
      "Batch loss:  0.11409065872430801\n",
      "Batch loss:  0.2313857078552246\n",
      "Batch loss:  0.4805961847305298\n",
      "Batch loss:  0.32257258892059326\n",
      "Batch loss:  0.20644859969615936\n",
      "Batch loss:  0.23495855927467346\n",
      "Batch loss:  0.18482303619384766\n",
      "Batch loss:  0.3384767174720764\n",
      "Current average loss: 0.45110155016168374\n",
      "Batch loss:  0.2420753538608551\n",
      "Batch loss:  0.32737621665000916\n",
      "Batch loss:  0.24134767055511475\n",
      "Batch loss:  0.26315420866012573\n",
      "Batch loss:  0.22894896566867828\n",
      "Batch loss:  0.2359900027513504\n",
      "Batch loss:  0.17163950204849243\n",
      "Batch loss:  0.2390918880701065\n",
      "Batch loss:  0.1498769074678421\n",
      "Batch loss:  0.2354360818862915\n",
      "Batch loss:  0.2903881371021271\n",
      "Batch loss:  0.19263219833374023\n",
      "Batch loss:  0.31422701478004456\n",
      "Batch loss:  0.21051660180091858\n",
      "Batch loss:  0.22439555823802948\n",
      "Batch loss:  0.1025073230266571\n",
      "Batch loss:  0.3027753233909607\n",
      "Batch loss:  0.24969923496246338\n",
      "Batch loss:  0.18743252754211426\n",
      "Batch loss:  0.1598050594329834\n",
      "Batch loss:  0.2662006616592407\n",
      "Batch loss:  0.27589595317840576\n",
      "Batch loss:  0.3796271085739136\n",
      "Batch loss:  0.23307205736637115\n",
      "Batch loss:  0.16056494414806366\n",
      "Batch loss:  0.12243860960006714\n",
      "Batch loss:  0.24662312865257263\n",
      "Batch loss:  0.21097543835639954\n",
      "Batch loss:  0.14893309772014618\n",
      "Batch loss:  0.188794806599617\n",
      "Batch loss:  0.34018227458000183\n",
      "Batch loss:  0.10840582102537155\n",
      "Batch loss:  0.3766968548297882\n",
      "Batch loss:  0.1107422411441803\n",
      "Batch loss:  0.2190319150686264\n",
      "Batch loss:  0.26580044627189636\n",
      "Batch loss:  0.21073420345783234\n",
      "Batch loss:  0.1260136514902115\n",
      "Batch loss:  0.26593974232673645\n",
      "Batch loss:  0.24174322187900543\n",
      "Batch loss:  0.32692432403564453\n",
      "Batch loss:  0.3394527733325958\n",
      "Batch loss:  0.32117199897766113\n",
      "Batch loss:  0.22776399552822113\n",
      "Batch loss:  0.2620244026184082\n",
      "Batch loss:  0.18048113584518433\n",
      "Batch loss:  0.20050637423992157\n",
      "Batch loss:  0.23651883006095886\n",
      "Batch loss:  0.18205511569976807\n",
      "Batch loss:  0.11830278486013412\n",
      "Batch loss:  0.19631189107894897\n",
      "Batch loss:  0.22410538792610168\n",
      "Batch loss:  0.26083147525787354\n",
      "Batch loss:  0.19848333299160004\n",
      "Batch loss:  0.21238093078136444\n",
      "Batch loss:  0.22146356105804443\n",
      "Batch loss:  0.2196684032678604\n",
      "Batch loss:  0.2788669168949127\n",
      "Batch loss:  0.13623732328414917\n",
      "Batch loss:  0.20954538881778717\n",
      "Batch loss:  0.19242964684963226\n",
      "Batch loss:  0.8717700839042664\n",
      "Batch loss:  0.26758667826652527\n",
      "Batch loss:  0.32351937890052795\n",
      "Batch loss:  0.279489666223526\n",
      "Batch loss:  0.15791146457195282\n",
      "Batch loss:  0.3303357660770416\n",
      "Batch loss:  0.33250346779823303\n",
      "Batch loss:  0.3778124749660492\n",
      "Batch loss:  0.3232191205024719\n",
      "Batch loss:  0.22651296854019165\n",
      "Batch loss:  0.1833042949438095\n",
      "Batch loss:  0.40636029839515686\n",
      "Batch loss:  0.20867377519607544\n",
      "Batch loss:  0.49961385130882263\n",
      "Batch loss:  0.21041202545166016\n",
      "Batch loss:  0.3441672921180725\n",
      "Batch loss:  0.3074984550476074\n",
      "Batch loss:  0.20790044963359833\n",
      "Batch loss:  0.23868167400360107\n",
      "Batch loss:  0.20097318291664124\n",
      "Batch loss:  0.39506518840789795\n",
      "Batch loss:  0.37156563997268677\n",
      "Batch loss:  0.1171356737613678\n",
      "Batch loss:  0.16292516887187958\n",
      "Batch loss:  0.29032889008522034\n",
      "Batch loss:  0.11470243334770203\n",
      "Batch loss:  0.2088918834924698\n",
      "Batch loss:  0.3775556981563568\n",
      "Batch loss:  0.14875471591949463\n",
      "Batch loss:  0.30142176151275635\n",
      "Batch loss:  0.7000836730003357\n",
      "Batch loss:  0.286127507686615\n",
      "Batch loss:  0.35268092155456543\n",
      "Batch loss:  0.36132803559303284\n",
      "Batch loss:  0.2193661332130432\n",
      "Batch loss:  0.15399424731731415\n",
      "Batch loss:  0.2407436966896057\n",
      "Batch loss:  0.28403976559638977\n",
      "Batch loss:  0.25452011823654175\n",
      "Current average loss: 0.4485816912272621\n",
      "Batch loss:  0.3400891125202179\n",
      "Batch loss:  0.33469581604003906\n",
      "Batch loss:  0.1419154554605484\n",
      "Batch loss:  0.465447336435318\n",
      "Batch loss:  0.2665084898471832\n",
      "Batch loss:  0.19710704684257507\n",
      "Batch loss:  0.3103604316711426\n",
      "Batch loss:  0.20239493250846863\n",
      "Batch loss:  0.32294583320617676\n",
      "Batch loss:  0.3959052562713623\n",
      "Batch loss:  0.16723692417144775\n",
      "Batch loss:  0.2588313817977905\n",
      "Batch loss:  0.4244607388973236\n",
      "Batch loss:  0.20170235633850098\n",
      "Batch loss:  0.2983565032482147\n",
      "Batch loss:  0.284132719039917\n",
      "Batch loss:  0.40126389265060425\n",
      "Batch loss:  0.17729367315769196\n",
      "Batch loss:  0.17607976496219635\n",
      "Batch loss:  0.25276994705200195\n",
      "Batch loss:  0.22735057771205902\n",
      "Batch loss:  0.5360414385795593\n",
      "Batch loss:  0.23388385772705078\n",
      "Batch loss:  0.34596899151802063\n",
      "Batch loss:  0.23702998459339142\n",
      "Batch loss:  0.34713947772979736\n",
      "Batch loss:  0.41957998275756836\n",
      "Batch loss:  0.2413761466741562\n",
      "Batch loss:  0.27624407410621643\n",
      "Batch loss:  0.2636168897151947\n",
      "Batch loss:  0.32598182559013367\n",
      "Batch loss:  0.15862055122852325\n",
      "Batch loss:  0.13378973305225372\n",
      "Batch loss:  0.24696624279022217\n",
      "Batch loss:  0.33986353874206543\n",
      "Batch loss:  0.3335675001144409\n",
      "Batch loss:  0.23022356629371643\n",
      "Batch loss:  0.335947722196579\n",
      "Batch loss:  0.23918737471103668\n",
      "Batch loss:  0.34361717104911804\n",
      "Batch loss:  0.3142711818218231\n",
      "Batch loss:  0.24137546122074127\n",
      "Batch loss:  0.24931205809116364\n",
      "Batch loss:  0.2636984884738922\n",
      "Batch loss:  0.16572020947933197\n",
      "Batch loss:  0.3383007049560547\n",
      "Batch loss:  0.5822155475616455\n",
      "Batch loss:  0.18383264541625977\n",
      "Batch loss:  0.3132361173629761\n",
      "Batch loss:  0.3065958023071289\n",
      "Batch loss:  0.26599588990211487\n",
      "Batch loss:  0.20538164675235748\n",
      "Batch loss:  0.2430524379014969\n",
      "Batch loss:  0.2937904894351959\n",
      "Batch loss:  0.2163306474685669\n",
      "Batch loss:  0.30831021070480347\n",
      "Batch loss:  0.26476582884788513\n",
      "Batch loss:  0.41025349497795105\n",
      "Batch loss:  0.2564166486263275\n",
      "Batch loss:  0.29635342955589294\n",
      "Batch loss:  0.3126566708087921\n",
      "Batch loss:  0.19070816040039062\n",
      "Batch loss:  0.14065714180469513\n",
      "Batch loss:  0.1988094598054886\n",
      "Batch loss:  0.13421952724456787\n",
      "Batch loss:  0.35593506693840027\n",
      "Batch loss:  0.1275874674320221\n",
      "Batch loss:  0.4197918474674225\n",
      "Batch loss:  0.2747536301612854\n",
      "Batch loss:  0.28161191940307617\n",
      "Batch loss:  0.2365221232175827\n",
      "Batch loss:  0.3282109498977661\n",
      "Batch loss:  0.26529866456985474\n",
      "Batch loss:  0.3098936378955841\n",
      "Batch loss:  0.1716039776802063\n",
      "Batch loss:  0.4634398818016052\n",
      "Batch loss:  0.26513904333114624\n",
      "Batch loss:  0.20462726056575775\n",
      "Batch loss:  0.2941599488258362\n",
      "Batch loss:  0.32789814472198486\n",
      "Batch loss:  0.20428545773029327\n",
      "Batch loss:  0.2743600010871887\n",
      "Batch loss:  0.41224855184555054\n",
      "Batch loss:  0.3470100164413452\n",
      "Batch loss:  0.29628491401672363\n",
      "Batch loss:  0.3012562096118927\n",
      "Batch loss:  0.42114102840423584\n",
      "Batch loss:  0.3628540635108948\n",
      "Batch loss:  0.2775885760784149\n",
      "Batch loss:  0.30182504653930664\n",
      "Batch loss:  0.5163322687149048\n",
      "Batch loss:  0.2774111032485962\n",
      "Batch loss:  0.19635231792926788\n",
      "Batch loss:  0.14271937310695648\n",
      "Batch loss:  0.14472636580467224\n",
      "Batch loss:  0.3367452919483185\n",
      "Batch loss:  0.3727169930934906\n",
      "Batch loss:  0.2658339738845825\n",
      "Batch loss:  0.14055368304252625\n",
      "Batch loss:  0.17661337554454803\n",
      "Current average loss: 0.4464767573217684\n",
      "Batch loss:  0.3001503646373749\n",
      "Batch loss:  0.13117274641990662\n",
      "Batch loss:  0.29414102435112\n",
      "Batch loss:  0.15200594067573547\n",
      "Batch loss:  0.23074835538864136\n",
      "Batch loss:  0.21119509637355804\n",
      "Batch loss:  0.23228056728839874\n",
      "Batch loss:  0.23726843297481537\n",
      "Batch loss:  0.16693484783172607\n",
      "Batch loss:  0.28174424171447754\n",
      "Batch loss:  0.26122090220451355\n",
      "Batch loss:  0.21912480890750885\n",
      "Batch loss:  0.3714612126350403\n",
      "Batch loss:  0.21351182460784912\n",
      "Batch loss:  0.6062960028648376\n",
      "Batch loss:  0.13886089622974396\n",
      "Batch loss:  0.26902225613594055\n",
      "Batch loss:  0.29973191022872925\n",
      "Batch loss:  0.21472683548927307\n",
      "Batch loss:  0.28799715638160706\n",
      "Batch loss:  0.19169317185878754\n",
      "Batch loss:  0.24030669033527374\n",
      "Batch loss:  0.24763014912605286\n",
      "Batch loss:  0.24989622831344604\n",
      "Batch loss:  0.27997589111328125\n",
      "Batch loss:  0.1616179496049881\n",
      "Batch loss:  0.21998338401317596\n",
      "Batch loss:  0.24111595749855042\n",
      "Batch loss:  0.17979696393013\n",
      "Batch loss:  0.33188390731811523\n",
      "Batch loss:  0.5349892973899841\n",
      "Batch loss:  0.22847220301628113\n",
      "Batch loss:  0.30160942673683167\n",
      "Batch loss:  0.19389306008815765\n",
      "Batch loss:  0.32161176204681396\n",
      "Batch loss:  0.36529144644737244\n",
      "Batch loss:  0.2653566896915436\n",
      "Batch loss:  0.29037243127822876\n",
      "Batch loss:  0.33391082286834717\n",
      "Batch loss:  0.2140164077281952\n",
      "Batch loss:  0.2293076068162918\n",
      "Batch loss:  0.1387770026922226\n",
      "Batch loss:  0.3074786365032196\n",
      "Batch loss:  0.25658687949180603\n",
      "Batch loss:  0.1703702211380005\n",
      "Batch loss:  0.14805172383785248\n",
      "Batch loss:  0.39376065135002136\n",
      "Batch loss:  0.22646959125995636\n",
      "Batch loss:  0.368596613407135\n",
      "Batch loss:  0.1791934370994568\n",
      "Batch loss:  0.20181258022785187\n",
      "Batch loss:  0.19613727927207947\n",
      "Batch loss:  0.27458178997039795\n",
      "Batch loss:  0.1847577840089798\n",
      "Batch loss:  0.26263487339019775\n",
      "Batch loss:  0.2332252860069275\n",
      "Batch loss:  0.3258759081363678\n",
      "Batch loss:  0.1933615654706955\n",
      "Batch loss:  0.21499089896678925\n",
      "Batch loss:  0.13037250936031342\n",
      "Batch loss:  0.20775334537029266\n",
      "Batch loss:  0.48706236481666565\n",
      "Batch loss:  0.2557290494441986\n",
      "Batch loss:  0.3051711320877075\n",
      "Batch loss:  0.395999550819397\n",
      "Batch loss:  0.40587320923805237\n",
      "Batch loss:  0.33261361718177795\n",
      "Batch loss:  0.28374555706977844\n",
      "Batch loss:  0.13724730908870697\n",
      "Batch loss:  0.22072654962539673\n",
      "Batch loss:  0.20901170372962952\n",
      "Batch loss:  0.25479069352149963\n",
      "Batch loss:  0.2428399622440338\n",
      "Batch loss:  0.20085158944129944\n",
      "Batch loss:  0.2989003360271454\n",
      "Batch loss:  0.21451038122177124\n",
      "Batch loss:  0.29538416862487793\n",
      "Batch loss:  0.34433311223983765\n",
      "Batch loss:  0.420864075422287\n",
      "Batch loss:  0.37013334035873413\n",
      "Batch loss:  0.2032984048128128\n",
      "Batch loss:  0.30327510833740234\n",
      "Batch loss:  0.32041415572166443\n",
      "Batch loss:  0.18717923760414124\n",
      "Batch loss:  0.2099166214466095\n",
      "Batch loss:  0.32244986295700073\n",
      "Batch loss:  0.2445637583732605\n",
      "Batch loss:  0.21181213855743408\n",
      "Batch loss:  0.28273719549179077\n",
      "Batch loss:  0.1551767736673355\n",
      "Batch loss:  0.44341525435447693\n",
      "Batch loss:  0.29564568400382996\n",
      "Batch loss:  0.32513970136642456\n",
      "Batch loss:  0.18972136080265045\n",
      "Batch loss:  0.16672620177268982\n",
      "Batch loss:  0.28662654757499695\n",
      "Batch loss:  0.17253291606903076\n",
      "Batch loss:  0.15932969748973846\n",
      "Batch loss:  0.21736940741539001\n",
      "Batch loss:  0.21328310668468475\n",
      "Current average loss: 0.4441390296067391\n",
      "Batch loss:  0.2143615186214447\n",
      "Batch loss:  0.2956372797489166\n",
      "Batch loss:  0.4476943016052246\n",
      "Batch loss:  0.44423288106918335\n",
      "Batch loss:  0.353593647480011\n",
      "Batch loss:  0.222747802734375\n",
      "Batch loss:  0.3392678499221802\n",
      "Batch loss:  0.23197658360004425\n",
      "Batch loss:  0.20314210653305054\n",
      "Batch loss:  0.34017083048820496\n",
      "Batch loss:  0.19974106550216675\n",
      "Batch loss:  0.1850060373544693\n",
      "Batch loss:  0.16995838284492493\n",
      "Batch loss:  0.2266644686460495\n",
      "Batch loss:  0.29026198387145996\n",
      "Batch loss:  0.29341885447502136\n",
      "Batch loss:  0.26337236166000366\n",
      "Batch loss:  0.38719338178634644\n",
      "Batch loss:  0.2460528314113617\n",
      "Batch loss:  0.3012041449546814\n",
      "Batch loss:  0.2657659351825714\n",
      "Batch loss:  0.20596781373023987\n",
      "Batch loss:  0.2025027573108673\n",
      "Batch loss:  0.2765609323978424\n",
      "Batch loss:  0.1890830397605896\n",
      "Batch loss:  0.39203959703445435\n",
      "Batch loss:  0.23202988505363464\n",
      "Batch loss:  0.27928727865219116\n",
      "Batch loss:  0.3803282380104065\n",
      "Batch loss:  0.2244596630334854\n",
      "Batch loss:  0.17903712391853333\n",
      "Batch loss:  0.22260479629039764\n",
      "Batch loss:  0.2840898334980011\n",
      "Batch loss:  0.1537138670682907\n",
      "Batch loss:  0.2733295261859894\n",
      "Batch loss:  0.14680729806423187\n",
      "Batch loss:  0.2263280302286148\n",
      "Batch loss:  0.4800630509853363\n",
      "Batch loss:  0.2284913808107376\n",
      "Batch loss:  0.24999824166297913\n",
      "Batch loss:  0.22082971036434174\n",
      "Batch loss:  0.31315308809280396\n",
      "Batch loss:  0.23880672454833984\n",
      "Batch loss:  0.23927973210811615\n",
      "Batch loss:  0.2592943012714386\n",
      "Batch loss:  0.26427388191223145\n",
      "Batch loss:  0.2347392737865448\n",
      "Batch loss:  0.3560512065887451\n",
      "Batch loss:  0.20552630722522736\n",
      "Batch loss:  0.24730287492275238\n",
      "Batch loss:  0.16128812730312347\n",
      "Batch loss:  0.1287986934185028\n",
      "Batch loss:  0.408367782831192\n",
      "Batch loss:  0.3416072428226471\n",
      "Batch loss:  0.3261231780052185\n",
      "Batch loss:  0.17079898715019226\n",
      "Batch loss:  0.38148611783981323\n",
      "Batch loss:  0.2939058244228363\n",
      "Batch loss:  0.3014974296092987\n",
      "Batch loss:  0.28737872838974\n",
      "Batch loss:  0.2787611782550812\n",
      "Batch loss:  0.20288600027561188\n",
      "Batch loss:  0.2317046821117401\n",
      "Batch loss:  0.1926746517419815\n",
      "Batch loss:  0.2726929187774658\n",
      "Batch loss:  0.14968055486679077\n",
      "Batch loss:  0.1935781091451645\n",
      "Batch loss:  0.24937757849693298\n",
      "Batch loss:  0.2880104184150696\n",
      "Batch loss:  0.29929953813552856\n",
      "Batch loss:  0.13497193157672882\n",
      "Batch loss:  0.1588849276304245\n",
      "Batch loss:  0.2550015449523926\n",
      "Batch loss:  0.2680751085281372\n",
      "Batch loss:  0.40284839272499084\n",
      "Batch loss:  0.25605520606040955\n",
      "Batch loss:  0.26991036534309387\n",
      "Batch loss:  0.2090063989162445\n",
      "Batch loss:  0.3153133988380432\n",
      "Batch loss:  0.4634471535682678\n",
      "Batch loss:  0.20341138541698456\n",
      "Batch loss:  0.17247360944747925\n",
      "Batch loss:  0.4102327227592468\n",
      "Batch loss:  0.22766369581222534\n",
      "Batch loss:  0.24003759026527405\n",
      "Batch loss:  0.2241794764995575\n",
      "Batch loss:  0.20911389589309692\n",
      "Batch loss:  0.17982257902622223\n",
      "Batch loss:  0.24429957568645477\n",
      "Batch loss:  0.22094759345054626\n",
      "Batch loss:  0.42101797461509705\n",
      "Batch loss:  0.11599189043045044\n",
      "Batch loss:  0.31416648626327515\n",
      "Batch loss:  0.2840181589126587\n",
      "Batch loss:  0.23162047564983368\n",
      "Batch loss:  0.20003636181354523\n",
      "Batch loss:  0.13832055032253265\n",
      "Batch loss:  0.1781155914068222\n",
      "Batch loss:  0.23277555406093597\n",
      "Batch loss:  0.24348151683807373\n",
      "Current average loss: 0.4418552004002286\n",
      "Batch loss:  0.1680750995874405\n",
      "Batch loss:  0.4415087103843689\n",
      "Batch loss:  0.2975560128688812\n",
      "Batch loss:  0.32996904850006104\n",
      "Batch loss:  0.19879703223705292\n",
      "Batch loss:  0.20518088340759277\n",
      "Batch loss:  0.4137979745864868\n",
      "Batch loss:  0.26701271533966064\n",
      "Batch loss:  0.3920954465866089\n",
      "Batch loss:  0.2027851790189743\n",
      "Batch loss:  0.18918198347091675\n",
      "Batch loss:  0.28644248843193054\n",
      "Batch loss:  0.2735976576805115\n",
      "Batch loss:  0.34339508414268494\n",
      "Batch loss:  0.27233389019966125\n",
      "Batch loss:  0.31731176376342773\n",
      "Batch loss:  0.15803250670433044\n",
      "Batch loss:  0.1519094854593277\n",
      "Batch loss:  0.3075522780418396\n",
      "Batch loss:  0.33771049976348877\n",
      "Batch loss:  0.2299722582101822\n",
      "Batch loss:  0.19739772379398346\n",
      "Batch loss:  0.24236315488815308\n",
      "Batch loss:  0.15204651653766632\n",
      "Batch loss:  0.14049357175827026\n",
      "Batch loss:  0.224226713180542\n",
      "Batch loss:  0.25683480501174927\n",
      "Batch loss:  0.2816134989261627\n",
      "Batch loss:  0.22560498118400574\n",
      "Batch loss:  0.2878332734107971\n",
      "Batch loss:  0.2508781850337982\n",
      "Batch loss:  0.24029326438903809\n",
      "Batch loss:  0.17878057062625885\n",
      "Batch loss:  0.23875539004802704\n",
      "Batch loss:  0.3781324028968811\n",
      "Batch loss:  0.2401232272386551\n",
      "Batch loss:  0.46120843291282654\n",
      "Batch loss:  0.18491952121257782\n",
      "Batch loss:  0.34056010842323303\n",
      "Batch loss:  0.17489269375801086\n",
      "Batch loss:  0.10568725317716599\n",
      "Batch loss:  0.12883661687374115\n",
      "Batch loss:  0.3534984588623047\n",
      "Batch loss:  0.4779050648212433\n",
      "Batch loss:  0.3265412449836731\n",
      "Batch loss:  0.14397302269935608\n",
      "Batch loss:  0.2992027997970581\n",
      "Batch loss:  0.3096848130226135\n",
      "Batch loss:  0.09978652000427246\n",
      "Batch loss:  0.17187927663326263\n",
      "Batch loss:  0.15837295353412628\n",
      "Batch loss:  0.28188735246658325\n",
      "Batch loss:  0.501374363899231\n",
      "Batch loss:  0.3227272033691406\n",
      "Batch loss:  0.12915772199630737\n",
      "Batch loss:  0.36184754967689514\n",
      "Batch loss:  0.2601453363895416\n",
      "Batch loss:  0.16304101049900055\n",
      "Batch loss:  0.1658959686756134\n",
      "Batch loss:  0.239054873585701\n",
      "Batch loss:  0.1562761515378952\n",
      "Batch loss:  0.15520605444908142\n",
      "Batch loss:  0.26805004477500916\n",
      "Batch loss:  0.3088206946849823\n",
      "Batch loss:  0.3068722188472748\n",
      "Batch loss:  0.10957985371351242\n",
      "Batch loss:  0.33133190870285034\n",
      "Batch loss:  0.21564079821109772\n",
      "Batch loss:  0.3808194696903229\n",
      "Batch loss:  0.38897132873535156\n",
      "Batch loss:  0.20621269941329956\n",
      "Batch loss:  0.19421148300170898\n",
      "Batch loss:  0.3440815508365631\n",
      "Batch loss:  0.18490035831928253\n",
      "Batch loss:  0.23399297893047333\n",
      "Batch loss:  0.2991466522216797\n",
      "Batch loss:  0.18819622695446014\n",
      "Batch loss:  0.3785654604434967\n",
      "Batch loss:  0.15086372196674347\n",
      "Batch loss:  0.35263586044311523\n",
      "Batch loss:  0.24648024141788483\n",
      "Batch loss:  0.17986087501049042\n",
      "Batch loss:  0.20730631053447723\n",
      "Batch loss:  0.32440486550331116\n",
      "Batch loss:  0.2718883156776428\n",
      "Batch loss:  0.3499811887741089\n",
      "Batch loss:  0.2545984089374542\n",
      "Batch loss:  0.3122105002403259\n",
      "Batch loss:  0.21476978063583374\n",
      "Batch loss:  0.23311103880405426\n",
      "Batch loss:  0.17141172289848328\n",
      "Batch loss:  0.2129756510257721\n",
      "Batch loss:  0.238350510597229\n",
      "Batch loss:  0.1935417652130127\n",
      "Batch loss:  0.273790568113327\n",
      "Batch loss:  0.36457979679107666\n",
      "Batch loss:  0.28775906562805176\n",
      "Batch loss:  0.1821497678756714\n",
      "Batch loss:  0.21021181344985962\n",
      "Batch loss:  0.3394075930118561\n",
      "Current average loss: 0.4396017338345144\n",
      "Batch loss:  0.29360082745552063\n",
      "Batch loss:  0.22914424538612366\n",
      "Batch loss:  0.33150142431259155\n",
      "Batch loss:  0.2746879458427429\n",
      "Batch loss:  0.13514629006385803\n",
      "Batch loss:  0.20426775515079498\n",
      "Batch loss:  0.23862813413143158\n",
      "Batch loss:  0.3045721650123596\n",
      "Batch loss:  0.31104108691215515\n",
      "Batch loss:  0.19030079245567322\n",
      "Batch loss:  0.14926649630069733\n",
      "Batch loss:  0.2772085666656494\n",
      "Batch loss:  0.5904322266578674\n",
      "Batch loss:  0.2337629795074463\n",
      "Batch loss:  0.319740891456604\n",
      "Batch loss:  0.3756602704524994\n",
      "Batch loss:  0.22701996564865112\n",
      "Batch loss:  0.2922583520412445\n",
      "Batch loss:  0.2744635045528412\n",
      "Batch loss:  0.17109103500843048\n",
      "Batch loss:  0.23594388365745544\n",
      "Batch loss:  0.2494724541902542\n",
      "Batch loss:  0.21108053624629974\n",
      "Batch loss:  0.18847040832042694\n",
      "Batch loss:  0.23628053069114685\n",
      "Batch loss:  0.3588242828845978\n",
      "Batch loss:  0.17551064491271973\n",
      "Batch loss:  0.2437330186367035\n",
      "Batch loss:  0.39369240403175354\n",
      "Batch loss:  0.3383082151412964\n",
      "Batch loss:  0.24625158309936523\n",
      "Batch loss:  0.24839383363723755\n",
      "Batch loss:  0.16759063303470612\n",
      "Batch loss:  0.22686265408992767\n",
      "Batch loss:  0.19067911803722382\n",
      "Batch loss:  0.32141807675361633\n",
      "Batch loss:  0.22113408148288727\n",
      "Batch loss:  0.3756769895553589\n",
      "Batch loss:  0.23749418556690216\n",
      "Batch loss:  0.24513883888721466\n",
      "Batch loss:  0.3251517117023468\n",
      "Batch loss:  0.1853799968957901\n",
      "Batch loss:  0.22930815815925598\n",
      "Batch loss:  0.2744729816913605\n",
      "Batch loss:  0.4619453549385071\n",
      "Batch loss:  0.2672891914844513\n",
      "Batch loss:  0.16746027767658234\n",
      "Batch loss:  0.36000776290893555\n",
      "Batch loss:  0.10095496475696564\n",
      "Batch loss:  0.16282051801681519\n",
      "Batch loss:  0.3994033932685852\n",
      "Batch loss:  0.34322941303253174\n",
      "Batch loss:  0.26537641882896423\n",
      "Batch loss:  0.23175334930419922\n",
      "Batch loss:  0.14140470325946808\n",
      "Batch loss:  0.16829818487167358\n",
      "Batch loss:  0.17958121001720428\n",
      "Batch loss:  0.2705542743206024\n",
      "Batch loss:  0.339058518409729\n",
      "Batch loss:  0.6165401935577393\n",
      "Batch loss:  0.3346969783306122\n",
      "Batch loss:  0.26195472478866577\n",
      "Batch loss:  0.14940081536769867\n",
      "Batch loss:  0.17064055800437927\n",
      "Batch loss:  0.3454435169696808\n",
      "Batch loss:  0.3250208795070648\n",
      "Batch loss:  0.2510015368461609\n",
      "Batch loss:  0.25390660762786865\n",
      "Batch loss:  0.3144546151161194\n",
      "Batch loss:  0.1459275782108307\n",
      "Batch loss:  0.3302444815635681\n",
      "Batch loss:  0.25383588671684265\n",
      "Batch loss:  0.3002813458442688\n",
      "Batch loss:  0.28383520245552063\n",
      "Batch loss:  0.3334350287914276\n",
      "Batch loss:  0.20088797807693481\n",
      "Batch loss:  0.32708144187927246\n",
      "Batch loss:  0.17178384959697723\n",
      "Batch loss:  0.247230663895607\n",
      "Batch loss:  0.31158554553985596\n",
      "Batch loss:  0.2706664502620697\n",
      "Batch loss:  0.28915905952453613\n",
      "Batch loss:  0.19286486506462097\n",
      "Batch loss:  0.29050859808921814\n",
      "Batch loss:  0.1465849131345749\n",
      "Batch loss:  0.15621846914291382\n",
      "Batch loss:  0.24822485446929932\n",
      "Batch loss:  0.19734995067119598\n",
      "Batch loss:  0.2929755747318268\n",
      "Batch loss:  0.26646268367767334\n",
      "Batch loss:  0.12950880825519562\n",
      "Batch loss:  0.2984977960586548\n",
      "Batch loss:  0.45248255133628845\n",
      "Batch loss:  0.3878454566001892\n",
      "Batch loss:  0.15685613453388214\n",
      "Batch loss:  0.18674598634243011\n",
      "Batch loss:  0.4027494490146637\n",
      "Batch loss:  0.2318422794342041\n",
      "Batch loss:  0.25195223093032837\n",
      "Batch loss:  0.1503472626209259\n",
      "Current average loss: 0.4374788644369173\n",
      "Batch loss:  0.1659892201423645\n",
      "Batch loss:  0.4063498377799988\n",
      "Batch loss:  0.2967831492424011\n",
      "Batch loss:  0.08440349251031876\n",
      "Batch loss:  0.25417280197143555\n",
      "Batch loss:  0.1612589806318283\n",
      "Batch loss:  0.3405733108520508\n",
      "Batch loss:  0.19348616898059845\n",
      "Batch loss:  0.24633869528770447\n",
      "Batch loss:  0.38228267431259155\n",
      "Batch loss:  0.1291983276605606\n",
      "Batch loss:  0.2453196793794632\n",
      "Batch loss:  0.18597210943698883\n",
      "Batch loss:  0.22872358560562134\n",
      "Batch loss:  0.21318072080612183\n",
      "Batch loss:  0.4168849289417267\n",
      "Batch loss:  0.30509915947914124\n",
      "Batch loss:  0.21716736257076263\n",
      "Batch loss:  0.3409087657928467\n",
      "Batch loss:  0.2577655613422394\n",
      "Batch loss:  0.20636029541492462\n",
      "Batch loss:  0.32377463579177856\n",
      "Batch loss:  0.2460445910692215\n",
      "Batch loss:  0.23158228397369385\n",
      "Batch loss:  0.33452466130256653\n",
      "Batch loss:  0.19459988176822662\n",
      "Batch loss:  0.242940291762352\n",
      "Batch loss:  0.5349679589271545\n",
      "Batch loss:  0.27550822496414185\n",
      "Batch loss:  0.1999787539243698\n",
      "Batch loss:  0.1452452838420868\n",
      "Batch loss:  0.21289674937725067\n",
      "Batch loss:  0.26830482482910156\n",
      "Batch loss:  0.1665363907814026\n",
      "Batch loss:  0.16661490499973297\n",
      "Batch loss:  0.30576926469802856\n",
      "Batch loss:  0.27730607986450195\n",
      "Batch loss:  0.2115730494260788\n",
      "Batch loss:  0.3231612741947174\n",
      "Batch loss:  0.8301920294761658\n",
      "Batch loss:  0.3758604824542999\n",
      "Batch loss:  0.31128984689712524\n",
      "Batch loss:  0.34588485956192017\n",
      "Batch loss:  0.3549366891384125\n",
      "Batch loss:  0.10214053839445114\n",
      "Batch loss:  0.17291325330734253\n",
      "Batch loss:  0.2366091012954712\n",
      "Batch loss:  0.3388197124004364\n",
      "Batch loss:  0.18614932894706726\n",
      "Batch loss:  0.33134639263153076\n",
      "Batch loss:  0.31714630126953125\n",
      "Batch loss:  0.3434313237667084\n",
      "Batch loss:  0.19884344935417175\n",
      "Batch loss:  0.33505725860595703\n",
      "Batch loss:  0.45615702867507935\n",
      "Batch loss:  0.2601071894168854\n",
      "Batch loss:  0.3573138415813446\n",
      "Batch loss:  0.21705517172813416\n",
      "Batch loss:  0.23883317410945892\n",
      "Batch loss:  0.21709923446178436\n",
      "Batch loss:  0.3774111568927765\n",
      "Batch loss:  0.2478061467409134\n",
      "Batch loss:  0.35462522506713867\n",
      "Batch loss:  0.35763370990753174\n",
      "Batch loss:  0.3314305245876312\n",
      "Batch loss:  0.2779562473297119\n",
      "Batch loss:  0.4858257472515106\n",
      "Batch loss:  0.25117945671081543\n",
      "Batch loss:  0.263790100812912\n",
      "Batch loss:  0.5428670644760132\n",
      "Batch loss:  0.26232749223709106\n",
      "Batch loss:  0.197445347905159\n",
      "Batch loss:  0.22721454501152039\n",
      "Batch loss:  0.3809034824371338\n",
      "Batch loss:  0.21834442019462585\n",
      "Batch loss:  0.33538126945495605\n",
      "Batch loss:  0.31243571639060974\n",
      "Batch loss:  0.25401824712753296\n",
      "Batch loss:  0.2807216942310333\n",
      "Batch loss:  0.3596458435058594\n",
      "Batch loss:  0.7105387449264526\n",
      "Batch loss:  0.19819453358650208\n",
      "Batch loss:  0.22828349471092224\n",
      "Batch loss:  0.21804460883140564\n",
      "Batch loss:  0.31210240721702576\n",
      "Batch loss:  0.433429092168808\n",
      "Batch loss:  0.17269740998744965\n",
      "Batch loss:  0.3327677249908447\n",
      "Batch loss:  0.19990454614162445\n",
      "Batch loss:  0.4814690053462982\n",
      "Batch loss:  0.2336970418691635\n",
      "Batch loss:  0.2587469816207886\n",
      "Batch loss:  0.3888756334781647\n",
      "Batch loss:  0.15651413798332214\n",
      "Batch loss:  0.1939639002084732\n",
      "Batch loss:  0.2941800653934479\n",
      "Batch loss:  0.20674166083335876\n",
      "Batch loss:  0.5859743356704712\n",
      "Batch loss:  0.22747480869293213\n",
      "Batch loss:  0.14886295795440674\n",
      "Current average loss: 0.43569577602212195\n",
      "Batch loss:  0.19456876814365387\n",
      "Batch loss:  0.18703782558441162\n",
      "Batch loss:  0.44341033697128296\n",
      "Batch loss:  0.194208025932312\n",
      "Batch loss:  0.2170560508966446\n",
      "Batch loss:  0.1612730473279953\n",
      "Batch loss:  0.2849428057670593\n",
      "Batch loss:  0.4774089455604553\n",
      "Batch loss:  0.16422969102859497\n",
      "Batch loss:  0.2121487557888031\n",
      "Batch loss:  0.3052787780761719\n",
      "Batch loss:  0.26873186230659485\n",
      "Batch loss:  0.20978206396102905\n",
      "Batch loss:  0.22835801541805267\n",
      "Batch loss:  0.2178984433412552\n",
      "Batch loss:  0.2854171395301819\n",
      "Batch loss:  0.11199088394641876\n",
      "Batch loss:  0.20293745398521423\n",
      "Batch loss:  0.3147062063217163\n",
      "Batch loss:  0.436685174703598\n",
      "Batch loss:  0.27207228541374207\n",
      "Batch loss:  0.34936434030532837\n",
      "Batch loss:  0.31325989961624146\n",
      "Batch loss:  0.37796279788017273\n",
      "Batch loss:  0.22982463240623474\n",
      "Batch loss:  0.22939340770244598\n",
      "Batch loss:  0.28258833289146423\n",
      "Batch loss:  0.3571919798851013\n",
      "Batch loss:  0.14575473964214325\n",
      "Batch loss:  0.33495238423347473\n",
      "Batch loss:  0.2771179974079132\n",
      "Batch loss:  0.2621234059333801\n",
      "Batch loss:  0.22646167874336243\n",
      "Batch loss:  0.33863699436187744\n",
      "Batch loss:  0.29154661297798157\n",
      "Batch loss:  0.24541513621807098\n",
      "Batch loss:  0.16401201486587524\n",
      "Batch loss:  0.23582790791988373\n",
      "Batch loss:  0.2910877764225006\n",
      "Batch loss:  0.15827898681163788\n",
      "Batch loss:  0.18921911716461182\n",
      "Batch loss:  0.3100760579109192\n",
      "Batch loss:  0.2843628227710724\n",
      "Batch loss:  0.3476710915565491\n",
      "Batch loss:  0.34060293436050415\n",
      "Batch loss:  0.2169685810804367\n",
      "Batch loss:  0.2279132455587387\n",
      "Batch loss:  0.2560529410839081\n",
      "Batch loss:  0.46908074617385864\n",
      "Batch loss:  0.2438773214817047\n",
      "Batch loss:  0.2376607060432434\n",
      "Batch loss:  0.20785488188266754\n",
      "Batch loss:  0.27047961950302124\n",
      "Batch loss:  0.20268340408802032\n",
      "Batch loss:  0.46069079637527466\n",
      "Batch loss:  0.3565366566181183\n",
      "Batch loss:  0.27417993545532227\n",
      "Batch loss:  0.3350045382976532\n",
      "Batch loss:  0.16869045794010162\n",
      "Batch loss:  0.5566385984420776\n",
      "Batch loss:  0.43937382102012634\n",
      "Batch loss:  0.2043466866016388\n",
      "Batch loss:  0.3343663215637207\n",
      "Batch loss:  0.1737736314535141\n",
      "Batch loss:  0.36007604002952576\n",
      "Batch loss:  0.1782957911491394\n",
      "Batch loss:  0.614403486251831\n",
      "Batch loss:  0.5609419941902161\n",
      "Batch loss:  0.18503087759017944\n",
      "Batch loss:  0.22923000156879425\n",
      "Batch loss:  0.1992618590593338\n",
      "Batch loss:  0.3294816017150879\n",
      "Batch loss:  0.13675610721111298\n",
      "Batch loss:  0.3236333131790161\n",
      "Batch loss:  0.2966727614402771\n",
      "Batch loss:  0.21263696253299713\n",
      "Batch loss:  0.1430242359638214\n",
      "Batch loss:  0.16880221664905548\n",
      "Batch loss:  0.23991890251636505\n",
      "Batch loss:  0.2340952455997467\n",
      "Batch loss:  0.27973899245262146\n",
      "Batch loss:  0.47216302156448364\n",
      "Batch loss:  0.16013413667678833\n",
      "Batch loss:  0.4987049698829651\n",
      "Batch loss:  0.24289275705814362\n",
      "Batch loss:  0.20200638473033905\n",
      "Batch loss:  0.2738756835460663\n",
      "Batch loss:  0.1833018809556961\n",
      "Batch loss:  0.21878400444984436\n",
      "Batch loss:  0.24314896762371063\n",
      "Batch loss:  0.19123095273971558\n",
      "Batch loss:  0.36975905299186707\n",
      "Batch loss:  0.21952302753925323\n",
      "Batch loss:  0.2713809609413147\n",
      "Batch loss:  0.375299334526062\n",
      "Batch loss:  0.2252381294965744\n",
      "Batch loss:  0.2683318257331848\n",
      "Batch loss:  0.24891988933086395\n",
      "Batch loss:  0.19600176811218262\n",
      "Batch loss:  0.3253667652606964\n",
      "Current average loss: 0.43380394386128246\n",
      "Batch loss:  0.34214454889297485\n",
      "Batch loss:  0.2088262140750885\n",
      "Batch loss:  0.21406930685043335\n",
      "Batch loss:  0.21657823026180267\n",
      "Batch loss:  0.20418016612529755\n",
      "Batch loss:  0.2930618226528168\n",
      "Batch loss:  0.5056588053703308\n",
      "Batch loss:  0.2813448905944824\n",
      "Batch loss:  0.3317698538303375\n",
      "Batch loss:  0.28621551394462585\n",
      "Batch loss:  0.16728679835796356\n",
      "Batch loss:  0.20354606211185455\n",
      "Batch loss:  0.12877459824085236\n",
      "Batch loss:  0.18541957437992096\n",
      "Batch loss:  0.27907806634902954\n",
      "Batch loss:  0.19751454889774323\n",
      "Batch loss:  0.17687733471393585\n",
      "Batch loss:  0.17390777170658112\n",
      "Batch loss:  0.2782752811908722\n",
      "Batch loss:  0.21864093840122223\n",
      "Batch loss:  0.33841851353645325\n",
      "Batch loss:  0.318632036447525\n",
      "Batch loss:  0.2410532534122467\n",
      "Batch loss:  0.2618351876735687\n",
      "Batch loss:  0.1465969830751419\n",
      "Batch loss:  0.4306751787662506\n",
      "Batch loss:  0.2202441543340683\n",
      "Batch loss:  0.3452676832675934\n",
      "Batch loss:  0.3015058934688568\n",
      "Batch loss:  0.2654324471950531\n",
      "Batch loss:  0.282554030418396\n",
      "Batch loss:  0.18036751449108124\n",
      "Batch loss:  0.27854570746421814\n",
      "Batch loss:  0.26534509658813477\n",
      "Batch loss:  0.3753510117530823\n",
      "Batch loss:  0.2257433533668518\n",
      "Batch loss:  0.17688730359077454\n",
      "Batch loss:  0.2801780700683594\n",
      "Batch loss:  0.32290980219841003\n",
      "Batch loss:  0.3290560245513916\n",
      "Batch loss:  0.22618158161640167\n",
      "Batch loss:  0.26334884762763977\n",
      "Batch loss:  0.30794867873191833\n",
      "Batch loss:  0.1092178001999855\n",
      "Batch loss:  0.24842379987239838\n",
      "Batch loss:  0.27436089515686035\n",
      "Batch loss:  0.2465461790561676\n",
      "Batch loss:  0.187537282705307\n",
      "Batch loss:  0.24063001573085785\n",
      "Batch loss:  0.17198564112186432\n",
      "Batch loss:  0.5675602555274963\n",
      "Batch loss:  0.22324346005916595\n",
      "Batch loss:  0.3076755106449127\n",
      "Batch loss:  0.34450459480285645\n",
      "Batch loss:  0.18723814189434052\n",
      "Batch loss:  0.19392277300357819\n",
      "Batch loss:  0.1882593035697937\n",
      "Batch loss:  0.3790673017501831\n",
      "Batch loss:  0.2559773325920105\n",
      "Batch loss:  0.5805580019950867\n",
      "Batch loss:  0.17175401747226715\n",
      "Batch loss:  0.27856674790382385\n",
      "Batch loss:  0.1719459444284439\n",
      "Batch loss:  0.2888019382953644\n",
      "Batch loss:  0.23831185698509216\n",
      "Batch loss:  0.2572494149208069\n",
      "Batch loss:  0.3079133629798889\n",
      "Batch loss:  0.19114428758621216\n",
      "Batch loss:  0.257515549659729\n",
      "Batch loss:  0.2665083706378937\n",
      "Batch loss:  0.3635724186897278\n",
      "Batch loss:  0.43234363198280334\n",
      "Batch loss:  0.2635590732097626\n",
      "Batch loss:  0.29516875743865967\n",
      "Batch loss:  0.2557065486907959\n",
      "Batch loss:  0.1132204458117485\n",
      "Batch loss:  0.18389712274074554\n",
      "Batch loss:  0.17598336935043335\n",
      "Batch loss:  0.5727720260620117\n",
      "Batch loss:  0.10319044440984726\n",
      "Batch loss:  0.2992020845413208\n",
      "Batch loss:  0.25039950013160706\n",
      "Batch loss:  0.2155989706516266\n",
      "Batch loss:  0.2832101583480835\n",
      "Batch loss:  0.18136149644851685\n",
      "Batch loss:  0.2388300895690918\n",
      "Batch loss:  0.315827876329422\n",
      "Batch loss:  0.1840934455394745\n",
      "Batch loss:  0.26998311281204224\n",
      "Batch loss:  0.16920411586761475\n",
      "Batch loss:  0.20062601566314697\n",
      "Batch loss:  0.19992516934871674\n",
      "Batch loss:  0.5387478470802307\n",
      "Batch loss:  0.28749439120292664\n",
      "Batch loss:  0.1675628274679184\n",
      "Batch loss:  0.19854414463043213\n",
      "Batch loss:  0.39088374376296997\n",
      "Batch loss:  0.24503406882286072\n",
      "Batch loss:  0.28094756603240967\n",
      "Batch loss:  0.1836993247270584\n",
      "Current average loss: 0.43181508952601677\n",
      "Batch loss:  0.3761221170425415\n",
      "Batch loss:  0.14976802468299866\n",
      "Batch loss:  0.19672104716300964\n",
      "Batch loss:  0.4541497528553009\n",
      "Batch loss:  0.299574613571167\n",
      "Batch loss:  0.2624609172344208\n",
      "Batch loss:  0.25756847858428955\n",
      "Batch loss:  0.26268163323402405\n",
      "Batch loss:  0.14650367200374603\n",
      "Batch loss:  0.17393338680267334\n",
      "Batch loss:  0.23667596280574799\n",
      "Batch loss:  0.20774567127227783\n",
      "Batch loss:  0.19128109514713287\n",
      "Batch loss:  0.2776889204978943\n",
      "Batch loss:  0.21568278968334198\n",
      "Batch loss:  0.21866562962532043\n",
      "Batch loss:  0.23782967031002045\n",
      "Batch loss:  0.3122060000896454\n",
      "Batch loss:  0.321103036403656\n",
      "Batch loss:  0.20816677808761597\n",
      "Batch loss:  0.417397141456604\n",
      "Batch loss:  0.26706045866012573\n",
      "Batch loss:  0.13630492985248566\n",
      "Batch loss:  0.3956553637981415\n",
      "Batch loss:  0.30280056595802307\n",
      "Batch loss:  0.1973067820072174\n",
      "Batch loss:  0.27470824122428894\n",
      "Batch loss:  0.32386747002601624\n",
      "Batch loss:  0.46395841240882874\n",
      "Batch loss:  0.29817911982536316\n",
      "Batch loss:  0.31561824679374695\n",
      "Batch loss:  0.2272116243839264\n",
      "Batch loss:  0.20137237012386322\n",
      "Batch loss:  0.1688055843114853\n",
      "Batch loss:  0.5512943267822266\n",
      "Batch loss:  0.20382201671600342\n",
      "Batch loss:  0.37243494391441345\n",
      "Batch loss:  0.2222554087638855\n",
      "Batch loss:  0.1726570427417755\n",
      "Batch loss:  0.14526748657226562\n",
      "Batch loss:  0.2956092357635498\n",
      "Batch loss:  0.2670882046222687\n",
      "Batch loss:  0.17869873344898224\n",
      "Batch loss:  0.41298961639404297\n",
      "Batch loss:  0.32723763585090637\n",
      "Batch loss:  0.21951089799404144\n",
      "Batch loss:  0.23777225613594055\n",
      "Batch loss:  0.22631695866584778\n",
      "Batch loss:  0.29390719532966614\n",
      "Batch loss:  0.20150257647037506\n",
      "Batch loss:  0.23908793926239014\n",
      "Batch loss:  0.2200121283531189\n",
      "Batch loss:  0.611632227897644\n",
      "Batch loss:  0.2214241921901703\n",
      "Batch loss:  0.19515492022037506\n",
      "Batch loss:  0.22947826981544495\n",
      "Batch loss:  0.3755389153957367\n",
      "Batch loss:  0.2487521916627884\n",
      "Batch loss:  0.32102301716804504\n",
      "Batch loss:  0.24312296509742737\n",
      "Batch loss:  0.4305647313594818\n",
      "Batch loss:  0.20226061344146729\n",
      "Batch loss:  0.29593637585639954\n",
      "Batch loss:  0.3086583614349365\n",
      "Batch loss:  0.21303237974643707\n",
      "Batch loss:  0.269932359457016\n",
      "Batch loss:  0.15367861092090607\n",
      "Batch loss:  0.2144308239221573\n",
      "Batch loss:  0.43379613757133484\n",
      "Batch loss:  0.2836534380912781\n",
      "Batch loss:  0.3563310503959656\n",
      "Batch loss:  0.2087453454732895\n",
      "Batch loss:  0.2333195060491562\n",
      "Batch loss:  0.15649716556072235\n",
      "Batch loss:  0.19564397633075714\n",
      "Batch loss:  0.244171604514122\n",
      "Batch loss:  0.24806001782417297\n",
      "Batch loss:  0.2792893946170807\n",
      "Batch loss:  0.203622967004776\n",
      "Batch loss:  0.22417855262756348\n",
      "Batch loss:  0.20544371008872986\n",
      "Batch loss:  0.3150462210178375\n",
      "Batch loss:  0.19449372589588165\n",
      "Batch loss:  0.5520868301391602\n",
      "Batch loss:  0.24918712675571442\n",
      "Batch loss:  0.23579294979572296\n",
      "Batch loss:  0.2962662875652313\n",
      "Batch loss:  0.29405975341796875\n",
      "Batch loss:  0.2954067885875702\n",
      "Batch loss:  0.40882885456085205\n",
      "Batch loss:  0.41198858618736267\n",
      "Batch loss:  0.3028697669506073\n",
      "Batch loss:  0.2452094405889511\n",
      "Batch loss:  0.21196188032627106\n",
      "Batch loss:  0.23238101601600647\n",
      "Batch loss:  0.2702385485172272\n",
      "Batch loss:  0.28033873438835144\n",
      "Batch loss:  0.2673247754573822\n",
      "Batch loss:  0.14104300737380981\n",
      "Batch loss:  0.3307822048664093\n",
      "Current average loss: 0.42996994660902454\n",
      "Batch loss:  0.43727123737335205\n",
      "Batch loss:  0.5535190105438232\n",
      "Batch loss:  0.14524419605731964\n",
      "Batch loss:  0.26339513063430786\n",
      "Batch loss:  0.3804054260253906\n",
      "Batch loss:  0.2564762234687805\n",
      "Batch loss:  0.30114129185676575\n",
      "Batch loss:  0.18252667784690857\n",
      "Batch loss:  0.21159783005714417\n",
      "Batch loss:  0.31461986899375916\n",
      "Batch loss:  0.3689121901988983\n",
      "Batch loss:  0.25495675206184387\n",
      "Batch loss:  0.28125980496406555\n",
      "Batch loss:  0.333319753408432\n",
      "Batch loss:  0.29262298345565796\n",
      "Batch loss:  0.1163310557603836\n",
      "Batch loss:  0.15201574563980103\n",
      "Batch loss:  0.29728302359580994\n",
      "Batch loss:  0.1161620020866394\n",
      "Batch loss:  0.24946466088294983\n",
      "Batch loss:  0.11381329596042633\n",
      "Batch loss:  0.1322816163301468\n",
      "Batch loss:  0.11873374134302139\n",
      "Batch loss:  0.4823276698589325\n",
      "Batch loss:  0.3324154317378998\n",
      "Batch loss:  0.33371007442474365\n",
      "Batch loss:  0.4674036502838135\n",
      "Batch loss:  0.352063924074173\n",
      "Batch loss:  0.1834476739168167\n",
      "Batch loss:  0.25379279255867004\n",
      "Batch loss:  0.1775858849287033\n",
      "Batch loss:  0.2000579982995987\n",
      "Batch loss:  0.177093505859375\n",
      "Batch loss:  0.17841333150863647\n",
      "Batch loss:  0.11187117546796799\n",
      "Batch loss:  0.3021942675113678\n",
      "Batch loss:  0.2556267976760864\n",
      "Batch loss:  0.2094184160232544\n",
      "Batch loss:  0.31733062863349915\n",
      "Batch loss:  0.14469324052333832\n",
      "Batch loss:  0.2926141023635864\n",
      "Batch loss:  0.1465318351984024\n",
      "Batch loss:  0.28745418787002563\n",
      "Batch loss:  0.13159431517124176\n",
      "Batch loss:  0.41765284538269043\n",
      "Batch loss:  0.1457916796207428\n",
      "Batch loss:  0.27270257472991943\n",
      "Batch loss:  0.2357892543077469\n",
      "Batch loss:  0.14359726011753082\n",
      "Batch loss:  0.3651247024536133\n",
      "Batch loss:  0.23895271122455597\n",
      "Batch loss:  0.13884073495864868\n",
      "Batch loss:  0.17710407078266144\n",
      "Batch loss:  0.28515395522117615\n",
      "Batch loss:  0.16936582326889038\n",
      "Batch loss:  0.23580242693424225\n",
      "Batch loss:  0.22914491593837738\n",
      "Batch loss:  0.2892772853374481\n",
      "Batch loss:  0.3218236565589905\n",
      "Batch loss:  0.15485835075378418\n",
      "Batch loss:  0.2657397985458374\n",
      "Batch loss:  0.32670870423316956\n",
      "Batch loss:  0.1113913357257843\n",
      "Batch loss:  0.1625412404537201\n",
      "Batch loss:  0.21932023763656616\n",
      "Batch loss:  0.1339934915304184\n",
      "Batch loss:  0.38634422421455383\n",
      "Batch loss:  0.44175562262535095\n",
      "Batch loss:  0.23110124468803406\n",
      "Batch loss:  0.17119456827640533\n",
      "Batch loss:  0.08340443670749664\n",
      "Batch loss:  0.15623757243156433\n",
      "Batch loss:  0.175152987241745\n",
      "Batch loss:  0.2819041907787323\n",
      "Batch loss:  0.21800114214420319\n",
      "Batch loss:  0.22246839106082916\n",
      "Batch loss:  0.33833226561546326\n",
      "Batch loss:  0.4518453776836395\n",
      "Batch loss:  0.4177779257297516\n",
      "Batch loss:  0.23848024010658264\n",
      "Batch loss:  0.2369602918624878\n",
      "Batch loss:  0.425220787525177\n",
      "Batch loss:  0.19237282872200012\n",
      "Batch loss:  0.20412354171276093\n",
      "Batch loss:  0.1764923334121704\n",
      "Batch loss:  0.40943726897239685\n",
      "Batch loss:  0.2815900444984436\n",
      "Batch loss:  0.15725193917751312\n",
      "Batch loss:  0.13692055642604828\n",
      "Batch loss:  0.29619744420051575\n",
      "Batch loss:  0.32202959060668945\n",
      "Batch loss:  0.3744666278362274\n",
      "Batch loss:  0.17395226657390594\n",
      "Batch loss:  0.18174317479133606\n",
      "Batch loss:  0.2472819834947586\n",
      "Batch loss:  0.22548986971378326\n",
      "Batch loss:  0.21925850212574005\n",
      "Batch loss:  0.1804005354642868\n",
      "Batch loss:  0.20718501508235931\n",
      "Batch loss:  0.10550978034734726\n",
      "Current average loss: 0.4279045632877088\n",
      "Batch loss:  0.10202816873788834\n",
      "Batch loss:  0.36300820112228394\n",
      "Batch loss:  0.2539443075656891\n",
      "Batch loss:  0.2811001241207123\n",
      "Batch loss:  0.3028745651245117\n",
      "Batch loss:  0.1563820242881775\n",
      "Batch loss:  0.26875847578048706\n",
      "Batch loss:  0.33536311984062195\n",
      "Batch loss:  0.23967009782791138\n",
      "Batch loss:  0.3531639873981476\n",
      "Batch loss:  0.2662031054496765\n",
      "Batch loss:  0.25629135966300964\n",
      "Batch loss:  0.1983805149793625\n",
      "Batch loss:  0.3856155276298523\n",
      "Batch loss:  0.20957276225090027\n",
      "Batch loss:  0.3142077922821045\n",
      "Batch loss:  0.11426443606615067\n",
      "Batch loss:  0.25824645161628723\n",
      "Batch loss:  0.2844444513320923\n",
      "Batch loss:  0.25935831665992737\n",
      "Batch loss:  0.2141781598329544\n",
      "Batch loss:  0.32575663924217224\n",
      "Batch loss:  0.1736707091331482\n",
      "Batch loss:  0.19260062277317047\n",
      "Batch loss:  0.37574219703674316\n",
      "Batch loss:  0.4621449410915375\n",
      "Batch loss:  0.14162056148052216\n",
      "Batch loss:  0.37090668082237244\n",
      "Batch loss:  0.3674398362636566\n",
      "Batch loss:  0.186778724193573\n",
      "Batch loss:  0.2975873053073883\n",
      "Batch loss:  0.37916478514671326\n",
      "Batch loss:  0.29076218605041504\n",
      "Batch loss:  0.22426803410053253\n",
      "Batch loss:  0.189286470413208\n",
      "Batch loss:  0.3964857757091522\n",
      "Batch loss:  0.22913900017738342\n",
      "Batch loss:  0.2980888783931732\n",
      "Batch loss:  0.13018707931041718\n",
      "Batch loss:  0.18424057960510254\n",
      "Batch loss:  0.2707558274269104\n",
      "Batch loss:  0.3054814636707306\n",
      "Batch loss:  0.23617541790008545\n",
      "Batch loss:  0.2672121524810791\n",
      "Batch loss:  0.35912996530532837\n",
      "Batch loss:  0.2511102855205536\n",
      "Batch loss:  0.16535820066928864\n",
      "Batch loss:  0.19307371973991394\n",
      "Batch loss:  0.19138601422309875\n",
      "Batch loss:  0.27520751953125\n",
      "Batch loss:  0.3253018856048584\n",
      "Batch loss:  0.25382983684539795\n",
      "Batch loss:  0.34948304295539856\n",
      "Batch loss:  0.27726641297340393\n",
      "Batch loss:  0.19740243256092072\n",
      "Batch loss:  0.14298652112483978\n",
      "Batch loss:  0.18046130239963531\n",
      "Batch loss:  0.32434728741645813\n",
      "Batch loss:  0.13466079533100128\n",
      "Batch loss:  0.23099663853645325\n",
      "Batch loss:  0.21262571215629578\n",
      "Batch loss:  0.22378146648406982\n",
      "Batch loss:  0.15608225762844086\n",
      "Batch loss:  0.21386957168579102\n",
      "Batch loss:  0.23749542236328125\n",
      "Batch loss:  0.22613735496997833\n",
      "Batch loss:  0.471443772315979\n",
      "Batch loss:  0.34074506163597107\n",
      "Batch loss:  0.1993415355682373\n",
      "Batch loss:  0.26550936698913574\n",
      "Batch loss:  0.18357597291469574\n",
      "Batch loss:  0.15964673459529877\n",
      "Batch loss:  0.2516906261444092\n",
      "Batch loss:  0.11516886204481125\n",
      "Batch loss:  0.09388716518878937\n",
      "Batch loss:  0.19448421895503998\n",
      "Batch loss:  0.12074412405490875\n",
      "Batch loss:  0.5582435131072998\n",
      "Batch loss:  0.4160447120666504\n",
      "Batch loss:  0.197265163064003\n",
      "Batch loss:  0.24858759343624115\n",
      "Batch loss:  0.3212994635105133\n",
      "Batch loss:  0.2086847573518753\n",
      "Batch loss:  0.18469735980033875\n",
      "Batch loss:  0.4579901099205017\n",
      "Batch loss:  0.13580474257469177\n",
      "Batch loss:  0.23273679614067078\n",
      "Batch loss:  0.20783713459968567\n",
      "Batch loss:  0.19334469735622406\n",
      "Batch loss:  0.42428484559059143\n",
      "Batch loss:  0.2637486159801483\n",
      "Batch loss:  0.2503432631492615\n",
      "Batch loss:  0.10718721151351929\n",
      "Batch loss:  0.21884843707084656\n",
      "Batch loss:  0.1850498467683792\n",
      "Batch loss:  0.4375704228878021\n",
      "Batch loss:  0.2340255081653595\n",
      "Batch loss:  0.398776650428772\n",
      "Batch loss:  0.17406049370765686\n",
      "Batch loss:  0.18948228657245636\n",
      "Current average loss: 0.42595941557393774\n",
      "Batch loss:  0.10268455743789673\n",
      "Batch loss:  0.24301941692829132\n",
      "Batch loss:  0.2380487620830536\n",
      "Batch loss:  0.1701652556657791\n",
      "Batch loss:  0.08203113824129105\n",
      "Batch loss:  0.2293263077735901\n",
      "Batch loss:  0.29600584506988525\n",
      "Batch loss:  0.2820588946342468\n",
      "Batch loss:  0.2845589220523834\n",
      "Batch loss:  0.3591398000717163\n",
      "Batch loss:  0.3176047205924988\n",
      "Batch loss:  0.23650741577148438\n",
      "Batch loss:  0.3963412940502167\n",
      "Batch loss:  0.21794083714485168\n",
      "Batch loss:  0.18083108961582184\n",
      "Batch loss:  0.25719940662384033\n",
      "Batch loss:  0.14300338923931122\n",
      "Batch loss:  0.2207697182893753\n",
      "Batch loss:  0.17705312371253967\n",
      "Batch loss:  0.2772296071052551\n",
      "Batch loss:  0.19000917673110962\n",
      "Batch loss:  0.29461026191711426\n",
      "Batch loss:  0.333568811416626\n",
      "Batch loss:  0.18929436802864075\n",
      "Batch loss:  0.1162644624710083\n",
      "Batch loss:  0.21762917935848236\n",
      "Batch loss:  0.2248784899711609\n",
      "Batch loss:  0.3343431055545807\n",
      "Batch loss:  0.2542298436164856\n",
      "Batch loss:  0.16494010388851166\n",
      "Batch loss:  0.26261675357818604\n",
      "Batch loss:  0.2243923842906952\n",
      "Batch loss:  0.18599236011505127\n",
      "Batch loss:  0.2958088517189026\n",
      "Batch loss:  0.14810805022716522\n",
      "Batch loss:  0.32065051794052124\n",
      "Batch loss:  0.39664480090141296\n",
      "Batch loss:  0.20514290034770966\n",
      "Batch loss:  0.25435954332351685\n",
      "Batch loss:  0.2611430287361145\n",
      "Batch loss:  0.18800315260887146\n",
      "Batch loss:  0.2591273784637451\n",
      "Batch loss:  0.26104748249053955\n",
      "Batch loss:  0.11247994005680084\n",
      "Batch loss:  0.26825425028800964\n",
      "Batch loss:  0.2738380432128906\n",
      "Batch loss:  0.18927237391471863\n",
      "Batch loss:  0.23154756426811218\n",
      "Batch loss:  0.1399882435798645\n",
      "Batch loss:  0.2522842586040497\n",
      "Batch loss:  0.3578130006790161\n",
      "Batch loss:  0.2876548767089844\n",
      "Batch loss:  0.1894865781068802\n",
      "Batch loss:  0.383914977312088\n",
      "Batch loss:  0.13471165299415588\n",
      "Batch loss:  0.3165132403373718\n",
      "Batch loss:  0.43216705322265625\n",
      "Batch loss:  0.22625842690467834\n",
      "Batch loss:  0.30928024649620056\n",
      "Batch loss:  0.21303217113018036\n",
      "Batch loss:  0.25396302342414856\n",
      "Batch loss:  0.32795169949531555\n",
      "Batch loss:  0.24502968788146973\n",
      "Batch loss:  0.33683332800865173\n",
      "Batch loss:  0.18592563271522522\n",
      "Batch loss:  0.38778579235076904\n",
      "Batch loss:  0.2162487506866455\n",
      "Batch loss:  0.21623830497264862\n",
      "Batch loss:  0.47651010751724243\n",
      "Batch loss:  0.4801238179206848\n",
      "Batch loss:  0.167469322681427\n",
      "Batch loss:  0.23543113470077515\n",
      "Batch loss:  0.23143315315246582\n",
      "Batch loss:  0.20130428671836853\n",
      "Batch loss:  0.3430742919445038\n",
      "Batch loss:  0.2198186069726944\n",
      "Batch loss:  0.28595197200775146\n",
      "Batch loss:  0.3697821795940399\n",
      "Batch loss:  0.11952953785657883\n",
      "Batch loss:  0.2674441635608673\n",
      "Batch loss:  0.17418771982192993\n",
      "Batch loss:  0.23538731038570404\n",
      "Batch loss:  0.15887881815433502\n",
      "Batch loss:  0.16838394105434418\n",
      "Batch loss:  0.35084259510040283\n",
      "Batch loss:  0.25165972113609314\n",
      "Batch loss:  0.18749219179153442\n",
      "Batch loss:  0.16586917638778687\n",
      "Batch loss:  0.3242884874343872\n",
      "Batch loss:  0.4302615523338318\n",
      "Batch loss:  0.20879757404327393\n",
      "Batch loss:  0.358231782913208\n",
      "Batch loss:  0.3658226430416107\n",
      "Batch loss:  0.345049113035202\n",
      "Batch loss:  0.3243301510810852\n",
      "Batch loss:  0.23932920396327972\n",
      "Batch loss:  0.20362205803394318\n",
      "Batch loss:  0.31473129987716675\n",
      "Batch loss:  0.33462202548980713\n",
      "Batch loss:  0.2930904030799866\n",
      "Current average loss: 0.42407291478653003\n",
      "Batch loss:  0.32007357478141785\n",
      "Batch loss:  0.38611292839050293\n",
      "Batch loss:  0.3370917737483978\n",
      "Batch loss:  0.3285311162471771\n",
      "Batch loss:  0.23037615418434143\n",
      "Batch loss:  0.1799689680337906\n",
      "Batch loss:  0.26389163732528687\n",
      "Batch loss:  0.1339557021856308\n",
      "Batch loss:  0.24774709343910217\n",
      "Batch loss:  0.3355957865715027\n",
      "Batch loss:  0.30791741609573364\n",
      "Batch loss:  0.1913634091615677\n",
      "Batch loss:  0.18204349279403687\n",
      "Batch loss:  0.2549128532409668\n",
      "Batch loss:  0.35081249475479126\n",
      "Batch loss:  0.14594320952892303\n",
      "Batch loss:  0.2997162938117981\n",
      "Batch loss:  0.20887358486652374\n",
      "Batch loss:  0.21150369942188263\n",
      "Batch loss:  0.20426423847675323\n",
      "Batch loss:  0.30572307109832764\n",
      "Batch loss:  0.4305836260318756\n",
      "Batch loss:  0.20572145283222198\n",
      "Batch loss:  0.152640238404274\n",
      "Batch loss:  0.21852339804172516\n",
      "Batch loss:  0.18334165215492249\n",
      "Batch loss:  0.1506667584180832\n",
      "Batch loss:  0.1565721482038498\n",
      "Batch loss:  0.23268797993659973\n",
      "Batch loss:  0.0761498212814331\n",
      "Batch loss:  0.20589305460453033\n",
      "Batch loss:  0.17093779146671295\n",
      "Batch loss:  0.19250157475471497\n",
      "Batch loss:  0.29726386070251465\n",
      "Batch loss:  0.24298043549060822\n",
      "Batch loss:  0.20626400411128998\n",
      "Batch loss:  0.2389211505651474\n",
      "Batch loss:  0.2311341017484665\n",
      "Batch loss:  0.19916518032550812\n",
      "Batch loss:  0.32938018441200256\n",
      "Batch loss:  0.26947417855262756\n",
      "Batch loss:  0.5888553261756897\n",
      "Batch loss:  0.21032944321632385\n",
      "Batch loss:  0.2943970859050751\n",
      "Batch loss:  0.2915879786014557\n",
      "Batch loss:  0.299552857875824\n",
      "Batch loss:  0.16503946483135223\n",
      "Batch loss:  0.2641665041446686\n",
      "Batch loss:  0.18416085839271545\n",
      "Batch loss:  0.1654374897480011\n",
      "Batch loss:  0.17056728899478912\n",
      "Batch loss:  0.2891484797000885\n",
      "Batch loss:  0.2504224479198456\n",
      "Batch loss:  0.15432602167129517\n",
      "Batch loss:  0.31500497460365295\n",
      "Batch loss:  0.1773034930229187\n",
      "Batch loss:  0.2873920202255249\n",
      "Batch loss:  0.2526015043258667\n",
      "Batch loss:  0.3062678873538971\n",
      "Batch loss:  0.25411784648895264\n",
      "Batch loss:  0.2608402967453003\n",
      "Batch loss:  0.1636108011007309\n",
      "Batch loss:  0.3694794774055481\n",
      "Batch loss:  0.2750616669654846\n",
      "Batch loss:  0.3366492986679077\n",
      "Batch loss:  0.2395758330821991\n",
      "Batch loss:  0.303524374961853\n",
      "Batch loss:  0.22220131754875183\n",
      "Batch loss:  0.1952623724937439\n",
      "Batch loss:  0.38912829756736755\n",
      "Batch loss:  0.33511820435523987\n",
      "Batch loss:  0.365321546792984\n",
      "Batch loss:  0.2174106240272522\n",
      "Batch loss:  0.23254820704460144\n",
      "Batch loss:  0.3749195635318756\n",
      "Batch loss:  0.2603248655796051\n",
      "Batch loss:  0.28638118505477905\n",
      "Batch loss:  0.17267340421676636\n",
      "Batch loss:  0.19846056401729584\n",
      "Batch loss:  0.2037365883588791\n",
      "Batch loss:  0.11999684572219849\n",
      "Batch loss:  0.3836873471736908\n",
      "Batch loss:  0.34345701336860657\n",
      "Batch loss:  0.21012485027313232\n",
      "Batch loss:  0.35918062925338745\n",
      "Batch loss:  0.4307163953781128\n",
      "Batch loss:  0.2615768015384674\n",
      "Batch loss:  0.33365195989608765\n",
      "Batch loss:  0.1275414675474167\n",
      "Batch loss:  0.23915250599384308\n",
      "Batch loss:  0.37328025698661804\n",
      "Batch loss:  0.47313499450683594\n",
      "Batch loss:  0.1268129199743271\n",
      "Batch loss:  0.1037522405385971\n",
      "Batch loss:  0.3364417254924774\n",
      "Batch loss:  0.2105507105588913\n",
      "Batch loss:  0.23079878091812134\n",
      "Batch loss:  0.25260743498802185\n",
      "Batch loss:  0.19752559065818787\n",
      "Batch loss:  0.14329098165035248\n",
      "Current average loss: 0.4222039024281824\n",
      "Batch loss:  0.18507254123687744\n",
      "Batch loss:  0.1799955517053604\n",
      "Batch loss:  0.2651892900466919\n",
      "Batch loss:  0.151792511343956\n",
      "Batch loss:  0.3160444498062134\n",
      "Batch loss:  0.17175108194351196\n",
      "Batch loss:  0.1909986436367035\n",
      "Batch loss:  0.22104346752166748\n",
      "Batch loss:  0.5309794545173645\n",
      "Batch loss:  0.23113591969013214\n",
      "Batch loss:  0.20846690237522125\n",
      "Batch loss:  0.411784291267395\n",
      "Batch loss:  0.23543383181095123\n",
      "Batch loss:  0.25691813230514526\n",
      "Batch loss:  0.2690778970718384\n",
      "Batch loss:  0.27784714102745056\n",
      "Batch loss:  0.1760842204093933\n",
      "Batch loss:  0.35706719756126404\n",
      "Batch loss:  0.16150528192520142\n",
      "Batch loss:  0.24812260270118713\n",
      "Batch loss:  0.3225301206111908\n",
      "Batch loss:  0.17762261629104614\n",
      "Batch loss:  0.34465739130973816\n",
      "Batch loss:  0.2613639831542969\n",
      "Batch loss:  0.17327073216438293\n",
      "Batch loss:  0.3606165945529938\n",
      "Batch loss:  0.47589221596717834\n",
      "Batch loss:  0.2609652578830719\n",
      "Batch loss:  0.426897794008255\n",
      "Batch loss:  0.47548800706863403\n",
      "Batch loss:  0.45467478036880493\n",
      "Batch loss:  0.15543268620967865\n",
      "Batch loss:  0.15768112242221832\n",
      "Batch loss:  0.16544213891029358\n",
      "Batch loss:  0.2586181163787842\n",
      "Batch loss:  0.11873836815357208\n",
      "Batch loss:  0.27932387590408325\n",
      "Batch loss:  0.21041402220726013\n",
      "Batch loss:  0.2280605286359787\n",
      "Batch loss:  0.18941070139408112\n",
      "Batch loss:  0.2261982262134552\n",
      "Batch loss:  0.19609859585762024\n",
      "Batch loss:  0.19507692754268646\n",
      "Batch loss:  0.11893035471439362\n",
      "Batch loss:  0.3420807719230652\n",
      "Batch loss:  0.28242403268814087\n",
      "Batch loss:  0.508449912071228\n",
      "Batch loss:  0.21518883109092712\n",
      "Batch loss:  0.20660610496997833\n",
      "Batch loss:  0.2604071795940399\n",
      "Batch loss:  0.2232540398836136\n",
      "Batch loss:  0.2522886395454407\n",
      "Batch loss:  0.2112015187740326\n",
      "Batch loss:  0.5030672550201416\n",
      "Batch loss:  0.16068841516971588\n",
      "Batch loss:  0.35228532552719116\n",
      "Batch loss:  0.1761411428451538\n",
      "Batch loss:  0.14452004432678223\n",
      "Batch loss:  0.20585553348064423\n",
      "Batch loss:  0.4214266836643219\n",
      "Batch loss:  0.23520292341709137\n",
      "Batch loss:  0.12851113080978394\n",
      "Batch loss:  0.10794161260128021\n",
      "Batch loss:  0.23896348476409912\n",
      "Batch loss:  0.28721940517425537\n",
      "Batch loss:  0.20109596848487854\n",
      "Batch loss:  0.23565897345542908\n",
      "Batch loss:  0.1938517838716507\n",
      "Batch loss:  0.20288747549057007\n",
      "Batch loss:  0.24937714636325836\n",
      "Batch loss:  0.22188521921634674\n",
      "Batch loss:  0.5499857068061829\n",
      "Batch loss:  0.1766529679298401\n",
      "Batch loss:  0.2534293234348297\n",
      "Batch loss:  0.34553414583206177\n",
      "Batch loss:  0.4162069857120514\n",
      "Batch loss:  0.20585417747497559\n",
      "Batch loss:  0.29030364751815796\n",
      "Batch loss:  0.38126862049102783\n",
      "Batch loss:  0.2719508111476898\n",
      "Batch loss:  0.21271216869354248\n",
      "Batch loss:  0.11036581546068192\n",
      "Batch loss:  0.41509345173835754\n",
      "Batch loss:  0.27825266122817993\n",
      "Batch loss:  0.531977117061615\n",
      "Batch loss:  0.20244619250297546\n",
      "Batch loss:  0.2879309356212616\n",
      "Batch loss:  0.3626939654350281\n",
      "Batch loss:  0.23947827517986298\n",
      "Batch loss:  0.27073875069618225\n",
      "Batch loss:  0.36851781606674194\n",
      "Batch loss:  0.2546997368335724\n",
      "Batch loss:  0.36872780323028564\n",
      "Batch loss:  0.18577362596988678\n",
      "Batch loss:  0.2644166648387909\n",
      "Batch loss:  0.21791483461856842\n",
      "Batch loss:  0.3472496569156647\n",
      "Batch loss:  0.4597262442111969\n",
      "Batch loss:  0.22174374759197235\n",
      "Batch loss:  0.24761426448822021\n",
      "Current average loss: 0.4205265919177989\n",
      "Batch loss:  0.18973100185394287\n",
      "Batch loss:  0.2890141010284424\n",
      "Batch loss:  0.39434394240379333\n",
      "Batch loss:  0.3113585412502289\n",
      "Batch loss:  0.3348122239112854\n",
      "Batch loss:  0.22193484008312225\n",
      "Batch loss:  0.22735510766506195\n",
      "Batch loss:  0.1593044102191925\n",
      "Batch loss:  0.1974847912788391\n",
      "Batch loss:  0.22587108612060547\n",
      "Batch loss:  0.4956922233104706\n",
      "Batch loss:  0.13792794942855835\n",
      "Batch loss:  0.28442874550819397\n",
      "Batch loss:  0.18945026397705078\n",
      "Batch loss:  0.23418791592121124\n",
      "Batch loss:  0.2618744969367981\n",
      "Batch loss:  0.2846667468547821\n",
      "Batch loss:  0.2398436814546585\n",
      "Batch loss:  0.2507948875427246\n",
      "Batch loss:  0.12159458547830582\n",
      "Batch loss:  0.11882731318473816\n",
      "Batch loss:  0.17865431308746338\n",
      "Batch loss:  0.5583289861679077\n",
      "Batch loss:  0.32969972491264343\n",
      "Batch loss:  0.2207353562116623\n",
      "Batch loss:  0.14035780727863312\n",
      "Batch loss:  0.18871547281742096\n",
      "Batch loss:  0.3209856152534485\n",
      "Batch loss:  0.24852444231510162\n",
      "Batch loss:  0.35083457827568054\n",
      "Batch loss:  0.198513925075531\n",
      "Batch loss:  0.24658668041229248\n",
      "Batch loss:  0.17294193804264069\n",
      "Batch loss:  0.1617347002029419\n",
      "Batch loss:  0.23260396718978882\n",
      "Batch loss:  0.36506393551826477\n",
      "Batch loss:  0.24727851152420044\n",
      "Batch loss:  0.251870721578598\n",
      "Batch loss:  0.22228281199932098\n",
      "Batch loss:  0.12433676421642303\n",
      "Batch loss:  0.2738543748855591\n",
      "Batch loss:  0.18846631050109863\n",
      "Batch loss:  0.23768046498298645\n",
      "Batch loss:  0.2023049294948578\n",
      "Batch loss:  0.21573187410831451\n",
      "Batch loss:  0.3056105077266693\n",
      "Batch loss:  0.2667618989944458\n",
      "Batch loss:  0.32401183247566223\n",
      "Batch loss:  0.22900643944740295\n",
      "Batch loss:  0.2615455687046051\n",
      "Batch loss:  0.3549818694591522\n",
      "Batch loss:  0.3284265398979187\n",
      "Batch loss:  0.17126411199569702\n",
      "Batch loss:  0.22220759093761444\n",
      "Batch loss:  0.21801242232322693\n",
      "Batch loss:  0.4107789695262909\n",
      "Batch loss:  0.23311983048915863\n",
      "Batch loss:  0.2653407156467438\n",
      "Batch loss:  0.1951134353876114\n",
      "Batch loss:  0.36169394850730896\n",
      "Batch loss:  0.40957045555114746\n",
      "Batch loss:  0.31492066383361816\n",
      "Batch loss:  0.3523901402950287\n",
      "Batch loss:  0.15465065836906433\n",
      "Batch loss:  0.23383522033691406\n",
      "Batch loss:  0.26954641938209534\n",
      "Batch loss:  0.152607724070549\n",
      "Batch loss:  0.3305633068084717\n",
      "Batch loss:  0.22874636948108673\n",
      "Batch loss:  0.27979686856269836\n",
      "Batch loss:  0.1394326239824295\n",
      "Batch loss:  0.26493096351623535\n",
      "Batch loss:  0.1671464443206787\n",
      "Batch loss:  0.2689424753189087\n",
      "Batch loss:  0.24514105916023254\n",
      "Batch loss:  0.26625850796699524\n",
      "Batch loss:  0.43943339586257935\n",
      "Batch loss:  0.34210535883903503\n",
      "Batch loss:  0.15571367740631104\n",
      "Batch loss:  0.16316312551498413\n",
      "Batch loss:  0.1756153106689453\n",
      "Batch loss:  0.2682611048221588\n",
      "Batch loss:  0.23659589886665344\n",
      "Batch loss:  0.12328546494245529\n",
      "Batch loss:  0.22374503314495087\n",
      "Batch loss:  0.24833528697490692\n",
      "Batch loss:  0.24585308134555817\n",
      "Batch loss:  0.48092642426490784\n",
      "Batch loss:  0.19743987917900085\n",
      "Batch loss:  0.19647763669490814\n",
      "Batch loss:  0.2780037224292755\n",
      "Batch loss:  0.2821276783943176\n",
      "Batch loss:  0.40463733673095703\n",
      "Batch loss:  0.1381618231534958\n",
      "Batch loss:  0.2826460599899292\n",
      "Batch loss:  0.37280553579330444\n",
      "Batch loss:  0.2054905742406845\n",
      "Batch loss:  0.20833027362823486\n",
      "Batch loss:  0.2019827961921692\n",
      "Batch loss:  0.12997952103614807\n",
      "Current average loss: 0.41872285032574175\n",
      "Batch loss:  0.22354236245155334\n",
      "Batch loss:  0.2643916606903076\n",
      "Batch loss:  0.17507526278495789\n",
      "Batch loss:  0.29033592343330383\n",
      "Batch loss:  0.2640928328037262\n",
      "Batch loss:  0.25703728199005127\n",
      "Batch loss:  0.20697948336601257\n",
      "Batch loss:  0.18559394776821136\n",
      "Batch loss:  0.19376488029956818\n",
      "Batch loss:  0.2638853192329407\n",
      "Batch loss:  0.2617014944553375\n",
      "Batch loss:  0.34535959362983704\n",
      "Batch loss:  0.24751585721969604\n",
      "Batch loss:  0.24796026945114136\n",
      "Batch loss:  0.19405204057693481\n",
      "Batch loss:  0.25946637988090515\n",
      "Batch loss:  0.24644766747951508\n",
      "Batch loss:  0.22301729023456573\n",
      "Batch loss:  0.21409541368484497\n",
      "Batch loss:  0.2133239507675171\n",
      "Batch loss:  0.11237309128046036\n",
      "Batch loss:  0.32527872920036316\n",
      "Batch loss:  0.25565609335899353\n",
      "Batch loss:  0.17038854956626892\n",
      "Batch loss:  0.17797452211380005\n",
      "Batch loss:  0.21121405065059662\n",
      "Batch loss:  0.399956613779068\n",
      "Batch loss:  0.12256224453449249\n",
      "Batch loss:  0.6755977869033813\n",
      "Batch loss:  0.18394863605499268\n",
      "Batch loss:  0.21233727037906647\n",
      "Batch loss:  0.16323688626289368\n",
      "Batch loss:  0.32524827122688293\n",
      "Batch loss:  0.20536890625953674\n",
      "Batch loss:  0.29302990436553955\n",
      "Batch loss:  0.14972786605358124\n",
      "Batch loss:  0.22208693623542786\n",
      "Batch loss:  0.4229947328567505\n",
      "Batch loss:  0.2203006148338318\n",
      "Batch loss:  0.1238018274307251\n",
      "Batch loss:  0.22827541828155518\n",
      "Batch loss:  0.14693008363246918\n",
      "Batch loss:  0.18000756204128265\n",
      "Batch loss:  0.19678650796413422\n",
      "Batch loss:  0.22442129254341125\n",
      "Batch loss:  0.19731451570987701\n",
      "Batch loss:  0.343941867351532\n",
      "Batch loss:  0.20582760870456696\n",
      "Batch loss:  0.17890535295009613\n",
      "Batch loss:  0.15616536140441895\n",
      "Batch loss:  0.20018723607063293\n",
      "Batch loss:  0.3436707854270935\n",
      "Batch loss:  0.2443227767944336\n",
      "Batch loss:  0.19610939919948578\n",
      "Batch loss:  0.22401465475559235\n",
      "Batch loss:  0.25130265951156616\n",
      "Batch loss:  0.2652706801891327\n",
      "Batch loss:  0.17619000375270844\n",
      "Batch loss:  0.17355011403560638\n",
      "Batch loss:  0.15978111326694489\n",
      "Batch loss:  0.2133283019065857\n",
      "Batch loss:  0.18146724998950958\n",
      "Batch loss:  0.12994195520877838\n",
      "Batch loss:  0.4362567067146301\n",
      "Batch loss:  0.19515511393547058\n",
      "Batch loss:  0.27003413438796997\n",
      "Batch loss:  0.18356944620609283\n",
      "Batch loss:  0.576777994632721\n",
      "Batch loss:  0.23900894820690155\n",
      "Batch loss:  0.23524720966815948\n",
      "Batch loss:  0.26373928785324097\n",
      "Batch loss:  0.28229615092277527\n",
      "Batch loss:  0.3106234073638916\n",
      "Batch loss:  0.10682028532028198\n",
      "Batch loss:  0.2710167169570923\n",
      "Batch loss:  0.08908705413341522\n",
      "Batch loss:  0.20735353231430054\n",
      "Batch loss:  0.2183033972978592\n",
      "Batch loss:  0.30024227499961853\n",
      "Batch loss:  0.262724369764328\n",
      "Batch loss:  0.2456216812133789\n",
      "Batch loss:  0.2529138922691345\n",
      "Batch loss:  0.37417036294937134\n",
      "Batch loss:  0.20815534889698029\n",
      "Batch loss:  0.20848798751831055\n",
      "Batch loss:  0.2648927569389343\n",
      "Batch loss:  0.4568091630935669\n",
      "Batch loss:  0.1796988695859909\n",
      "Batch loss:  0.3276172876358032\n",
      "Batch loss:  0.1430637538433075\n",
      "Batch loss:  0.31124210357666016\n",
      "Batch loss:  0.18983325362205505\n",
      "Batch loss:  0.49300292134284973\n",
      "Batch loss:  0.16792045533657074\n",
      "Batch loss:  0.29865264892578125\n",
      "Batch loss:  0.4879334568977356\n",
      "Batch loss:  0.15236817300319672\n",
      "Batch loss:  0.35365599393844604\n",
      "Batch loss:  0.1739315539598465\n",
      "Batch loss:  0.4166505038738251\n",
      "Current average loss: 0.4168986856744031\n",
      "Batch loss:  0.2258860170841217\n",
      "Batch loss:  0.2521553933620453\n",
      "Batch loss:  0.19598320126533508\n",
      "Batch loss:  0.38090455532073975\n",
      "Batch loss:  0.15168532729148865\n",
      "Batch loss:  0.14522379636764526\n",
      "Batch loss:  0.3403056561946869\n",
      "Batch loss:  0.30635127425193787\n",
      "Batch loss:  0.15903502702713013\n",
      "Batch loss:  0.368304044008255\n",
      "Batch loss:  0.16347914934158325\n",
      "Batch loss:  0.3018583655357361\n",
      "Batch loss:  0.21885357797145844\n",
      "Batch loss:  0.3205849528312683\n",
      "Batch loss:  0.16291123628616333\n",
      "Batch loss:  0.3171566128730774\n",
      "Batch loss:  0.21389414370059967\n",
      "Batch loss:  0.17282670736312866\n",
      "Batch loss:  0.2779320776462555\n",
      "Batch loss:  0.24491006135940552\n",
      "Batch loss:  0.1862601488828659\n",
      "Batch loss:  0.3110359311103821\n",
      "Batch loss:  0.08262570947408676\n",
      "Batch loss:  0.409624308347702\n",
      "Batch loss:  0.12825201451778412\n",
      "Batch loss:  0.3014336824417114\n",
      "Batch loss:  0.1245366781949997\n",
      "Batch loss:  0.25117799639701843\n",
      "Batch loss:  0.061589013785123825\n",
      "Batch loss:  0.20362035930156708\n",
      "Batch loss:  0.43718189001083374\n",
      "Batch loss:  0.2816961109638214\n",
      "Batch loss:  0.25360307097435\n",
      "Batch loss:  0.11026743054389954\n",
      "Batch loss:  0.30891457200050354\n",
      "Batch loss:  0.24904973804950714\n",
      "Batch loss:  0.16327746212482452\n",
      "Batch loss:  0.23874877393245697\n",
      "Batch loss:  0.24911245703697205\n",
      "Batch loss:  0.14301805198192596\n",
      "Batch loss:  0.25012674927711487\n",
      "Batch loss:  0.2426355481147766\n",
      "Batch loss:  0.177903413772583\n",
      "Batch loss:  0.29511818289756775\n",
      "Batch loss:  0.36970922350883484\n",
      "Batch loss:  0.20765580236911774\n",
      "Batch loss:  0.18883167207241058\n",
      "Batch loss:  0.17493975162506104\n",
      "Batch loss:  0.21890997886657715\n",
      "Batch loss:  0.3975158631801605\n",
      "Batch loss:  0.20469895005226135\n",
      "Batch loss:  0.10868239402770996\n",
      "Batch loss:  0.24176761507987976\n",
      "Batch loss:  0.17478521168231964\n",
      "Batch loss:  0.22856222093105316\n",
      "Batch loss:  0.27353033423423767\n",
      "Batch loss:  0.2790166735649109\n",
      "Batch loss:  0.20310808718204498\n",
      "Batch loss:  0.23998597264289856\n",
      "Batch loss:  0.15582621097564697\n",
      "Batch loss:  0.19322045147418976\n",
      "Batch loss:  0.224635511636734\n",
      "Batch loss:  0.30968159437179565\n",
      "Batch loss:  0.26234665513038635\n",
      "Batch loss:  0.19911915063858032\n",
      "Batch loss:  0.24902306497097015\n",
      "Batch loss:  0.12059485912322998\n",
      "Batch loss:  0.23758642375469208\n",
      "Batch loss:  0.15951228141784668\n",
      "Batch loss:  0.15348896384239197\n",
      "Batch loss:  0.12594975531101227\n",
      "Batch loss:  0.3613845109939575\n",
      "Batch loss:  0.18088564276695251\n",
      "Batch loss:  0.20382273197174072\n",
      "Batch loss:  0.15865808725357056\n",
      "Batch loss:  0.3878672420978546\n",
      "Batch loss:  0.4244319200515747\n",
      "Batch loss:  0.11143545061349869\n",
      "Batch loss:  0.4516933858394623\n",
      "Batch loss:  0.27629542350769043\n",
      "Batch loss:  0.2504027187824249\n",
      "Batch loss:  0.16964222490787506\n",
      "Batch loss:  0.2628711462020874\n",
      "Batch loss:  0.22578862309455872\n",
      "Batch loss:  0.2867441177368164\n",
      "Batch loss:  0.27460819482803345\n",
      "Batch loss:  0.2923547327518463\n",
      "Batch loss:  0.3339613974094391\n",
      "Batch loss:  0.1546541303396225\n",
      "Batch loss:  0.19038869440555573\n",
      "Batch loss:  0.323253870010376\n",
      "Batch loss:  0.22428223490715027\n",
      "Batch loss:  0.13506005704402924\n",
      "Batch loss:  0.3467957377433777\n",
      "Batch loss:  0.24025166034698486\n",
      "Batch loss:  0.22609098255634308\n",
      "Batch loss:  0.351593941450119\n",
      "Batch loss:  0.3600885570049286\n",
      "Batch loss:  0.29677748680114746\n",
      "Batch loss:  0.28702953457832336\n",
      "Current average loss: 0.41504445780527777\n",
      "Batch loss:  0.12747487425804138\n",
      "Batch loss:  0.2538727819919586\n",
      "Batch loss:  0.3952333331108093\n",
      "Batch loss:  0.1660640388727188\n",
      "Batch loss:  0.24644525349140167\n",
      "Batch loss:  0.21769967675209045\n",
      "Batch loss:  0.18400833010673523\n",
      "Batch loss:  0.16178786754608154\n",
      "Batch loss:  0.3064740002155304\n",
      "Batch loss:  0.3050885498523712\n",
      "Batch loss:  0.3894158601760864\n",
      "Batch loss:  0.27866995334625244\n",
      "Batch loss:  0.22613388299942017\n",
      "Batch loss:  0.28561562299728394\n",
      "Batch loss:  0.35797643661499023\n",
      "Batch loss:  0.11330997943878174\n",
      "Batch loss:  0.25411057472229004\n",
      "Batch loss:  0.429177850484848\n",
      "Batch loss:  0.20437149703502655\n",
      "Batch loss:  0.17383891344070435\n",
      "Batch loss:  0.198440283536911\n",
      "Batch loss:  0.07497291266918182\n",
      "Batch loss:  0.18169339001178741\n",
      "Batch loss:  0.7049670219421387\n",
      "Batch loss:  0.24185621738433838\n",
      "Batch loss:  0.2886272966861725\n",
      "Batch loss:  0.18252675235271454\n",
      "Batch loss:  0.15717695653438568\n",
      "Batch loss:  0.21829304099082947\n",
      "Batch loss:  0.22423766553401947\n",
      "Batch loss:  0.2647879421710968\n",
      "Batch loss:  0.24603714048862457\n",
      "Batch loss:  0.3551406264305115\n",
      "Batch loss:  0.36819639801979065\n",
      "Batch loss:  0.16816873848438263\n",
      "Batch loss:  0.26587429642677307\n",
      "Batch loss:  0.14878597855567932\n",
      "Batch loss:  0.4707827568054199\n",
      "Batch loss:  0.17238463461399078\n",
      "Batch loss:  0.19448590278625488\n",
      "Batch loss:  0.34115469455718994\n",
      "Batch loss:  0.2698959410190582\n",
      "Batch loss:  0.3146951198577881\n",
      "Batch loss:  0.14443840086460114\n",
      "Batch loss:  0.23503541946411133\n",
      "Batch loss:  0.19191035628318787\n",
      "Batch loss:  0.23229292035102844\n",
      "Batch loss:  0.2327224612236023\n",
      "Batch loss:  0.3390335738658905\n",
      "Batch loss:  0.2638838589191437\n",
      "Batch loss:  0.1691758781671524\n",
      "Batch loss:  0.21339792013168335\n",
      "Batch loss:  0.20995932817459106\n",
      "Batch loss:  0.13488076627254486\n",
      "Batch loss:  0.34857258200645447\n",
      "Batch loss:  0.22325091063976288\n",
      "Batch loss:  0.15969006717205048\n",
      "Batch loss:  0.21484702825546265\n",
      "Batch loss:  0.12450186163187027\n",
      "Batch loss:  0.3063856065273285\n",
      "Batch loss:  0.30470889806747437\n",
      "Batch loss:  0.4777784049510956\n",
      "Batch loss:  0.17176997661590576\n",
      "Batch loss:  0.3054751455783844\n",
      "Batch loss:  0.1723385453224182\n",
      "Batch loss:  0.17416153848171234\n",
      "Batch loss:  0.2425440102815628\n",
      "Batch loss:  0.2366291880607605\n",
      "Batch loss:  0.160567969083786\n",
      "Batch loss:  0.30253171920776367\n",
      "Batch loss:  0.35295751690864563\n",
      "Batch loss:  0.17230653762817383\n",
      "Batch loss:  0.33206498622894287\n",
      "Batch loss:  0.23339039087295532\n",
      "Batch loss:  0.20640526711940765\n",
      "Batch loss:  0.18298587203025818\n",
      "Batch loss:  0.19616767764091492\n",
      "Batch loss:  0.34147775173187256\n",
      "Batch loss:  0.35668331384658813\n",
      "Batch loss:  0.27175238728523254\n",
      "Batch loss:  0.2981783151626587\n",
      "Batch loss:  0.13913118839263916\n",
      "Batch loss:  0.2322317659854889\n",
      "Batch loss:  0.2276144176721573\n",
      "Batch loss:  0.2578612267971039\n",
      "Batch loss:  0.2682523727416992\n",
      "Batch loss:  0.3055085241794586\n",
      "Batch loss:  0.19411242008209229\n",
      "Batch loss:  0.14007392525672913\n",
      "Batch loss:  0.465092271566391\n",
      "Batch loss:  0.1798301339149475\n",
      "Batch loss:  0.2512738108634949\n",
      "Batch loss:  0.20087097585201263\n",
      "Batch loss:  0.26754868030548096\n",
      "Batch loss:  0.22741422057151794\n",
      "Batch loss:  0.3249441981315613\n",
      "Batch loss:  0.12927401065826416\n",
      "Batch loss:  0.33346426486968994\n",
      "Batch loss:  0.21758466958999634\n",
      "Batch loss:  0.18376606702804565\n",
      "Current average loss: 0.4133163262253695\n",
      "Batch loss:  0.1888028383255005\n",
      "Batch loss:  0.19894124567508698\n",
      "Batch loss:  0.3768906593322754\n",
      "Batch loss:  0.15454570949077606\n",
      "Batch loss:  0.1904498189687729\n",
      "Batch loss:  0.1986369639635086\n",
      "Batch loss:  0.1876855194568634\n",
      "Batch loss:  0.14445778727531433\n",
      "Batch loss:  0.25770288705825806\n",
      "Batch loss:  0.2110612839460373\n",
      "Batch loss:  0.1935078203678131\n",
      "Batch loss:  0.2615332305431366\n",
      "Batch loss:  0.22349579632282257\n",
      "Batch loss:  0.2857113480567932\n",
      "Batch loss:  0.3413103520870209\n",
      "Batch loss:  0.18047979474067688\n",
      "Batch loss:  0.16061677038669586\n",
      "Batch loss:  0.24737447500228882\n",
      "Batch loss:  0.18664222955703735\n",
      "Batch loss:  0.2680582106113434\n",
      "Batch loss:  0.12979449331760406\n",
      "Batch loss:  0.1251664161682129\n",
      "Batch loss:  0.12091584503650665\n",
      "Batch loss:  0.34671416878700256\n",
      "Batch loss:  0.21762068569660187\n",
      "Batch loss:  0.27918481826782227\n",
      "Batch loss:  0.2928221523761749\n",
      "Batch loss:  0.200692281126976\n",
      "Batch loss:  0.2281230241060257\n",
      "Batch loss:  0.31460776925086975\n",
      "Batch loss:  0.30100420117378235\n",
      "Batch loss:  0.12740342319011688\n",
      "Batch loss:  0.19239658117294312\n",
      "Batch loss:  0.3172163963317871\n",
      "Batch loss:  0.30811816453933716\n",
      "Batch loss:  0.2206844538450241\n",
      "Batch loss:  0.13783203065395355\n",
      "Batch loss:  0.12343384325504303\n",
      "Batch loss:  0.2753141224384308\n",
      "Batch loss:  0.18597634136676788\n",
      "Batch loss:  0.1743195801973343\n",
      "Batch loss:  0.4335702359676361\n",
      "Batch loss:  0.2149982750415802\n",
      "Batch loss:  0.5733202695846558\n",
      "Batch loss:  0.2904653549194336\n",
      "Batch loss:  0.226867213845253\n",
      "Batch loss:  0.2397719919681549\n",
      "Batch loss:  0.2778504192829132\n",
      "Batch loss:  0.318138062953949\n",
      "Batch loss:  0.2741236686706543\n",
      "Batch loss:  0.20940251648426056\n",
      "Batch loss:  0.27743300795555115\n",
      "Batch loss:  0.24001672863960266\n",
      "Batch loss:  0.3067145049571991\n",
      "Batch loss:  0.2919987440109253\n",
      "Batch loss:  0.24884822964668274\n",
      "Batch loss:  0.3978992700576782\n",
      "Batch loss:  0.2209356725215912\n",
      "Batch loss:  0.21773262321949005\n",
      "Batch loss:  0.1481596827507019\n",
      "Batch loss:  0.14645323157310486\n",
      "Batch loss:  0.27616626024246216\n",
      "Batch loss:  0.25339430570602417\n",
      "Batch loss:  0.1408289223909378\n",
      "Batch loss:  0.49879947304725647\n",
      "Batch loss:  0.11396152526140213\n",
      "Batch loss:  0.2316931188106537\n",
      "Batch loss:  0.2717881500720978\n",
      "Batch loss:  0.1315748393535614\n",
      "Batch loss:  0.19679388403892517\n",
      "Batch loss:  0.13751541078090668\n",
      "Batch loss:  0.15391600131988525\n",
      "Batch loss:  0.3546178936958313\n",
      "Batch loss:  0.387387216091156\n",
      "Batch loss:  0.15676972270011902\n",
      "Batch loss:  0.2024182677268982\n",
      "Batch loss:  0.28017666935920715\n",
      "Batch loss:  0.21472497284412384\n",
      "Batch loss:  0.24309004843235016\n",
      "Batch loss:  0.3207925856113434\n",
      "Batch loss:  0.17730100452899933\n",
      "Batch loss:  0.2962280809879303\n",
      "Batch loss:  0.49849316477775574\n",
      "Batch loss:  0.220889151096344\n",
      "Batch loss:  0.15160708129405975\n",
      "Batch loss:  0.12992575764656067\n",
      "Batch loss:  0.1754930019378662\n",
      "Batch loss:  0.15165026485919952\n",
      "Batch loss:  0.10970723628997803\n",
      "Batch loss:  0.20533190667629242\n",
      "Batch loss:  0.45217952132225037\n",
      "Batch loss:  0.34126394987106323\n",
      "Batch loss:  0.2753062844276428\n",
      "Batch loss:  0.22404249012470245\n",
      "Batch loss:  0.17864735424518585\n",
      "Batch loss:  0.10324195772409439\n",
      "Batch loss:  0.21742582321166992\n",
      "Batch loss:  0.29688015580177307\n",
      "Batch loss:  0.2576778531074524\n",
      "Batch loss:  0.10901682078838348\n",
      "Current average loss: 0.41150610364452056\n",
      "Batch loss:  0.3636508285999298\n",
      "Batch loss:  0.29045408964157104\n",
      "Batch loss:  0.22840912640094757\n",
      "Batch loss:  0.29599687457084656\n",
      "Batch loss:  0.2702409327030182\n",
      "Batch loss:  0.23557578027248383\n",
      "Batch loss:  0.24722576141357422\n",
      "Batch loss:  0.23314215242862701\n",
      "Batch loss:  0.3665403425693512\n",
      "Batch loss:  0.38661929965019226\n",
      "Batch loss:  0.2636583745479584\n",
      "Batch loss:  0.3924981355667114\n",
      "Batch loss:  0.32478010654449463\n",
      "Batch loss:  0.29746463894844055\n",
      "Batch loss:  0.11723459511995316\n",
      "Batch loss:  0.2024824172258377\n",
      "Batch loss:  0.33931344747543335\n",
      "Batch loss:  0.15395016968250275\n",
      "Batch loss:  0.3139279782772064\n",
      "Batch loss:  0.29778748750686646\n",
      "Batch loss:  0.20363079011440277\n",
      "Batch loss:  0.21384026110172272\n",
      "Batch loss:  0.17612431943416595\n",
      "Batch loss:  0.3312046527862549\n",
      "Batch loss:  0.23156799376010895\n",
      "Batch loss:  0.1711076945066452\n",
      "Batch loss:  0.22255848348140717\n",
      "Batch loss:  0.28557878732681274\n",
      "Batch loss:  0.22682656347751617\n",
      "Batch loss:  0.1856299489736557\n",
      "Batch loss:  0.3190561532974243\n",
      "Batch loss:  0.22823038697242737\n",
      "Batch loss:  0.36343157291412354\n",
      "Batch loss:  0.2653891146183014\n",
      "Batch loss:  0.5793330669403076\n",
      "Batch loss:  0.2408147156238556\n",
      "Batch loss:  0.20829105377197266\n",
      "Batch loss:  0.2124125361442566\n",
      "Batch loss:  0.27569451928138733\n",
      "Batch loss:  0.22526252269744873\n",
      "Batch loss:  0.28696906566619873\n",
      "Batch loss:  0.14745154976844788\n",
      "Batch loss:  0.4897187650203705\n",
      "Batch loss:  0.14861278235912323\n",
      "Batch loss:  0.2602359652519226\n",
      "Batch loss:  0.2735706865787506\n",
      "Batch loss:  0.1906522959470749\n",
      "Batch loss:  0.2380228191614151\n",
      "Batch loss:  0.16870474815368652\n",
      "Batch loss:  0.1525045931339264\n",
      "Batch loss:  0.13451309502124786\n",
      "Batch loss:  0.16467829048633575\n",
      "Batch loss:  0.1674022078514099\n",
      "Batch loss:  0.3819200098514557\n",
      "Batch loss:  0.20046480000019073\n",
      "Batch loss:  0.1993175894021988\n",
      "Batch loss:  0.16960661113262177\n",
      "Batch loss:  0.2066870629787445\n",
      "Batch loss:  0.12318341434001923\n",
      "Batch loss:  0.16742999851703644\n",
      "Batch loss:  0.37836161255836487\n",
      "Batch loss:  0.13276222348213196\n",
      "Batch loss:  0.13924738764762878\n",
      "Batch loss:  0.33895304799079895\n",
      "Batch loss:  0.22025369107723236\n",
      "Batch loss:  0.3379359841346741\n",
      "Batch loss:  0.2409595400094986\n",
      "Batch loss:  0.47462478280067444\n",
      "Batch loss:  0.2594321668148041\n",
      "Batch loss:  0.18970291316509247\n",
      "Batch loss:  0.20590843260288239\n",
      "Batch loss:  0.38175687193870544\n",
      "Batch loss:  0.3866455852985382\n",
      "Batch loss:  0.2659027874469757\n",
      "Batch loss:  0.38568639755249023\n",
      "Batch loss:  0.15253819525241852\n",
      "Batch loss:  0.25368738174438477\n",
      "Batch loss:  0.2695416510105133\n",
      "Batch loss:  0.18905232846736908\n",
      "Batch loss:  0.24850620329380035\n",
      "Batch loss:  0.21164405345916748\n",
      "Batch loss:  0.23206780850887299\n",
      "Batch loss:  0.19258010387420654\n",
      "Batch loss:  0.26767390966415405\n",
      "Batch loss:  0.17395703494548798\n",
      "Batch loss:  0.22509926557540894\n",
      "Batch loss:  0.10561832040548325\n",
      "Batch loss:  0.31533578038215637\n",
      "Batch loss:  0.2709963023662567\n",
      "Batch loss:  0.1553438901901245\n",
      "Batch loss:  0.49897897243499756\n",
      "Batch loss:  0.21457001566886902\n",
      "Batch loss:  0.3021681010723114\n",
      "Batch loss:  0.15892915427684784\n",
      "Batch loss:  0.27936267852783203\n",
      "Batch loss:  0.26899951696395874\n",
      "Batch loss:  0.17235927283763885\n",
      "Batch loss:  0.2753123939037323\n",
      "Batch loss:  0.3819867670536041\n",
      "Batch loss:  0.35699811577796936\n",
      "Current average loss: 0.4099164101749479\n",
      "Batch loss:  0.20743416249752045\n",
      "Batch loss:  0.27210646867752075\n",
      "Batch loss:  0.3603442311286926\n",
      "Batch loss:  0.2956607937812805\n",
      "Batch loss:  0.15464574098587036\n",
      "Batch loss:  0.10164126753807068\n",
      "Batch loss:  0.3995707631111145\n",
      "Batch loss:  0.11107544600963593\n",
      "Batch loss:  0.3400905728340149\n",
      "Batch loss:  0.22744333744049072\n",
      "Batch loss:  0.24222715198993683\n",
      "Batch loss:  0.2493753284215927\n",
      "Batch loss:  0.3217040002346039\n",
      "Batch loss:  0.23838260769844055\n",
      "Batch loss:  0.2552047073841095\n",
      "Batch loss:  0.22322404384613037\n",
      "Batch loss:  0.34930527210235596\n",
      "Training loss epoch: 0.40964964066208454\n",
      "Training accuracy epoch: 88.46%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8507d02",
   "metadata": {
    "id": "f8507d02"
   },
   "source": [
    "In my experiments, I found that two epochs are needed for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0070c530",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0070c530",
    "outputId": "100532a3-c873-48f5-fd5a-2cf9d7ae886c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Batch loss:  0.30658015608787537\n",
      "Batch loss:  0.2090335339307785\n",
      "Batch loss:  0.11736591905355453\n",
      "Batch loss:  0.15671348571777344\n",
      "Batch loss:  0.14766599237918854\n",
      "Batch loss:  0.4427259564399719\n",
      "Batch loss:  0.15494859218597412\n",
      "Batch loss:  0.19636113941669464\n",
      "Batch loss:  0.09706836938858032\n",
      "Batch loss:  0.30986127257347107\n",
      "Batch loss:  0.2652578353881836\n",
      "Batch loss:  0.15982013940811157\n",
      "Batch loss:  0.35197415947914124\n",
      "Batch loss:  0.2020859569311142\n",
      "Batch loss:  0.13872545957565308\n",
      "Batch loss:  0.10717929899692535\n",
      "Batch loss:  0.13130062818527222\n",
      "Batch loss:  0.1123797670006752\n",
      "Batch loss:  0.16938932240009308\n",
      "Batch loss:  0.19514347612857819\n",
      "Batch loss:  0.21794208884239197\n",
      "Batch loss:  0.31663280725479126\n",
      "Batch loss:  0.17242123186588287\n",
      "Batch loss:  0.21658697724342346\n",
      "Batch loss:  0.28050267696380615\n",
      "Batch loss:  0.23515154421329498\n",
      "Batch loss:  0.1934249848127365\n",
      "Batch loss:  0.2820550203323364\n",
      "Batch loss:  0.2926201820373535\n",
      "Batch loss:  0.11386732012033463\n",
      "Batch loss:  0.20019355416297913\n",
      "Current average loss: 0.21272968638584192\n",
      "Batch loss:  0.18705005943775177\n",
      "Batch loss:  0.1156248226761818\n",
      "Batch loss:  0.13110382854938507\n",
      "Batch loss:  0.20997725427150726\n",
      "Batch loss:  0.3167059123516083\n",
      "Batch loss:  0.17268002033233643\n",
      "Batch loss:  0.16684401035308838\n",
      "Batch loss:  0.16241136193275452\n",
      "Batch loss:  0.1650944948196411\n",
      "Batch loss:  0.16204674541950226\n",
      "Batch loss:  0.3543287217617035\n",
      "Batch loss:  0.32809677720069885\n",
      "Batch loss:  0.31611061096191406\n",
      "Batch loss:  0.34461066126823425\n",
      "Batch loss:  0.17587441205978394\n",
      "Batch loss:  0.2720583379268646\n",
      "Batch loss:  0.1666068434715271\n",
      "Batch loss:  0.06460153311491013\n",
      "Batch loss:  0.28394827246665955\n",
      "Batch loss:  0.23054563999176025\n",
      "Batch loss:  0.15176764130592346\n",
      "Batch loss:  0.27367666363716125\n",
      "Batch loss:  0.30441001057624817\n",
      "Batch loss:  0.19938623905181885\n",
      "Batch loss:  0.1980179250240326\n",
      "Batch loss:  0.11767523735761642\n",
      "Batch loss:  0.11034879088401794\n",
      "Batch loss:  0.31028011441230774\n",
      "Batch loss:  0.22751100361347198\n",
      "Batch loss:  0.11357356607913971\n",
      "Batch loss:  0.088291235268116\n",
      "Batch loss:  0.3523806929588318\n",
      "Batch loss:  0.16196973621845245\n",
      "Batch loss:  0.12641680240631104\n",
      "Batch loss:  0.13444192707538605\n",
      "Batch loss:  0.16281919181346893\n",
      "Batch loss:  0.2377001941204071\n",
      "Batch loss:  0.23162619769573212\n",
      "Batch loss:  0.19665773212909698\n",
      "Batch loss:  0.1561255156993866\n",
      "Batch loss:  0.1386425495147705\n",
      "Batch loss:  0.12487868219614029\n",
      "Batch loss:  0.15554474294185638\n",
      "Batch loss:  0.1023566871881485\n",
      "Batch loss:  0.2153027504682541\n",
      "Batch loss:  0.13054516911506653\n",
      "Batch loss:  0.25252702832221985\n",
      "Batch loss:  0.10345456004142761\n",
      "Batch loss:  0.20696742832660675\n",
      "Batch loss:  0.10794182121753693\n",
      "Batch loss:  0.13575312495231628\n",
      "Batch loss:  0.2613834738731384\n",
      "Batch loss:  0.17011389136314392\n",
      "Batch loss:  0.20179104804992676\n",
      "Batch loss:  0.21690987050533295\n",
      "Batch loss:  0.25064194202423096\n",
      "Batch loss:  0.11437396705150604\n",
      "Batch loss:  0.23182640969753265\n",
      "Batch loss:  0.1540609449148178\n",
      "Batch loss:  0.26885029673576355\n",
      "Batch loss:  0.34124791622161865\n",
      "Batch loss:  0.18700319528579712\n",
      "Batch loss:  0.16283036768436432\n",
      "Batch loss:  0.22558780014514923\n",
      "Batch loss:  0.19165390729904175\n",
      "Batch loss:  0.14031624794006348\n",
      "Batch loss:  0.21589209139347076\n",
      "Batch loss:  0.22315652668476105\n",
      "Batch loss:  0.11685965955257416\n",
      "Batch loss:  0.29735660552978516\n",
      "Batch loss:  0.1783769130706787\n",
      "Batch loss:  0.17890885472297668\n",
      "Batch loss:  0.2182931900024414\n",
      "Batch loss:  0.16299501061439514\n",
      "Batch loss:  0.23653270304203033\n",
      "Batch loss:  0.18131478130817413\n",
      "Batch loss:  0.19390034675598145\n",
      "Batch loss:  0.18583053350448608\n",
      "Batch loss:  0.1438632756471634\n",
      "Batch loss:  0.2034589797258377\n",
      "Batch loss:  0.288892537355423\n",
      "Batch loss:  0.14148332178592682\n",
      "Batch loss:  0.19981180131435394\n",
      "Batch loss:  0.145375058054924\n",
      "Batch loss:  0.12992413341999054\n",
      "Batch loss:  0.36630013585090637\n",
      "Batch loss:  0.1750895082950592\n",
      "Batch loss:  0.2173791527748108\n",
      "Batch loss:  0.3937598466873169\n",
      "Batch loss:  0.2828289866447449\n",
      "Batch loss:  0.1924837827682495\n",
      "Batch loss:  0.16921912133693695\n",
      "Batch loss:  0.19029949605464935\n",
      "Batch loss:  0.3093312084674835\n",
      "Batch loss:  0.23269587755203247\n",
      "Batch loss:  0.22040122747421265\n",
      "Batch loss:  0.24745430052280426\n",
      "Batch loss:  0.152563214302063\n",
      "Batch loss:  0.10590634495019913\n",
      "Batch loss:  0.13756148517131805\n",
      "Current average loss: 0.21247702370528293\n",
      "Batch loss:  0.15622055530548096\n",
      "Batch loss:  0.2217147946357727\n",
      "Batch loss:  0.21005749702453613\n",
      "Batch loss:  0.16031980514526367\n",
      "Batch loss:  0.26853615045547485\n",
      "Batch loss:  0.17021912336349487\n",
      "Batch loss:  0.14154377579689026\n",
      "Batch loss:  0.14616644382476807\n",
      "Batch loss:  0.21323435008525848\n",
      "Batch loss:  0.13521026074886322\n",
      "Batch loss:  0.1854996234178543\n",
      "Batch loss:  0.26803120970726013\n",
      "Batch loss:  0.1182788535952568\n",
      "Batch loss:  0.45667657256126404\n",
      "Batch loss:  0.19869603216648102\n",
      "Batch loss:  0.10989784449338913\n",
      "Batch loss:  0.14991728961467743\n",
      "Batch loss:  0.3278716206550598\n",
      "Batch loss:  0.26392367482185364\n",
      "Batch loss:  0.10917870700359344\n",
      "Batch loss:  0.1385970562696457\n",
      "Batch loss:  0.2781953811645508\n",
      "Batch loss:  0.1912013590335846\n",
      "Batch loss:  0.29085755348205566\n",
      "Batch loss:  0.1496218740940094\n",
      "Batch loss:  0.1855657696723938\n",
      "Batch loss:  0.27999961376190186\n",
      "Batch loss:  0.09338032454252243\n",
      "Batch loss:  0.3200504779815674\n",
      "Batch loss:  0.16433057188987732\n",
      "Batch loss:  0.15759575366973877\n",
      "Batch loss:  0.08122821152210236\n",
      "Batch loss:  0.2971935570240021\n",
      "Batch loss:  0.49483034014701843\n",
      "Batch loss:  0.25853419303894043\n",
      "Batch loss:  0.18381571769714355\n",
      "Batch loss:  0.2858934998512268\n",
      "Batch loss:  0.12900730967521667\n",
      "Batch loss:  0.21908582746982574\n",
      "Batch loss:  0.09580299258232117\n",
      "Batch loss:  0.15544678270816803\n",
      "Batch loss:  0.15087707340717316\n",
      "Batch loss:  0.24347710609436035\n",
      "Batch loss:  0.10917078703641891\n",
      "Batch loss:  0.12650597095489502\n",
      "Batch loss:  0.1414320170879364\n",
      "Batch loss:  0.14248335361480713\n",
      "Batch loss:  0.08988336473703384\n",
      "Batch loss:  0.29080721735954285\n",
      "Batch loss:  0.4138389527797699\n",
      "Batch loss:  0.11654218286275864\n",
      "Batch loss:  0.10281172394752502\n",
      "Batch loss:  0.20228172838687897\n",
      "Batch loss:  0.2917754054069519\n",
      "Batch loss:  0.06444039195775986\n",
      "Batch loss:  0.17688089609146118\n",
      "Batch loss:  0.17668674886226654\n",
      "Batch loss:  0.11278755217790604\n",
      "Batch loss:  0.099762462079525\n",
      "Batch loss:  0.1399715691804886\n",
      "Batch loss:  0.11874544620513916\n",
      "Batch loss:  0.16134317219257355\n",
      "Batch loss:  0.10489539802074432\n",
      "Batch loss:  0.34179988503456116\n",
      "Batch loss:  0.24652016162872314\n",
      "Batch loss:  0.16577549278736115\n",
      "Batch loss:  0.17743021249771118\n",
      "Batch loss:  0.15160800516605377\n",
      "Batch loss:  0.14278028905391693\n",
      "Batch loss:  0.11013523489236832\n",
      "Batch loss:  0.2345624566078186\n",
      "Batch loss:  0.2265147715806961\n",
      "Batch loss:  0.20982497930526733\n",
      "Batch loss:  0.31806835532188416\n",
      "Batch loss:  0.1889074742794037\n",
      "Batch loss:  0.18412353098392487\n",
      "Batch loss:  0.09505513310432434\n",
      "Batch loss:  0.12434365600347519\n",
      "Batch loss:  0.3297630250453949\n",
      "Batch loss:  0.10843011736869812\n",
      "Batch loss:  0.24838887155056\n",
      "Batch loss:  0.18266867101192474\n",
      "Batch loss:  0.22253182530403137\n",
      "Batch loss:  0.2534676790237427\n",
      "Batch loss:  0.2079855501651764\n",
      "Batch loss:  0.24503204226493835\n",
      "Batch loss:  0.2678172290325165\n",
      "Batch loss:  0.1495148092508316\n",
      "Batch loss:  0.25591519474983215\n",
      "Batch loss:  0.11732158809900284\n",
      "Batch loss:  0.24920596182346344\n",
      "Batch loss:  0.08177962154150009\n",
      "Batch loss:  0.2172171175479889\n",
      "Batch loss:  0.19451892375946045\n",
      "Batch loss:  0.20680278539657593\n",
      "Batch loss:  0.11210852116346359\n",
      "Batch loss:  0.3787868022918701\n",
      "Batch loss:  0.14755100011825562\n",
      "Batch loss:  0.08142510056495667\n",
      "Batch loss:  0.131734237074852\n",
      "Current average loss: 0.2121042620534524\n",
      "Batch loss:  0.5864165425300598\n",
      "Batch loss:  0.1921614557504654\n",
      "Batch loss:  0.35670778155326843\n",
      "Batch loss:  0.2728199362754822\n",
      "Batch loss:  0.2656385898590088\n",
      "Batch loss:  0.11499109864234924\n",
      "Batch loss:  0.23681508004665375\n",
      "Batch loss:  0.24143674969673157\n",
      "Batch loss:  0.2229633927345276\n",
      "Batch loss:  0.15209351480007172\n",
      "Batch loss:  0.2033206671476364\n",
      "Batch loss:  0.22829672694206238\n",
      "Batch loss:  0.2668800354003906\n",
      "Batch loss:  0.24440476298332214\n",
      "Batch loss:  0.18936848640441895\n",
      "Batch loss:  0.2954244613647461\n",
      "Batch loss:  0.27636998891830444\n",
      "Batch loss:  0.23345167934894562\n",
      "Batch loss:  0.17893938720226288\n",
      "Batch loss:  0.11354117840528488\n",
      "Batch loss:  0.13633741438388824\n",
      "Batch loss:  0.12024541199207306\n",
      "Batch loss:  0.2607210576534271\n",
      "Batch loss:  0.09801004081964493\n",
      "Batch loss:  0.19302357733249664\n",
      "Batch loss:  0.18256571888923645\n",
      "Batch loss:  0.13053137063980103\n",
      "Batch loss:  0.1492447555065155\n",
      "Batch loss:  0.12872084975242615\n",
      "Batch loss:  0.09370279312133789\n",
      "Batch loss:  0.17814835906028748\n",
      "Batch loss:  0.1299220323562622\n",
      "Batch loss:  0.11883915960788727\n",
      "Batch loss:  0.08867304027080536\n",
      "Batch loss:  0.2575353682041168\n",
      "Batch loss:  0.18188782036304474\n",
      "Batch loss:  0.2669665813446045\n",
      "Batch loss:  0.20088103413581848\n",
      "Batch loss:  0.18249855935573578\n",
      "Batch loss:  0.19438183307647705\n",
      "Batch loss:  0.26826703548431396\n",
      "Batch loss:  0.45131877064704895\n",
      "Batch loss:  0.10829984396696091\n",
      "Batch loss:  0.13292227685451508\n",
      "Batch loss:  0.35587719082832336\n",
      "Batch loss:  0.19657054543495178\n",
      "Batch loss:  0.11016025394201279\n",
      "Batch loss:  0.0836755633354187\n",
      "Batch loss:  0.23856514692306519\n",
      "Batch loss:  0.10279741883277893\n",
      "Batch loss:  0.11795423924922943\n",
      "Batch loss:  0.2170521765947342\n",
      "Batch loss:  0.13176336884498596\n",
      "Batch loss:  0.1166047528386116\n",
      "Batch loss:  0.13636617362499237\n",
      "Batch loss:  0.16163231432437897\n",
      "Batch loss:  0.16068431735038757\n",
      "Batch loss:  0.1302042007446289\n",
      "Batch loss:  0.2583135962486267\n",
      "Batch loss:  0.08954860270023346\n",
      "Batch loss:  0.10682759433984756\n",
      "Batch loss:  0.27734988927841187\n",
      "Batch loss:  0.2739950716495514\n",
      "Batch loss:  0.14915360510349274\n",
      "Batch loss:  0.14856286346912384\n",
      "Batch loss:  0.13235819339752197\n",
      "Batch loss:  0.15914979577064514\n",
      "Batch loss:  0.13840971887111664\n",
      "Batch loss:  0.2134888619184494\n",
      "Batch loss:  0.413210391998291\n",
      "Batch loss:  0.16488215327262878\n",
      "Batch loss:  0.08849095553159714\n",
      "Batch loss:  0.2712468206882477\n",
      "Batch loss:  0.24026769399642944\n",
      "Batch loss:  0.19307692348957062\n",
      "Batch loss:  0.27310481667518616\n",
      "Batch loss:  0.285321980714798\n",
      "Batch loss:  0.4552799165248871\n",
      "Batch loss:  0.639733612537384\n",
      "Batch loss:  0.1753365844488144\n",
      "Batch loss:  0.17617252469062805\n",
      "Batch loss:  0.16693755984306335\n",
      "Batch loss:  0.2738357186317444\n",
      "Batch loss:  0.26149696111679077\n",
      "Batch loss:  0.3430802524089813\n",
      "Batch loss:  0.26124343276023865\n",
      "Batch loss:  0.29288724064826965\n",
      "Batch loss:  0.21570537984371185\n",
      "Batch loss:  0.1840561330318451\n",
      "Batch loss:  0.16845673322677612\n",
      "Batch loss:  0.27352192997932434\n",
      "Batch loss:  0.13768333196640015\n",
      "Batch loss:  0.21800875663757324\n",
      "Batch loss:  0.17459413409233093\n",
      "Batch loss:  0.18460901081562042\n",
      "Batch loss:  0.15030930936336517\n",
      "Batch loss:  0.27617141604423523\n",
      "Batch loss:  0.1456713080406189\n",
      "Batch loss:  0.1384996622800827\n",
      "Batch loss:  0.1637086719274521\n",
      "Current average loss: 0.21201368846920357\n",
      "Batch loss:  0.1525619775056839\n",
      "Batch loss:  0.1915789544582367\n",
      "Batch loss:  0.24898427724838257\n",
      "Batch loss:  0.15683497488498688\n",
      "Batch loss:  0.1767789125442505\n",
      "Batch loss:  0.418446809053421\n",
      "Batch loss:  0.2191300392150879\n",
      "Batch loss:  0.09979233145713806\n",
      "Batch loss:  0.1350327730178833\n",
      "Batch loss:  0.2640382647514343\n",
      "Batch loss:  0.2652112543582916\n",
      "Batch loss:  0.17928442358970642\n",
      "Batch loss:  0.3569624423980713\n",
      "Batch loss:  0.2579529285430908\n",
      "Batch loss:  0.17071229219436646\n",
      "Batch loss:  0.23330116271972656\n",
      "Batch loss:  0.12843555212020874\n",
      "Batch loss:  0.13570936024188995\n",
      "Batch loss:  0.17234943807125092\n",
      "Batch loss:  0.23804669082164764\n",
      "Batch loss:  0.31191614270210266\n",
      "Batch loss:  0.16709190607070923\n",
      "Batch loss:  0.12401598691940308\n",
      "Batch loss:  0.1443125307559967\n",
      "Batch loss:  0.17329423129558563\n",
      "Batch loss:  0.13259878754615784\n",
      "Batch loss:  0.04854058101773262\n",
      "Batch loss:  0.16237781941890717\n",
      "Batch loss:  0.14136511087417603\n",
      "Batch loss:  0.16758841276168823\n",
      "Batch loss:  0.15429405868053436\n",
      "Batch loss:  0.14442162215709686\n",
      "Batch loss:  0.18756355345249176\n",
      "Batch loss:  0.12790410220623016\n",
      "Batch loss:  0.4303651452064514\n",
      "Batch loss:  0.16309836506843567\n",
      "Batch loss:  0.1983419507741928\n",
      "Batch loss:  0.30687615275382996\n",
      "Batch loss:  0.249399334192276\n",
      "Batch loss:  0.12777173519134521\n",
      "Batch loss:  0.08181293308734894\n",
      "Batch loss:  0.15222935378551483\n",
      "Batch loss:  0.2973892092704773\n",
      "Batch loss:  0.2602206766605377\n",
      "Batch loss:  0.1423042118549347\n",
      "Batch loss:  0.250466525554657\n",
      "Batch loss:  0.20760588347911835\n",
      "Batch loss:  0.1855844259262085\n",
      "Batch loss:  0.11416473984718323\n",
      "Batch loss:  0.3661324083805084\n",
      "Batch loss:  0.0900086909532547\n",
      "Batch loss:  0.14949646592140198\n",
      "Batch loss:  0.18369890749454498\n",
      "Batch loss:  0.1078723818063736\n",
      "Batch loss:  0.5815798044204712\n",
      "Batch loss:  0.26218709349632263\n",
      "Batch loss:  0.22444486618041992\n",
      "Batch loss:  0.14351145923137665\n",
      "Batch loss:  0.1544126719236374\n",
      "Batch loss:  0.12449049204587936\n",
      "Batch loss:  0.3599720299243927\n",
      "Batch loss:  0.11152061074972153\n",
      "Batch loss:  0.19195768237113953\n",
      "Batch loss:  0.12365547567605972\n",
      "Batch loss:  0.14466704428195953\n",
      "Batch loss:  0.09782245010137558\n",
      "Batch loss:  0.3636181056499481\n",
      "Batch loss:  0.24350327253341675\n",
      "Batch loss:  0.20230992138385773\n",
      "Batch loss:  0.1442541927099228\n",
      "Batch loss:  0.11948295682668686\n",
      "Batch loss:  0.1397828459739685\n",
      "Batch loss:  0.31763166189193726\n",
      "Batch loss:  0.20114874839782715\n",
      "Batch loss:  0.16280274093151093\n",
      "Batch loss:  0.17131350934505463\n",
      "Batch loss:  0.18572700023651123\n",
      "Batch loss:  0.24156372249126434\n",
      "Batch loss:  0.2180153876543045\n",
      "Batch loss:  0.15160730481147766\n",
      "Batch loss:  0.30885034799575806\n",
      "Batch loss:  0.19113829731941223\n",
      "Batch loss:  0.0776127427816391\n",
      "Batch loss:  0.143886998295784\n",
      "Batch loss:  0.15284332633018494\n",
      "Batch loss:  0.2265142798423767\n",
      "Batch loss:  0.2533628046512604\n",
      "Batch loss:  0.13543888926506042\n",
      "Batch loss:  0.26006701588630676\n",
      "Batch loss:  0.21132558584213257\n",
      "Batch loss:  0.15930697321891785\n",
      "Batch loss:  0.24734868109226227\n",
      "Batch loss:  0.17583626508712769\n",
      "Batch loss:  0.3029475212097168\n",
      "Batch loss:  0.14262191951274872\n",
      "Batch loss:  0.16366802155971527\n",
      "Batch loss:  0.2539812922477722\n",
      "Batch loss:  0.14417213201522827\n",
      "Batch loss:  0.24108867347240448\n",
      "Batch loss:  0.31910231709480286\n",
      "Current average loss: 0.21177712885616884\n",
      "Batch loss:  0.3006882965564728\n",
      "Batch loss:  0.3886655867099762\n",
      "Batch loss:  0.2957998514175415\n",
      "Batch loss:  0.24158336222171783\n",
      "Batch loss:  0.11746126413345337\n",
      "Batch loss:  0.09541396051645279\n",
      "Batch loss:  0.47011247277259827\n",
      "Batch loss:  0.14306120574474335\n",
      "Batch loss:  0.2237468808889389\n",
      "Batch loss:  0.1642521619796753\n",
      "Batch loss:  0.38691002130508423\n",
      "Batch loss:  0.17900997400283813\n",
      "Batch loss:  0.18062859773635864\n",
      "Batch loss:  0.25009089708328247\n",
      "Batch loss:  0.21688145399093628\n",
      "Batch loss:  0.22840075194835663\n",
      "Batch loss:  0.16624878346920013\n",
      "Batch loss:  0.2297433316707611\n",
      "Batch loss:  0.3244050443172455\n",
      "Batch loss:  0.17963021993637085\n",
      "Batch loss:  0.10377206653356552\n",
      "Batch loss:  0.1720559448003769\n",
      "Batch loss:  0.26288169622421265\n",
      "Batch loss:  0.15044575929641724\n",
      "Batch loss:  0.16616496443748474\n",
      "Batch loss:  0.2689046859741211\n",
      "Batch loss:  0.15771490335464478\n",
      "Batch loss:  0.3578997552394867\n",
      "Batch loss:  0.09383918344974518\n",
      "Batch loss:  0.319011926651001\n",
      "Batch loss:  0.10417518019676208\n",
      "Batch loss:  0.2147119641304016\n",
      "Batch loss:  0.2202364057302475\n",
      "Batch loss:  0.21789923310279846\n",
      "Batch loss:  0.12059454619884491\n",
      "Batch loss:  0.13819976150989532\n",
      "Batch loss:  0.14784835278987885\n",
      "Batch loss:  0.32233279943466187\n",
      "Batch loss:  0.2651245892047882\n",
      "Batch loss:  0.14849995076656342\n",
      "Batch loss:  0.3755456209182739\n",
      "Batch loss:  0.16568264365196228\n",
      "Batch loss:  0.2060610055923462\n",
      "Batch loss:  0.16971877217292786\n",
      "Batch loss:  0.23879919946193695\n",
      "Batch loss:  0.10878629237413406\n",
      "Batch loss:  0.1553187221288681\n",
      "Batch loss:  0.1441287398338318\n",
      "Batch loss:  0.31795284152030945\n",
      "Batch loss:  0.266690731048584\n",
      "Batch loss:  0.12379360944032669\n",
      "Batch loss:  0.4382327198982239\n",
      "Batch loss:  0.1003037542104721\n",
      "Batch loss:  0.21700188517570496\n",
      "Batch loss:  0.2852730453014374\n",
      "Batch loss:  0.19044452905654907\n",
      "Batch loss:  0.12356126308441162\n",
      "Batch loss:  0.2008209079504013\n",
      "Batch loss:  0.1955491602420807\n",
      "Batch loss:  0.23421251773834229\n",
      "Batch loss:  0.2074902057647705\n",
      "Batch loss:  0.09835898876190186\n",
      "Batch loss:  0.21656374633312225\n",
      "Batch loss:  0.17447364330291748\n",
      "Batch loss:  0.13370370864868164\n",
      "Batch loss:  0.10796349495649338\n",
      "Batch loss:  0.2299679070711136\n",
      "Batch loss:  0.19947783648967743\n",
      "Batch loss:  0.24036146700382233\n",
      "Batch loss:  0.17895659804344177\n",
      "Batch loss:  0.35880717635154724\n",
      "Batch loss:  0.12002068012952805\n",
      "Batch loss:  0.11320976912975311\n",
      "Batch loss:  0.4527226984500885\n",
      "Batch loss:  0.1062861904501915\n",
      "Batch loss:  0.17998239398002625\n",
      "Batch loss:  0.261709988117218\n",
      "Batch loss:  0.2873738408088684\n",
      "Batch loss:  0.29744336009025574\n",
      "Batch loss:  0.2519405484199524\n",
      "Batch loss:  0.44423675537109375\n",
      "Batch loss:  0.16684311628341675\n",
      "Batch loss:  0.1689804643392563\n",
      "Batch loss:  0.13323542475700378\n",
      "Batch loss:  0.3688656985759735\n",
      "Batch loss:  0.16871146857738495\n",
      "Batch loss:  0.19806577265262604\n",
      "Batch loss:  0.13009919226169586\n",
      "Batch loss:  0.30801165103912354\n",
      "Batch loss:  0.3420240879058838\n",
      "Batch loss:  0.28411760926246643\n",
      "Batch loss:  0.1337764710187912\n",
      "Batch loss:  0.12021127343177795\n",
      "Batch loss:  0.2213563323020935\n",
      "Batch loss:  0.36072859168052673\n",
      "Batch loss:  0.18258367478847504\n",
      "Batch loss:  0.29049813747406006\n",
      "Batch loss:  0.14268799126148224\n",
      "Batch loss:  0.14976878464221954\n",
      "Batch loss:  0.11874140053987503\n",
      "Current average loss: 0.21186332974546354\n",
      "Batch loss:  0.12401093542575836\n",
      "Batch loss:  0.21463796496391296\n",
      "Batch loss:  0.15340867638587952\n",
      "Batch loss:  0.2691555917263031\n",
      "Batch loss:  0.2772540748119354\n",
      "Batch loss:  0.30819275975227356\n",
      "Batch loss:  0.1863115280866623\n",
      "Batch loss:  0.26755014061927795\n",
      "Batch loss:  0.12209440022706985\n",
      "Batch loss:  0.13592492043972015\n",
      "Batch loss:  0.1211177334189415\n",
      "Batch loss:  0.2384508103132248\n",
      "Batch loss:  0.23778071999549866\n",
      "Batch loss:  0.1676493138074875\n",
      "Batch loss:  0.17524129152297974\n",
      "Batch loss:  0.15895359218120575\n",
      "Batch loss:  0.13259448111057281\n",
      "Batch loss:  0.11736443638801575\n",
      "Batch loss:  0.12067937850952148\n",
      "Batch loss:  0.29795998334884644\n",
      "Batch loss:  0.13104049861431122\n",
      "Batch loss:  0.22247542440891266\n",
      "Batch loss:  0.20319439470767975\n",
      "Batch loss:  0.20117391645908356\n",
      "Batch loss:  0.24616114795207977\n",
      "Batch loss:  0.1333390325307846\n",
      "Batch loss:  0.22362704575061798\n",
      "Batch loss:  0.10310577601194382\n",
      "Batch loss:  0.15417692065238953\n",
      "Batch loss:  0.1488444209098816\n",
      "Batch loss:  0.2165439873933792\n",
      "Batch loss:  0.14542198181152344\n",
      "Batch loss:  0.08984476327896118\n",
      "Batch loss:  0.27741068601608276\n",
      "Batch loss:  0.2969442903995514\n",
      "Batch loss:  0.1425100713968277\n",
      "Batch loss:  0.2745758891105652\n",
      "Batch loss:  0.2230941504240036\n",
      "Batch loss:  0.07474678754806519\n",
      "Batch loss:  0.1777201145887375\n",
      "Batch loss:  0.11664676666259766\n",
      "Batch loss:  0.36072033643722534\n",
      "Batch loss:  0.18951116502285004\n",
      "Batch loss:  0.19587713479995728\n",
      "Batch loss:  0.17787303030490875\n",
      "Batch loss:  0.19358251988887787\n",
      "Batch loss:  0.17042703926563263\n",
      "Batch loss:  0.2362114042043686\n",
      "Batch loss:  0.3911307156085968\n",
      "Batch loss:  0.29689452052116394\n",
      "Batch loss:  0.10084056854248047\n",
      "Batch loss:  0.25179824233055115\n",
      "Batch loss:  0.1576179713010788\n",
      "Batch loss:  0.19400164484977722\n",
      "Batch loss:  0.3069285452365875\n",
      "Batch loss:  0.14419029653072357\n",
      "Batch loss:  0.15131504833698273\n",
      "Batch loss:  0.12517598271369934\n",
      "Batch loss:  0.10484815388917923\n",
      "Batch loss:  0.18309997022151947\n",
      "Batch loss:  0.16488397121429443\n",
      "Batch loss:  0.11695562303066254\n",
      "Batch loss:  0.15469153225421906\n",
      "Batch loss:  0.2108979970216751\n",
      "Batch loss:  0.21988284587860107\n",
      "Batch loss:  0.3466949760913849\n",
      "Batch loss:  0.12707537412643433\n",
      "Batch loss:  0.25339362025260925\n",
      "Batch loss:  0.2451227456331253\n",
      "Batch loss:  0.09013675153255463\n",
      "Batch loss:  0.24454009532928467\n",
      "Batch loss:  0.13481652736663818\n",
      "Batch loss:  0.1655043214559555\n",
      "Batch loss:  0.19966797530651093\n",
      "Batch loss:  0.09695971012115479\n",
      "Batch loss:  0.15309756994247437\n",
      "Batch loss:  0.44398701190948486\n",
      "Batch loss:  0.13143913447856903\n",
      "Batch loss:  0.38291189074516296\n",
      "Batch loss:  0.2508205473423004\n",
      "Batch loss:  0.19440294802188873\n",
      "Batch loss:  0.11044051498174667\n",
      "Batch loss:  0.14807304739952087\n",
      "Batch loss:  0.1451503187417984\n",
      "Batch loss:  0.23054495453834534\n",
      "Batch loss:  0.18732675909996033\n",
      "Batch loss:  0.12303312867879868\n",
      "Batch loss:  0.11734290421009064\n",
      "Batch loss:  0.2421124279499054\n",
      "Batch loss:  0.15990260243415833\n",
      "Batch loss:  0.12244278937578201\n",
      "Batch loss:  0.3726867139339447\n",
      "Batch loss:  0.1427866816520691\n",
      "Batch loss:  0.18498429656028748\n",
      "Batch loss:  0.1496700495481491\n",
      "Batch loss:  0.10449515283107758\n",
      "Batch loss:  0.13373248279094696\n",
      "Batch loss:  0.24064742028713226\n",
      "Batch loss:  0.17536628246307373\n",
      "Batch loss:  0.18137890100479126\n",
      "Current average loss: 0.21149514990773574\n",
      "Batch loss:  0.0905759260058403\n",
      "Batch loss:  0.15539981424808502\n",
      "Batch loss:  0.17441004514694214\n",
      "Batch loss:  0.20943738520145416\n",
      "Batch loss:  0.20990927517414093\n",
      "Batch loss:  0.3559791147708893\n",
      "Batch loss:  0.26893195509910583\n",
      "Batch loss:  0.25438863039016724\n",
      "Batch loss:  0.1954573541879654\n",
      "Batch loss:  0.32360002398490906\n",
      "Batch loss:  0.20788781344890594\n",
      "Batch loss:  0.2438478022813797\n",
      "Batch loss:  0.23455217480659485\n",
      "Batch loss:  0.08695842325687408\n",
      "Batch loss:  0.2635498642921448\n",
      "Batch loss:  0.16471892595291138\n",
      "Batch loss:  0.15028132498264313\n",
      "Batch loss:  0.18399304151535034\n",
      "Batch loss:  0.08518629521131516\n",
      "Batch loss:  0.618083119392395\n",
      "Batch loss:  0.14011473953723907\n",
      "Batch loss:  0.2540166974067688\n",
      "Batch loss:  0.4191354811191559\n",
      "Batch loss:  0.2867715060710907\n",
      "Batch loss:  0.16383247077465057\n",
      "Batch loss:  0.40799811482429504\n",
      "Batch loss:  0.19047150015830994\n",
      "Batch loss:  0.22968289256095886\n",
      "Batch loss:  0.14739099144935608\n",
      "Batch loss:  0.13211409747600555\n",
      "Batch loss:  0.31260842084884644\n",
      "Batch loss:  0.2580176293849945\n",
      "Batch loss:  0.24359534680843353\n",
      "Batch loss:  0.17656730115413666\n",
      "Batch loss:  0.07706951349973679\n",
      "Batch loss:  0.3017451763153076\n",
      "Batch loss:  0.24534058570861816\n",
      "Batch loss:  0.17441391944885254\n",
      "Batch loss:  0.1150665208697319\n",
      "Batch loss:  0.23483464121818542\n",
      "Batch loss:  0.25230076909065247\n",
      "Batch loss:  0.10196184366941452\n",
      "Batch loss:  0.21702790260314941\n",
      "Batch loss:  0.13382220268249512\n",
      "Batch loss:  0.2115103006362915\n",
      "Batch loss:  0.20644737780094147\n",
      "Batch loss:  0.2969663441181183\n",
      "Batch loss:  0.2754911780357361\n",
      "Batch loss:  0.21536493301391602\n",
      "Batch loss:  0.24395820498466492\n",
      "Batch loss:  0.2200029492378235\n",
      "Batch loss:  0.23613618314266205\n",
      "Batch loss:  0.18390808999538422\n",
      "Batch loss:  0.09950517117977142\n",
      "Batch loss:  0.2454061359167099\n",
      "Batch loss:  0.173819437623024\n",
      "Batch loss:  0.1490691751241684\n",
      "Batch loss:  0.16847647726535797\n",
      "Batch loss:  0.15512202680110931\n",
      "Batch loss:  0.059343501925468445\n",
      "Batch loss:  0.2579720914363861\n",
      "Batch loss:  0.19342686235904694\n",
      "Batch loss:  0.09251811355352402\n",
      "Batch loss:  0.15276572108268738\n",
      "Batch loss:  0.12257687747478485\n",
      "Batch loss:  0.14557301998138428\n",
      "Batch loss:  0.22437722980976105\n",
      "Batch loss:  0.2812008559703827\n",
      "Batch loss:  0.1714443564414978\n",
      "Batch loss:  0.10535769909620285\n",
      "Batch loss:  0.2528570294380188\n",
      "Batch loss:  0.09203799813985825\n",
      "Batch loss:  0.1698598563671112\n",
      "Batch loss:  0.221815824508667\n",
      "Batch loss:  0.255634605884552\n",
      "Batch loss:  0.22315877676010132\n",
      "Batch loss:  0.1436481773853302\n",
      "Batch loss:  0.08400506526231766\n",
      "Batch loss:  0.10094498842954636\n",
      "Batch loss:  0.11409930884838104\n",
      "Batch loss:  0.4189109206199646\n",
      "Batch loss:  0.24628446996212006\n",
      "Batch loss:  0.12714119255542755\n",
      "Batch loss:  0.19290752708911896\n",
      "Batch loss:  0.139793261885643\n",
      "Batch loss:  0.21901610493659973\n",
      "Batch loss:  0.3361571729183197\n",
      "Batch loss:  0.18196845054626465\n",
      "Batch loss:  0.17937235534191132\n",
      "Batch loss:  0.15400132536888123\n",
      "Batch loss:  0.26098600029945374\n",
      "Batch loss:  0.13699693977832794\n",
      "Batch loss:  0.11107442528009415\n",
      "Batch loss:  0.17968912422657013\n",
      "Batch loss:  0.3043997883796692\n",
      "Batch loss:  0.11975482106208801\n",
      "Batch loss:  0.38917237520217896\n",
      "Batch loss:  0.2426319718360901\n",
      "Batch loss:  0.2405555546283722\n",
      "Batch loss:  0.2756688594818115\n",
      "Current average loss: 0.2114140604897187\n",
      "Batch loss:  0.17979755997657776\n",
      "Batch loss:  0.2090575098991394\n",
      "Batch loss:  0.23506475985050201\n",
      "Batch loss:  0.06932337582111359\n",
      "Batch loss:  0.12418441474437714\n",
      "Batch loss:  0.10768238455057144\n",
      "Batch loss:  0.19798479974269867\n",
      "Batch loss:  0.1069413274526596\n",
      "Batch loss:  0.16725100576877594\n",
      "Batch loss:  0.12169218808412552\n",
      "Batch loss:  0.28016185760498047\n",
      "Batch loss:  0.16928842663764954\n",
      "Batch loss:  0.2650589346885681\n",
      "Batch loss:  0.16488152742385864\n",
      "Batch loss:  0.24774324893951416\n",
      "Batch loss:  0.2361699491739273\n",
      "Batch loss:  0.11253131181001663\n",
      "Batch loss:  0.12507112324237823\n",
      "Batch loss:  0.18137367069721222\n",
      "Batch loss:  0.2549297511577606\n",
      "Batch loss:  0.17666539549827576\n",
      "Batch loss:  0.10899312794208527\n",
      "Batch loss:  0.1442619264125824\n",
      "Batch loss:  0.13634926080703735\n",
      "Batch loss:  0.3972648084163666\n",
      "Batch loss:  0.2049993872642517\n",
      "Batch loss:  0.17173700034618378\n",
      "Batch loss:  0.19615887105464935\n",
      "Batch loss:  0.09865428507328033\n",
      "Batch loss:  0.4075425863265991\n",
      "Batch loss:  0.17285649478435516\n",
      "Batch loss:  0.15366831421852112\n",
      "Batch loss:  0.32101696729660034\n",
      "Batch loss:  0.26421457529067993\n",
      "Batch loss:  0.17044763267040253\n",
      "Batch loss:  0.32564422488212585\n",
      "Batch loss:  0.11515775322914124\n",
      "Batch loss:  0.24409376084804535\n",
      "Batch loss:  0.15535694360733032\n",
      "Batch loss:  0.23839278519153595\n",
      "Batch loss:  0.15897905826568604\n",
      "Batch loss:  0.18028604984283447\n",
      "Batch loss:  0.14062562584877014\n",
      "Batch loss:  0.17165084183216095\n",
      "Batch loss:  0.29509347677230835\n",
      "Batch loss:  0.14914114773273468\n",
      "Batch loss:  0.3040880560874939\n",
      "Batch loss:  0.13761483132839203\n",
      "Batch loss:  0.10192736983299255\n",
      "Batch loss:  0.3444864749908447\n",
      "Batch loss:  0.18207824230194092\n",
      "Batch loss:  0.1525510847568512\n",
      "Batch loss:  0.15246619284152985\n",
      "Batch loss:  0.252666175365448\n",
      "Batch loss:  0.24852833151817322\n",
      "Batch loss:  0.20145709812641144\n",
      "Batch loss:  0.27716606855392456\n",
      "Batch loss:  0.16557225584983826\n",
      "Batch loss:  0.23246915638446808\n",
      "Batch loss:  0.15813104808330536\n",
      "Batch loss:  0.14007553458213806\n",
      "Batch loss:  0.27722063660621643\n",
      "Batch loss:  0.19705702364444733\n",
      "Batch loss:  0.07341501116752625\n",
      "Batch loss:  0.15164993703365326\n",
      "Batch loss:  0.16994133591651917\n",
      "Batch loss:  0.27141720056533813\n",
      "Batch loss:  0.10285165160894394\n",
      "Batch loss:  0.22826148569583893\n",
      "Batch loss:  0.2629637122154236\n",
      "Batch loss:  0.2514248490333557\n",
      "Batch loss:  0.182242751121521\n",
      "Batch loss:  0.15699847042560577\n",
      "Batch loss:  0.13874848186969757\n",
      "Batch loss:  0.22901031374931335\n",
      "Batch loss:  0.2810618579387665\n",
      "Batch loss:  0.3410511910915375\n",
      "Batch loss:  0.15275271236896515\n",
      "Batch loss:  0.22211967408657074\n",
      "Batch loss:  0.14674755930900574\n",
      "Batch loss:  0.16230028867721558\n",
      "Batch loss:  0.18075509369373322\n",
      "Batch loss:  0.18323467671871185\n",
      "Batch loss:  0.22923755645751953\n",
      "Batch loss:  0.10193710029125214\n",
      "Batch loss:  0.251987099647522\n",
      "Batch loss:  0.0921817347407341\n",
      "Batch loss:  0.504014253616333\n",
      "Batch loss:  0.10773841291666031\n",
      "Batch loss:  0.1470838040113449\n",
      "Batch loss:  0.27477386593818665\n",
      "Batch loss:  0.16398893296718597\n",
      "Batch loss:  0.2345990091562271\n",
      "Batch loss:  0.26526281237602234\n",
      "Batch loss:  0.17042328417301178\n",
      "Batch loss:  0.30519402027130127\n",
      "Batch loss:  0.17188574373722076\n",
      "Batch loss:  0.101832315325737\n",
      "Batch loss:  0.33678627014160156\n",
      "Batch loss:  0.22019630670547485\n",
      "Current average loss: 0.21120491503056488\n",
      "Batch loss:  0.12208181619644165\n",
      "Batch loss:  0.15754535794258118\n",
      "Batch loss:  0.11696809530258179\n",
      "Batch loss:  0.22861434519290924\n",
      "Batch loss:  0.32711556553840637\n",
      "Batch loss:  0.2140238881111145\n",
      "Batch loss:  0.16856816411018372\n",
      "Batch loss:  0.1356208175420761\n",
      "Batch loss:  0.19229653477668762\n",
      "Batch loss:  0.2559468746185303\n",
      "Batch loss:  0.10835573822259903\n",
      "Batch loss:  0.22772061824798584\n",
      "Batch loss:  0.15577760338783264\n",
      "Batch loss:  0.23927655816078186\n",
      "Batch loss:  0.27098220586776733\n",
      "Batch loss:  0.2620203197002411\n",
      "Batch loss:  0.14737214148044586\n",
      "Batch loss:  0.2188069373369217\n",
      "Batch loss:  0.06484084576368332\n",
      "Batch loss:  0.10246085375547409\n",
      "Batch loss:  0.18040022253990173\n",
      "Batch loss:  0.266496479511261\n",
      "Batch loss:  0.2397322803735733\n",
      "Batch loss:  0.2005120813846588\n",
      "Batch loss:  0.2239745706319809\n",
      "Batch loss:  0.22884225845336914\n",
      "Batch loss:  0.2108546793460846\n",
      "Batch loss:  0.2532598376274109\n",
      "Batch loss:  0.4107614755630493\n",
      "Batch loss:  0.2593190371990204\n",
      "Batch loss:  0.2452143281698227\n",
      "Batch loss:  0.2651020586490631\n",
      "Batch loss:  0.20253199338912964\n",
      "Batch loss:  0.17106491327285767\n",
      "Batch loss:  0.24540242552757263\n",
      "Batch loss:  0.25029289722442627\n",
      "Batch loss:  0.15837638080120087\n",
      "Batch loss:  0.17702923715114594\n",
      "Batch loss:  0.15194222331047058\n",
      "Batch loss:  0.22418291866779327\n",
      "Batch loss:  0.20050032436847687\n",
      "Batch loss:  0.2251175045967102\n",
      "Batch loss:  0.17200933396816254\n",
      "Batch loss:  0.1444963663816452\n",
      "Batch loss:  0.2510662078857422\n",
      "Batch loss:  0.2455292046070099\n",
      "Batch loss:  0.13501445949077606\n",
      "Batch loss:  0.3400847911834717\n",
      "Batch loss:  0.1611376702785492\n",
      "Batch loss:  0.1504567414522171\n",
      "Batch loss:  0.2896476089954376\n",
      "Batch loss:  0.3282700181007385\n",
      "Batch loss:  0.1641642302274704\n",
      "Batch loss:  0.14619791507720947\n",
      "Batch loss:  0.2954069972038269\n",
      "Batch loss:  0.12402917444705963\n",
      "Batch loss:  0.18496140837669373\n",
      "Batch loss:  0.24795755743980408\n",
      "Batch loss:  0.2057553380727768\n",
      "Batch loss:  0.1705692857503891\n",
      "Batch loss:  0.40174925327301025\n",
      "Batch loss:  0.11433883756399155\n",
      "Batch loss:  0.09481920301914215\n",
      "Batch loss:  0.24116568267345428\n",
      "Batch loss:  0.12760329246520996\n",
      "Batch loss:  0.1956491768360138\n",
      "Batch loss:  0.24834151566028595\n",
      "Batch loss:  0.1970585733652115\n",
      "Batch loss:  0.2453421950340271\n",
      "Batch loss:  0.11117159575223923\n",
      "Batch loss:  0.1065995842218399\n",
      "Batch loss:  0.13588687777519226\n",
      "Batch loss:  0.28157034516334534\n",
      "Batch loss:  0.11468972265720367\n",
      "Batch loss:  0.2463330626487732\n",
      "Batch loss:  0.18246611952781677\n",
      "Batch loss:  0.1424756646156311\n",
      "Batch loss:  0.3010883331298828\n",
      "Batch loss:  0.16677191853523254\n",
      "Batch loss:  0.2461550235748291\n",
      "Batch loss:  0.28256484866142273\n",
      "Batch loss:  0.09047063440084457\n",
      "Batch loss:  0.1988973170518875\n",
      "Batch loss:  0.23268960416316986\n",
      "Batch loss:  0.16529671847820282\n",
      "Batch loss:  0.1449522227048874\n",
      "Batch loss:  0.15584588050842285\n",
      "Batch loss:  0.17592354118824005\n",
      "Batch loss:  0.12581154704093933\n",
      "Batch loss:  0.24127043783664703\n",
      "Batch loss:  0.23112955689430237\n",
      "Batch loss:  0.17560677230358124\n",
      "Batch loss:  0.23742100596427917\n",
      "Batch loss:  0.14628072082996368\n",
      "Batch loss:  0.27277621626853943\n",
      "Batch loss:  0.18482279777526855\n",
      "Batch loss:  0.29449549317359924\n",
      "Batch loss:  0.0612093023955822\n",
      "Batch loss:  0.2445661872625351\n",
      "Batch loss:  0.12911388278007507\n",
      "Current average loss: 0.21103873486853617\n",
      "Batch loss:  0.11571579426527023\n",
      "Batch loss:  0.1288287192583084\n",
      "Batch loss:  0.2221185714006424\n",
      "Batch loss:  0.3473793566226959\n",
      "Batch loss:  0.1136251837015152\n",
      "Batch loss:  0.23790153861045837\n",
      "Batch loss:  0.2538343369960785\n",
      "Batch loss:  0.19925889372825623\n",
      "Batch loss:  0.4920005202293396\n",
      "Batch loss:  0.09676404297351837\n",
      "Batch loss:  0.12797948718070984\n",
      "Batch loss:  0.3070075809955597\n",
      "Batch loss:  0.11026518791913986\n",
      "Batch loss:  0.13344348967075348\n",
      "Batch loss:  0.3124449849128723\n",
      "Batch loss:  0.16181470453739166\n",
      "Batch loss:  0.3393761217594147\n",
      "Batch loss:  0.0808354914188385\n",
      "Batch loss:  0.13103854656219482\n",
      "Batch loss:  0.15923288464546204\n",
      "Batch loss:  0.3054402470588684\n",
      "Batch loss:  0.12855948507785797\n",
      "Batch loss:  0.1729860007762909\n",
      "Batch loss:  0.29859593510627747\n",
      "Batch loss:  0.2880890667438507\n",
      "Batch loss:  0.30995410680770874\n",
      "Batch loss:  0.30764952301979065\n",
      "Batch loss:  0.14823222160339355\n",
      "Batch loss:  0.20472508668899536\n",
      "Batch loss:  0.1703140288591385\n",
      "Batch loss:  0.13161402940750122\n",
      "Batch loss:  0.12902608513832092\n",
      "Batch loss:  0.20250242948532104\n",
      "Batch loss:  0.2807435989379883\n",
      "Batch loss:  0.16881580650806427\n",
      "Batch loss:  0.13033059239387512\n",
      "Batch loss:  0.27157026529312134\n",
      "Batch loss:  0.21159306168556213\n",
      "Batch loss:  0.1773957461118698\n",
      "Batch loss:  0.1990259885787964\n",
      "Batch loss:  0.26221686601638794\n",
      "Batch loss:  0.16285598278045654\n",
      "Batch loss:  0.22964385151863098\n",
      "Batch loss:  0.11583863198757172\n",
      "Batch loss:  0.15772779285907745\n",
      "Batch loss:  0.1390247642993927\n",
      "Batch loss:  0.19439741969108582\n",
      "Batch loss:  0.1916658580303192\n",
      "Batch loss:  0.2254272699356079\n",
      "Batch loss:  0.3020167946815491\n",
      "Batch loss:  0.252056747674942\n",
      "Batch loss:  0.3090486228466034\n",
      "Batch loss:  0.1580042690038681\n",
      "Batch loss:  0.2063007950782776\n",
      "Batch loss:  0.21970704197883606\n",
      "Batch loss:  0.27948901057243347\n",
      "Batch loss:  0.2169530689716339\n",
      "Batch loss:  0.12528033554553986\n",
      "Batch loss:  0.26971349120140076\n",
      "Batch loss:  0.15396319329738617\n",
      "Batch loss:  0.16471301019191742\n",
      "Batch loss:  0.16551858186721802\n",
      "Batch loss:  0.24686749279499054\n",
      "Batch loss:  0.17464128136634827\n",
      "Batch loss:  0.34365594387054443\n",
      "Batch loss:  0.23748628795146942\n",
      "Batch loss:  0.18081940710544586\n",
      "Batch loss:  0.7495307922363281\n",
      "Batch loss:  0.08320727944374084\n",
      "Batch loss:  0.1694706678390503\n",
      "Batch loss:  0.1483413577079773\n",
      "Batch loss:  0.19631372392177582\n",
      "Batch loss:  0.32359829545021057\n",
      "Batch loss:  0.16650982201099396\n",
      "Batch loss:  0.2626592516899109\n",
      "Batch loss:  0.28534409403800964\n",
      "Batch loss:  0.28215858340263367\n",
      "Batch loss:  0.211944118142128\n",
      "Batch loss:  0.20998771488666534\n",
      "Batch loss:  0.22234387695789337\n",
      "Batch loss:  0.17516955733299255\n",
      "Batch loss:  0.17761288583278656\n",
      "Batch loss:  0.18429933488368988\n",
      "Batch loss:  0.19576877355575562\n",
      "Batch loss:  0.351896196603775\n",
      "Batch loss:  0.22449713945388794\n",
      "Batch loss:  0.16424962878227234\n",
      "Batch loss:  0.337145060300827\n",
      "Batch loss:  0.26092931628227234\n",
      "Batch loss:  0.15945546329021454\n",
      "Batch loss:  0.18554246425628662\n",
      "Batch loss:  0.09871773421764374\n",
      "Batch loss:  0.12649154663085938\n",
      "Batch loss:  0.07603668421506882\n",
      "Batch loss:  0.24035298824310303\n",
      "Batch loss:  0.187771275639534\n",
      "Batch loss:  0.16503788530826569\n",
      "Batch loss:  0.18537206947803497\n",
      "Batch loss:  0.20134449005126953\n",
      "Batch loss:  0.200337752699852\n",
      "Current average loss: 0.21105544845838908\n",
      "Batch loss:  0.2329491525888443\n",
      "Batch loss:  0.14469467103481293\n",
      "Batch loss:  0.21032646298408508\n",
      "Batch loss:  0.3174661099910736\n",
      "Batch loss:  0.2781429886817932\n",
      "Batch loss:  0.15331299602985382\n",
      "Batch loss:  0.2710009515285492\n",
      "Batch loss:  0.2292904406785965\n",
      "Batch loss:  0.1218767911195755\n",
      "Batch loss:  0.21352824568748474\n",
      "Batch loss:  0.22664688527584076\n",
      "Batch loss:  0.12874320149421692\n",
      "Batch loss:  0.1808682680130005\n",
      "Batch loss:  0.12655551731586456\n",
      "Batch loss:  0.23968958854675293\n",
      "Batch loss:  0.12186676263809204\n",
      "Batch loss:  0.2556367814540863\n",
      "Batch loss:  0.1600179374217987\n",
      "Batch loss:  0.2375980019569397\n",
      "Batch loss:  0.18333101272583008\n",
      "Batch loss:  0.14977958798408508\n",
      "Batch loss:  0.16654877364635468\n",
      "Batch loss:  0.1908234804868698\n",
      "Batch loss:  0.19690318405628204\n",
      "Batch loss:  0.18083596229553223\n",
      "Batch loss:  0.12415186315774918\n",
      "Batch loss:  0.11956513673067093\n",
      "Batch loss:  0.18283897638320923\n",
      "Batch loss:  0.22676919400691986\n",
      "Batch loss:  0.24711275100708008\n",
      "Batch loss:  0.17191492021083832\n",
      "Batch loss:  0.1651901751756668\n",
      "Batch loss:  0.21754223108291626\n",
      "Batch loss:  0.07985499501228333\n",
      "Batch loss:  0.11146244406700134\n",
      "Batch loss:  0.18270088732242584\n",
      "Batch loss:  0.2113184928894043\n",
      "Batch loss:  0.1818222999572754\n",
      "Batch loss:  0.1839519888162613\n",
      "Batch loss:  0.19242511689662933\n",
      "Batch loss:  0.11269828677177429\n",
      "Batch loss:  0.12664707005023956\n",
      "Batch loss:  0.21753528714179993\n",
      "Batch loss:  0.10870159417390823\n",
      "Batch loss:  0.11929847300052643\n",
      "Batch loss:  0.332204669713974\n",
      "Batch loss:  0.20658713579177856\n",
      "Batch loss:  0.2724064886569977\n",
      "Batch loss:  0.25587543845176697\n",
      "Batch loss:  0.2362620234489441\n",
      "Batch loss:  0.1793956607580185\n",
      "Batch loss:  0.39156031608581543\n",
      "Batch loss:  0.3472241759300232\n",
      "Batch loss:  0.20351748168468475\n",
      "Batch loss:  0.14992889761924744\n",
      "Batch loss:  0.05195291340351105\n",
      "Batch loss:  0.16082686185836792\n",
      "Batch loss:  0.11473299562931061\n",
      "Batch loss:  0.30004817247390747\n",
      "Batch loss:  0.18442480266094208\n",
      "Batch loss:  0.3245544731616974\n",
      "Batch loss:  0.2331864833831787\n",
      "Batch loss:  0.30952027440071106\n",
      "Batch loss:  0.165948748588562\n",
      "Batch loss:  0.24072401225566864\n",
      "Batch loss:  0.2839260697364807\n",
      "Batch loss:  0.07363976538181305\n",
      "Batch loss:  0.1900513619184494\n",
      "Batch loss:  0.21234704554080963\n",
      "Batch loss:  0.25531458854675293\n",
      "Batch loss:  0.21578086912631989\n",
      "Batch loss:  0.1419016718864441\n",
      "Batch loss:  0.18917317688465118\n",
      "Batch loss:  0.17385102808475494\n",
      "Batch loss:  0.21972760558128357\n",
      "Batch loss:  0.19267453253269196\n",
      "Batch loss:  0.32817453145980835\n",
      "Batch loss:  0.17771050333976746\n",
      "Batch loss:  0.24518683552742004\n",
      "Batch loss:  0.19588057696819305\n",
      "Batch loss:  0.18627820909023285\n",
      "Batch loss:  0.20038223266601562\n",
      "Batch loss:  0.1961822658777237\n",
      "Batch loss:  0.2677101790904999\n",
      "Batch loss:  0.21446631848812103\n",
      "Batch loss:  0.17166082561016083\n",
      "Batch loss:  0.13503707945346832\n",
      "Batch loss:  0.1508232057094574\n",
      "Batch loss:  0.21581801772117615\n",
      "Batch loss:  0.2986837923526764\n",
      "Batch loss:  0.17891402542591095\n",
      "Batch loss:  0.09917587786912918\n",
      "Batch loss:  0.20277643203735352\n",
      "Batch loss:  0.10528425127267838\n",
      "Batch loss:  0.13201211392879486\n",
      "Batch loss:  0.2493799924850464\n",
      "Batch loss:  0.07213450968265533\n",
      "Batch loss:  0.08089296519756317\n",
      "Batch loss:  0.2189071923494339\n",
      "Batch loss:  0.24456943571567535\n",
      "Current average loss: 0.21080110788250978\n",
      "Batch loss:  0.25357717275619507\n",
      "Batch loss:  0.24577036499977112\n",
      "Batch loss:  0.335962176322937\n",
      "Batch loss:  0.18155217170715332\n",
      "Batch loss:  0.10777147114276886\n",
      "Batch loss:  0.28548184037208557\n",
      "Batch loss:  0.18209603428840637\n",
      "Batch loss:  0.2168354094028473\n",
      "Batch loss:  0.23648200929164886\n",
      "Batch loss:  0.15354964137077332\n",
      "Batch loss:  0.10780790448188782\n",
      "Batch loss:  0.24247989058494568\n",
      "Batch loss:  0.17992918193340302\n",
      "Batch loss:  0.19623135030269623\n",
      "Batch loss:  0.15828797221183777\n",
      "Batch loss:  0.11669629067182541\n",
      "Batch loss:  0.30714520812034607\n",
      "Batch loss:  0.19722920656204224\n",
      "Batch loss:  0.15036994218826294\n",
      "Batch loss:  0.1763114184141159\n",
      "Batch loss:  0.2137637883424759\n",
      "Batch loss:  0.21685020625591278\n",
      "Batch loss:  0.20358781516551971\n",
      "Batch loss:  0.24052532017230988\n",
      "Batch loss:  0.16962675750255585\n",
      "Batch loss:  0.21891386806964874\n",
      "Batch loss:  0.1308526247739792\n",
      "Batch loss:  0.20455269515514374\n",
      "Batch loss:  0.16921988129615784\n",
      "Batch loss:  0.2213028073310852\n",
      "Batch loss:  0.28116923570632935\n",
      "Batch loss:  0.2865935266017914\n",
      "Batch loss:  0.4249909818172455\n",
      "Batch loss:  0.2740037441253662\n",
      "Batch loss:  0.06990887224674225\n",
      "Batch loss:  0.20294414460659027\n",
      "Batch loss:  0.13978299498558044\n",
      "Batch loss:  0.21632419526576996\n",
      "Batch loss:  0.2546139061450958\n",
      "Batch loss:  0.08559031039476395\n",
      "Batch loss:  0.2034626454114914\n",
      "Batch loss:  0.14956162869930267\n",
      "Batch loss:  0.1996922791004181\n",
      "Batch loss:  0.28514739871025085\n",
      "Batch loss:  0.4505065083503723\n",
      "Batch loss:  0.1534150391817093\n",
      "Batch loss:  0.2510325610637665\n",
      "Batch loss:  0.1205526813864708\n",
      "Batch loss:  0.1486433893442154\n",
      "Batch loss:  0.14336839318275452\n",
      "Batch loss:  0.1852385699748993\n",
      "Batch loss:  0.1344234049320221\n",
      "Batch loss:  0.11218328028917313\n",
      "Batch loss:  0.2549028992652893\n",
      "Batch loss:  0.1190035492181778\n",
      "Batch loss:  0.5758394598960876\n",
      "Batch loss:  0.3530348241329193\n",
      "Batch loss:  0.1676320731639862\n",
      "Batch loss:  0.24032579362392426\n",
      "Batch loss:  0.1383170485496521\n",
      "Batch loss:  0.19149485230445862\n",
      "Batch loss:  0.19953706860542297\n",
      "Batch loss:  0.19151821732521057\n",
      "Batch loss:  0.1453113704919815\n",
      "Batch loss:  0.1529041975736618\n",
      "Batch loss:  0.22220462560653687\n",
      "Batch loss:  0.1803303211927414\n",
      "Batch loss:  0.11054705828428268\n",
      "Batch loss:  0.2436864972114563\n",
      "Batch loss:  0.11898121237754822\n",
      "Batch loss:  0.2508258521556854\n",
      "Batch loss:  0.1653497964143753\n",
      "Batch loss:  0.1420484483242035\n",
      "Batch loss:  0.22971799969673157\n",
      "Batch loss:  0.1209198385477066\n",
      "Batch loss:  0.24188099801540375\n",
      "Batch loss:  0.1668415516614914\n",
      "Batch loss:  0.1639605611562729\n",
      "Batch loss:  0.19273284077644348\n",
      "Batch loss:  0.13845942914485931\n",
      "Batch loss:  0.2410762906074524\n",
      "Batch loss:  0.2426367551088333\n",
      "Batch loss:  0.2167114019393921\n",
      "Batch loss:  0.10919034481048584\n",
      "Batch loss:  0.2006254494190216\n",
      "Batch loss:  0.2517285645008087\n",
      "Batch loss:  0.1581726372241974\n",
      "Batch loss:  0.1688731163740158\n",
      "Batch loss:  0.15108978748321533\n",
      "Batch loss:  0.1373201161623001\n",
      "Batch loss:  0.16193559765815735\n",
      "Batch loss:  0.21751879155635834\n",
      "Batch loss:  0.2012663036584854\n",
      "Batch loss:  0.17890922725200653\n",
      "Batch loss:  0.2379940003156662\n",
      "Batch loss:  0.1368628889322281\n",
      "Batch loss:  0.22789423167705536\n",
      "Batch loss:  0.25696077942848206\n",
      "Batch loss:  0.2581538259983063\n",
      "Batch loss:  0.15487855672836304\n",
      "Current average loss: 0.2106440694258655\n",
      "Batch loss:  0.3613698482513428\n",
      "Batch loss:  0.17977671325206757\n",
      "Batch loss:  0.09288094192743301\n",
      "Batch loss:  0.145011767745018\n",
      "Batch loss:  0.15013621747493744\n",
      "Batch loss:  0.1536717265844345\n",
      "Batch loss:  0.12773045897483826\n",
      "Batch loss:  0.09640142321586609\n",
      "Batch loss:  0.27908679842948914\n",
      "Batch loss:  0.14134031534194946\n",
      "Batch loss:  0.22429531812667847\n",
      "Batch loss:  0.22393611073493958\n",
      "Batch loss:  0.38183629512786865\n",
      "Batch loss:  0.14437517523765564\n",
      "Batch loss:  0.2547593116760254\n",
      "Batch loss:  0.20738737285137177\n",
      "Batch loss:  0.2901988923549652\n",
      "Batch loss:  0.11751895397901535\n",
      "Batch loss:  0.25851571559906006\n",
      "Batch loss:  0.2885262370109558\n",
      "Batch loss:  0.22409312427043915\n",
      "Batch loss:  0.19253559410572052\n",
      "Batch loss:  0.12733009457588196\n",
      "Batch loss:  0.15965062379837036\n",
      "Batch loss:  0.18041421473026276\n",
      "Batch loss:  0.13594846427440643\n",
      "Batch loss:  0.12720729410648346\n",
      "Batch loss:  0.2673814594745636\n",
      "Batch loss:  0.26213371753692627\n",
      "Batch loss:  0.12334033101797104\n",
      "Batch loss:  0.3025140166282654\n",
      "Batch loss:  0.14846695959568024\n",
      "Batch loss:  0.1237962543964386\n",
      "Batch loss:  0.2220831960439682\n",
      "Batch loss:  0.1607055515050888\n",
      "Batch loss:  0.28568610548973083\n",
      "Batch loss:  0.23390254378318787\n",
      "Batch loss:  0.2699663043022156\n",
      "Batch loss:  0.3085462749004364\n",
      "Batch loss:  0.1631094217300415\n",
      "Batch loss:  0.1301802694797516\n",
      "Batch loss:  0.23609821498394012\n",
      "Batch loss:  0.23949213325977325\n",
      "Batch loss:  0.21614302694797516\n",
      "Batch loss:  0.07854489237070084\n",
      "Batch loss:  0.25272253155708313\n",
      "Batch loss:  0.17411234974861145\n",
      "Batch loss:  0.19800028204917908\n",
      "Batch loss:  0.2756301164627075\n",
      "Batch loss:  0.08616147190332413\n",
      "Batch loss:  0.1748204380273819\n",
      "Batch loss:  0.26795560121536255\n",
      "Batch loss:  0.2881077826023102\n",
      "Batch loss:  0.19938728213310242\n",
      "Batch loss:  0.32850056886672974\n",
      "Batch loss:  0.4281928241252899\n",
      "Batch loss:  0.17351019382476807\n",
      "Batch loss:  0.23583903908729553\n",
      "Batch loss:  0.2904055118560791\n",
      "Batch loss:  0.1480521410703659\n",
      "Batch loss:  0.19649051129817963\n",
      "Batch loss:  0.11036447435617447\n",
      "Batch loss:  0.31001752614974976\n",
      "Batch loss:  0.18224577605724335\n",
      "Batch loss:  0.14610135555267334\n",
      "Batch loss:  0.10821235924959183\n",
      "Batch loss:  0.1532716453075409\n",
      "Batch loss:  0.18364299833774567\n",
      "Batch loss:  0.21086791157722473\n",
      "Batch loss:  0.21207328140735626\n",
      "Batch loss:  0.133904829621315\n",
      "Batch loss:  0.1303156316280365\n",
      "Batch loss:  0.20731009542942047\n",
      "Batch loss:  0.22307395935058594\n",
      "Batch loss:  0.1194322407245636\n",
      "Batch loss:  0.16296343505382538\n",
      "Batch loss:  0.10969206690788269\n",
      "Batch loss:  0.07541346549987793\n",
      "Batch loss:  0.14928634464740753\n",
      "Batch loss:  0.2549768090248108\n",
      "Batch loss:  0.11149843782186508\n",
      "Batch loss:  0.3714492917060852\n",
      "Batch loss:  0.31488358974456787\n",
      "Batch loss:  0.1510181427001953\n",
      "Batch loss:  0.08423028886318207\n",
      "Batch loss:  0.21041256189346313\n",
      "Batch loss:  0.21459195017814636\n",
      "Batch loss:  0.23436523973941803\n",
      "Batch loss:  0.12812168896198273\n",
      "Batch loss:  0.17363755404949188\n",
      "Batch loss:  0.24706152081489563\n",
      "Batch loss:  0.19010521471500397\n",
      "Batch loss:  0.24246980249881744\n",
      "Batch loss:  0.08376983553171158\n",
      "Batch loss:  0.2678264379501343\n",
      "Batch loss:  0.29015052318573\n",
      "Batch loss:  0.22832798957824707\n",
      "Batch loss:  0.12054717540740967\n",
      "Batch loss:  0.2156057506799698\n",
      "Batch loss:  0.11479441076517105\n",
      "Current average loss: 0.210461766731641\n",
      "Batch loss:  0.23599493503570557\n",
      "Batch loss:  0.17569129168987274\n",
      "Batch loss:  0.1953001469373703\n",
      "Batch loss:  0.19758844375610352\n",
      "Batch loss:  0.22768080234527588\n",
      "Batch loss:  0.3986127972602844\n",
      "Batch loss:  0.19356468319892883\n",
      "Batch loss:  0.21996217966079712\n",
      "Batch loss:  0.14111042022705078\n",
      "Batch loss:  0.1681603342294693\n",
      "Batch loss:  0.11942214518785477\n",
      "Batch loss:  0.1467696875333786\n",
      "Batch loss:  0.12590350210666656\n",
      "Batch loss:  0.08880773186683655\n",
      "Batch loss:  0.18358294665813446\n",
      "Batch loss:  0.36714401841163635\n",
      "Batch loss:  0.14465086162090302\n",
      "Batch loss:  0.09280629456043243\n",
      "Batch loss:  0.16951891779899597\n",
      "Batch loss:  0.20564629137516022\n",
      "Batch loss:  0.08821577578783035\n",
      "Batch loss:  0.14777839183807373\n",
      "Batch loss:  0.3386760354042053\n",
      "Batch loss:  0.07368797063827515\n",
      "Batch loss:  0.16236236691474915\n",
      "Batch loss:  0.3369452655315399\n",
      "Batch loss:  0.10784845799207687\n",
      "Batch loss:  0.20453764498233795\n",
      "Batch loss:  0.1828356832265854\n",
      "Batch loss:  0.06567293405532837\n",
      "Batch loss:  0.14475490152835846\n",
      "Batch loss:  0.19553926587104797\n",
      "Batch loss:  0.12389609962701797\n",
      "Batch loss:  0.3890838027000427\n",
      "Batch loss:  0.26942914724349976\n",
      "Batch loss:  0.138003408908844\n",
      "Batch loss:  0.24365182220935822\n",
      "Batch loss:  0.1996999830007553\n",
      "Batch loss:  0.13363458216190338\n",
      "Batch loss:  0.26234856247901917\n",
      "Batch loss:  0.26570582389831543\n",
      "Batch loss:  0.20986934006214142\n",
      "Batch loss:  0.20520344376564026\n",
      "Batch loss:  0.33170872926712036\n",
      "Batch loss:  0.20241664350032806\n",
      "Batch loss:  0.26248636841773987\n",
      "Batch loss:  0.2848101258277893\n",
      "Batch loss:  0.22488252818584442\n",
      "Batch loss:  0.14598649740219116\n",
      "Batch loss:  0.10101151466369629\n",
      "Batch loss:  0.11132332682609558\n",
      "Batch loss:  0.21575024724006653\n",
      "Batch loss:  0.2884480953216553\n",
      "Batch loss:  0.19936199486255646\n",
      "Batch loss:  0.1945204734802246\n",
      "Batch loss:  0.2583949863910675\n",
      "Batch loss:  0.2935914695262909\n",
      "Batch loss:  0.14462369680404663\n",
      "Batch loss:  0.12645870447158813\n",
      "Batch loss:  0.14896397292613983\n",
      "Batch loss:  0.09623222798109055\n",
      "Batch loss:  0.10890766978263855\n",
      "Batch loss:  0.15121065080165863\n",
      "Batch loss:  0.24578189849853516\n",
      "Batch loss:  0.20111708343029022\n",
      "Batch loss:  0.22021277248859406\n",
      "Batch loss:  0.24204444885253906\n",
      "Batch loss:  0.2590517997741699\n",
      "Batch loss:  0.1893884241580963\n",
      "Batch loss:  0.05866740271449089\n",
      "Batch loss:  0.22963207960128784\n",
      "Batch loss:  0.15788742899894714\n",
      "Batch loss:  0.2772468626499176\n",
      "Batch loss:  0.26133930683135986\n",
      "Batch loss:  0.13669651746749878\n",
      "Batch loss:  0.1536990851163864\n",
      "Batch loss:  0.28425952792167664\n",
      "Batch loss:  0.20788243412971497\n",
      "Batch loss:  0.40089893341064453\n",
      "Batch loss:  0.10649478435516357\n",
      "Batch loss:  0.2664540410041809\n",
      "Batch loss:  0.17346741259098053\n",
      "Batch loss:  0.1319374144077301\n",
      "Batch loss:  0.15255983173847198\n",
      "Batch loss:  0.17743024230003357\n",
      "Batch loss:  0.26397231221199036\n",
      "Batch loss:  0.1229848563671112\n",
      "Batch loss:  0.6453450918197632\n",
      "Batch loss:  0.24825449287891388\n",
      "Batch loss:  0.14672918617725372\n",
      "Batch loss:  0.23295089602470398\n",
      "Batch loss:  0.3004748523235321\n",
      "Batch loss:  0.17279799282550812\n",
      "Batch loss:  0.4451582431793213\n",
      "Batch loss:  0.2306184470653534\n",
      "Batch loss:  0.12350353598594666\n",
      "Batch loss:  0.172169491648674\n",
      "Batch loss:  0.14268521964550018\n",
      "Batch loss:  0.2023746222257614\n",
      "Batch loss:  0.2532180845737457\n",
      "Current average loss: 0.21036171903273565\n",
      "Batch loss:  0.12150897085666656\n",
      "Batch loss:  0.1327458918094635\n",
      "Batch loss:  0.19905254244804382\n",
      "Batch loss:  0.1705581247806549\n",
      "Batch loss:  0.16574335098266602\n",
      "Batch loss:  0.11881253868341446\n",
      "Batch loss:  0.20451068878173828\n",
      "Batch loss:  0.19489218294620514\n",
      "Batch loss:  0.08694293349981308\n",
      "Batch loss:  0.610610842704773\n",
      "Batch loss:  0.16358017921447754\n",
      "Batch loss:  0.25928178429603577\n",
      "Batch loss:  0.22843895852565765\n",
      "Batch loss:  0.20146386325359344\n",
      "Batch loss:  0.09535149484872818\n",
      "Batch loss:  0.12069373577833176\n",
      "Batch loss:  0.34249770641326904\n",
      "Batch loss:  0.3311983048915863\n",
      "Batch loss:  0.1506342738866806\n",
      "Batch loss:  0.11551126837730408\n",
      "Batch loss:  0.16683870553970337\n",
      "Batch loss:  0.08602556586265564\n",
      "Batch loss:  0.21031640470027924\n",
      "Batch loss:  0.21780478954315186\n",
      "Batch loss:  0.1908729523420334\n",
      "Batch loss:  0.11763284355401993\n",
      "Batch loss:  0.15538987517356873\n",
      "Batch loss:  0.14362893998622894\n",
      "Batch loss:  0.1402771919965744\n",
      "Batch loss:  0.1385289877653122\n",
      "Batch loss:  0.1634281873703003\n",
      "Batch loss:  0.2523079514503479\n",
      "Batch loss:  0.30236145853996277\n",
      "Batch loss:  0.14083848893642426\n",
      "Batch loss:  0.14081956446170807\n",
      "Batch loss:  0.14805705845355988\n",
      "Batch loss:  0.18978317081928253\n",
      "Batch loss:  0.15253682434558868\n",
      "Batch loss:  0.2638918161392212\n",
      "Batch loss:  0.12567327916622162\n",
      "Batch loss:  0.17913135886192322\n",
      "Batch loss:  0.20765803754329681\n",
      "Batch loss:  0.18984249234199524\n",
      "Batch loss:  0.10663927346467972\n",
      "Batch loss:  0.20671413838863373\n",
      "Batch loss:  0.249852254986763\n",
      "Batch loss:  0.3245631754398346\n",
      "Batch loss:  0.2107045203447342\n",
      "Batch loss:  0.21615560352802277\n",
      "Batch loss:  0.2724652588367462\n",
      "Batch loss:  0.16126416623592377\n",
      "Batch loss:  0.1086951345205307\n",
      "Batch loss:  0.4451400339603424\n",
      "Batch loss:  0.1959061473608017\n",
      "Batch loss:  0.15358172357082367\n",
      "Batch loss:  0.6851548552513123\n",
      "Batch loss:  0.12329410761594772\n",
      "Batch loss:  0.20796369016170502\n",
      "Batch loss:  0.14311793446540833\n",
      "Batch loss:  0.13535556197166443\n",
      "Batch loss:  0.10701080411672592\n",
      "Batch loss:  0.2552569806575775\n",
      "Batch loss:  0.212283194065094\n",
      "Batch loss:  0.14000433683395386\n",
      "Batch loss:  0.14483095705509186\n",
      "Batch loss:  0.17326481640338898\n",
      "Batch loss:  0.20263125002384186\n",
      "Batch loss:  0.3740653395652771\n",
      "Batch loss:  0.28505516052246094\n",
      "Batch loss:  0.1689169853925705\n",
      "Batch loss:  0.12438417226076126\n",
      "Batch loss:  0.18716982007026672\n",
      "Batch loss:  0.1734541654586792\n",
      "Batch loss:  0.10576923936605453\n",
      "Batch loss:  0.1481935679912567\n",
      "Batch loss:  0.17700918018817902\n",
      "Batch loss:  0.3199370205402374\n",
      "Batch loss:  0.14133784174919128\n",
      "Batch loss:  0.1829894632101059\n",
      "Batch loss:  0.2850843369960785\n",
      "Batch loss:  0.20685125887393951\n",
      "Batch loss:  0.20263446867465973\n",
      "Batch loss:  0.235115185379982\n",
      "Batch loss:  0.09285636246204376\n",
      "Batch loss:  0.09827879816293716\n",
      "Batch loss:  0.2062852829694748\n",
      "Batch loss:  0.5393193364143372\n",
      "Batch loss:  0.1233169212937355\n",
      "Batch loss:  0.16665583848953247\n",
      "Batch loss:  0.1724034547805786\n",
      "Batch loss:  0.6013599634170532\n",
      "Batch loss:  0.1227078065276146\n",
      "Batch loss:  0.2988552153110504\n",
      "Batch loss:  0.118462473154068\n",
      "Batch loss:  0.12211693078279495\n",
      "Batch loss:  0.1374613344669342\n",
      "Batch loss:  0.23372401297092438\n",
      "Batch loss:  0.11174707114696503\n",
      "Batch loss:  0.11517175287008286\n",
      "Batch loss:  0.26289236545562744\n",
      "Current average loss: 0.21020938904129272\n",
      "Batch loss:  0.18999677896499634\n",
      "Batch loss:  0.23586274683475494\n",
      "Batch loss:  0.11008723825216293\n",
      "Batch loss:  0.2590758800506592\n",
      "Batch loss:  0.21628665924072266\n",
      "Batch loss:  0.10602082312107086\n",
      "Batch loss:  0.3202403485774994\n",
      "Batch loss:  0.2731572389602661\n",
      "Batch loss:  0.1962319165468216\n",
      "Batch loss:  0.17060914635658264\n",
      "Batch loss:  0.2131204456090927\n",
      "Batch loss:  0.16296841204166412\n",
      "Batch loss:  0.20899546146392822\n",
      "Batch loss:  0.3185475468635559\n",
      "Batch loss:  0.14870479702949524\n",
      "Batch loss:  0.11672105640172958\n",
      "Batch loss:  0.14737297594547272\n",
      "Batch loss:  0.2849372625350952\n",
      "Batch loss:  0.21751908957958221\n",
      "Batch loss:  0.14588060975074768\n",
      "Batch loss:  0.22687199711799622\n",
      "Batch loss:  0.3082199692726135\n",
      "Batch loss:  0.12827686965465546\n",
      "Batch loss:  0.21603716909885406\n",
      "Batch loss:  0.09421142935752869\n",
      "Batch loss:  0.24974234402179718\n",
      "Batch loss:  0.09487325698137283\n",
      "Batch loss:  0.16445544362068176\n",
      "Batch loss:  0.3641907274723053\n",
      "Batch loss:  0.2870149612426758\n",
      "Batch loss:  0.1616881638765335\n",
      "Batch loss:  0.38091573119163513\n",
      "Batch loss:  0.2527426779270172\n",
      "Batch loss:  0.20254650712013245\n",
      "Batch loss:  0.21573364734649658\n",
      "Batch loss:  0.15921282768249512\n",
      "Batch loss:  0.10276257991790771\n",
      "Batch loss:  0.12266472727060318\n",
      "Batch loss:  0.1572016179561615\n",
      "Batch loss:  0.3147452771663666\n",
      "Batch loss:  0.22767965495586395\n",
      "Batch loss:  0.23748549818992615\n",
      "Batch loss:  0.21973876655101776\n",
      "Batch loss:  0.1116604432463646\n",
      "Batch loss:  0.18374162912368774\n",
      "Batch loss:  0.12843017280101776\n",
      "Batch loss:  0.14174653589725494\n",
      "Batch loss:  0.16655685007572174\n",
      "Batch loss:  0.15754887461662292\n",
      "Batch loss:  0.13640813529491425\n",
      "Batch loss:  0.13064271211624146\n",
      "Batch loss:  0.1572307050228119\n",
      "Batch loss:  0.15706051886081696\n",
      "Batch loss:  0.23862165212631226\n",
      "Batch loss:  0.13756230473518372\n",
      "Batch loss:  0.14836837351322174\n",
      "Batch loss:  0.14021608233451843\n",
      "Batch loss:  0.23972460627555847\n",
      "Batch loss:  0.12963703274726868\n",
      "Batch loss:  0.17278984189033508\n",
      "Batch loss:  0.17764952778816223\n",
      "Batch loss:  0.15389439463615417\n",
      "Batch loss:  0.14366231858730316\n",
      "Batch loss:  0.1700148582458496\n",
      "Batch loss:  0.09627359360456467\n",
      "Batch loss:  0.14026346802711487\n",
      "Batch loss:  0.20942510664463043\n",
      "Batch loss:  0.36936959624290466\n",
      "Batch loss:  0.21132056415081024\n",
      "Batch loss:  0.1620762199163437\n",
      "Batch loss:  0.18999747931957245\n",
      "Batch loss:  0.23011226952075958\n",
      "Batch loss:  0.15232624113559723\n",
      "Batch loss:  0.15740197896957397\n",
      "Batch loss:  0.2571432590484619\n",
      "Batch loss:  0.10836119949817657\n",
      "Batch loss:  0.13875974714756012\n",
      "Batch loss:  0.1784081757068634\n",
      "Batch loss:  0.07089762389659882\n",
      "Batch loss:  0.16162049770355225\n",
      "Batch loss:  0.16995447874069214\n",
      "Batch loss:  0.15835247933864594\n",
      "Batch loss:  0.15049749612808228\n",
      "Batch loss:  0.1826552450656891\n",
      "Batch loss:  0.2070988565683365\n",
      "Batch loss:  0.18177849054336548\n",
      "Batch loss:  0.32458680868148804\n",
      "Batch loss:  0.2208247184753418\n",
      "Batch loss:  0.22120971977710724\n",
      "Batch loss:  0.2107439786195755\n",
      "Batch loss:  0.2115519642829895\n",
      "Batch loss:  0.37572547793388367\n",
      "Batch loss:  0.25133398175239563\n",
      "Batch loss:  0.11471923440694809\n",
      "Batch loss:  0.12880711257457733\n",
      "Batch loss:  0.2605111598968506\n",
      "Batch loss:  0.1602926254272461\n",
      "Batch loss:  0.2846857011318207\n",
      "Batch loss:  0.27361786365509033\n",
      "Batch loss:  0.21309594810009003\n",
      "Current average loss: 0.20995856260021564\n",
      "Batch loss:  0.26408761739730835\n",
      "Batch loss:  0.09896725416183472\n",
      "Batch loss:  0.20050381124019623\n",
      "Batch loss:  0.08891255408525467\n",
      "Batch loss:  0.24820555746555328\n",
      "Batch loss:  0.21357545256614685\n",
      "Batch loss:  0.1104239970445633\n",
      "Batch loss:  0.16138628125190735\n",
      "Batch loss:  0.16507276892662048\n",
      "Batch loss:  0.3530711531639099\n",
      "Batch loss:  0.1408698409795761\n",
      "Batch loss:  0.2540704905986786\n",
      "Batch loss:  0.15841135382652283\n",
      "Batch loss:  0.11069890111684799\n",
      "Batch loss:  0.22056417167186737\n",
      "Batch loss:  0.1362789124250412\n",
      "Batch loss:  0.23312819004058838\n",
      "Batch loss:  0.12232955545186996\n",
      "Batch loss:  0.15886996686458588\n",
      "Batch loss:  0.22044552862644196\n",
      "Batch loss:  0.1145998165011406\n",
      "Batch loss:  0.3479342758655548\n",
      "Batch loss:  0.13679200410842896\n",
      "Batch loss:  0.1844579577445984\n",
      "Batch loss:  0.7391237020492554\n",
      "Batch loss:  0.13817332684993744\n",
      "Batch loss:  0.19445373117923737\n",
      "Batch loss:  0.18492169678211212\n",
      "Batch loss:  0.17378437519073486\n",
      "Batch loss:  0.143303781747818\n",
      "Batch loss:  0.09107538312673569\n",
      "Batch loss:  0.12946610152721405\n",
      "Batch loss:  0.1946195363998413\n",
      "Batch loss:  0.24169887602329254\n",
      "Batch loss:  0.35780036449432373\n",
      "Batch loss:  0.24046948552131653\n",
      "Batch loss:  0.2032356560230255\n",
      "Batch loss:  0.1701989471912384\n",
      "Batch loss:  0.20113065838813782\n",
      "Batch loss:  0.17155638337135315\n",
      "Batch loss:  0.1621459722518921\n",
      "Batch loss:  0.18452510237693787\n",
      "Batch loss:  0.24641729891300201\n",
      "Batch loss:  0.302486389875412\n",
      "Batch loss:  0.42951685190200806\n",
      "Batch loss:  0.22388741374015808\n",
      "Batch loss:  0.14510592818260193\n",
      "Batch loss:  0.3405182361602783\n",
      "Batch loss:  0.22534698247909546\n",
      "Batch loss:  0.12985266745090485\n",
      "Batch loss:  0.24471375346183777\n",
      "Batch loss:  0.3274993598461151\n",
      "Batch loss:  0.08989733457565308\n",
      "Batch loss:  0.12333562225103378\n",
      "Batch loss:  0.20656369626522064\n",
      "Batch loss:  0.07802911102771759\n",
      "Batch loss:  0.29688751697540283\n",
      "Batch loss:  0.1424277275800705\n",
      "Batch loss:  0.3507189154624939\n",
      "Batch loss:  0.1548723429441452\n",
      "Batch loss:  0.1286642700433731\n",
      "Batch loss:  0.15356850624084473\n",
      "Batch loss:  0.2758984863758087\n",
      "Batch loss:  0.13613486289978027\n",
      "Batch loss:  0.16567206382751465\n",
      "Batch loss:  0.19028353691101074\n",
      "Batch loss:  0.08966987580060959\n",
      "Batch loss:  0.25660645961761475\n",
      "Batch loss:  0.1432197242975235\n",
      "Batch loss:  0.15475685894489288\n",
      "Batch loss:  0.10858828574419022\n",
      "Batch loss:  0.08047059178352356\n",
      "Batch loss:  0.07699218392372131\n",
      "Batch loss:  0.09467912465333939\n",
      "Batch loss:  0.12703873217105865\n",
      "Batch loss:  0.1420096606016159\n",
      "Batch loss:  0.16642186045646667\n",
      "Batch loss:  0.11687441170215607\n",
      "Batch loss:  0.2135215401649475\n",
      "Batch loss:  0.3395032286643982\n",
      "Batch loss:  0.26081591844558716\n",
      "Batch loss:  0.156774640083313\n",
      "Batch loss:  0.12549035251140594\n",
      "Batch loss:  0.2482660710811615\n",
      "Batch loss:  0.2188263237476349\n",
      "Batch loss:  0.2375756949186325\n",
      "Batch loss:  0.23187389969825745\n",
      "Batch loss:  0.36072972416877747\n",
      "Batch loss:  0.27845874428749084\n",
      "Batch loss:  0.11380891501903534\n",
      "Batch loss:  0.1956658810377121\n",
      "Batch loss:  0.3526081144809723\n",
      "Batch loss:  0.25948113203048706\n",
      "Batch loss:  0.3070005178451538\n",
      "Batch loss:  0.1857212334871292\n",
      "Batch loss:  0.26053386926651\n",
      "Batch loss:  0.19827720522880554\n",
      "Batch loss:  0.12431995570659637\n",
      "Batch loss:  0.19412510097026825\n",
      "Batch loss:  0.1682707667350769\n",
      "Current average loss: 0.20981718397701696\n",
      "Batch loss:  0.19539867341518402\n",
      "Batch loss:  0.2763283848762512\n",
      "Batch loss:  0.16740351915359497\n",
      "Batch loss:  0.14574937522411346\n",
      "Batch loss:  0.1459629386663437\n",
      "Batch loss:  0.10413559526205063\n",
      "Batch loss:  0.18146982789039612\n",
      "Batch loss:  0.192805215716362\n",
      "Batch loss:  0.2513057589530945\n",
      "Batch loss:  0.18567565083503723\n",
      "Batch loss:  0.11381658166646957\n",
      "Batch loss:  0.12412768602371216\n",
      "Batch loss:  0.29209041595458984\n",
      "Batch loss:  0.22011543810367584\n",
      "Batch loss:  0.28646206855773926\n",
      "Batch loss:  0.251591295003891\n",
      "Batch loss:  0.14714834094047546\n",
      "Batch loss:  0.19864514470100403\n",
      "Batch loss:  0.1293616145849228\n",
      "Batch loss:  0.27685216069221497\n",
      "Batch loss:  0.2078751176595688\n",
      "Batch loss:  0.3341177999973297\n",
      "Batch loss:  0.13282105326652527\n",
      "Batch loss:  0.1556878685951233\n",
      "Batch loss:  0.23079979419708252\n",
      "Batch loss:  0.2138536274433136\n",
      "Batch loss:  0.16634729504585266\n",
      "Batch loss:  0.11278092116117477\n",
      "Batch loss:  0.10371744632720947\n",
      "Batch loss:  0.1375133991241455\n",
      "Batch loss:  0.07851813733577728\n",
      "Batch loss:  0.1904173642396927\n",
      "Batch loss:  0.24442879855632782\n",
      "Batch loss:  0.240998774766922\n",
      "Batch loss:  0.16587184369564056\n",
      "Batch loss:  0.19440092146396637\n",
      "Batch loss:  0.23101487755775452\n",
      "Batch loss:  0.1657000184059143\n",
      "Batch loss:  0.38745883107185364\n",
      "Batch loss:  0.5731488466262817\n",
      "Batch loss:  0.23363932967185974\n",
      "Batch loss:  0.08102099597454071\n",
      "Batch loss:  0.23901008069515228\n",
      "Batch loss:  0.31009241938591003\n",
      "Batch loss:  0.2279580533504486\n",
      "Batch loss:  0.12713290750980377\n",
      "Batch loss:  0.3175339698791504\n",
      "Batch loss:  0.23399193584918976\n",
      "Batch loss:  0.11198076605796814\n",
      "Batch loss:  0.2680618166923523\n",
      "Batch loss:  0.1790667027235031\n",
      "Batch loss:  0.21168260276317596\n",
      "Batch loss:  0.10806769132614136\n",
      "Batch loss:  0.17778216302394867\n",
      "Batch loss:  0.12280027568340302\n",
      "Batch loss:  0.15187394618988037\n",
      "Batch loss:  0.20181581377983093\n",
      "Batch loss:  0.12387412041425705\n",
      "Batch loss:  0.2483399659395218\n",
      "Batch loss:  0.36684125661849976\n",
      "Batch loss:  0.3486318290233612\n",
      "Batch loss:  0.1418401151895523\n",
      "Batch loss:  0.13215789198875427\n",
      "Batch loss:  0.12985894083976746\n",
      "Batch loss:  0.13698627054691315\n",
      "Batch loss:  0.24558253586292267\n",
      "Batch loss:  0.2547435164451599\n",
      "Batch loss:  0.10265469551086426\n",
      "Batch loss:  0.2674782872200012\n",
      "Batch loss:  0.22543944418430328\n",
      "Batch loss:  0.16442762315273285\n",
      "Batch loss:  0.1569940745830536\n",
      "Batch loss:  0.30330103635787964\n",
      "Batch loss:  0.30018216371536255\n",
      "Batch loss:  0.21213474869728088\n",
      "Batch loss:  0.11950218677520752\n",
      "Batch loss:  0.13025826215744019\n",
      "Batch loss:  0.3474201261997223\n",
      "Batch loss:  0.14146104454994202\n",
      "Batch loss:  0.17325277626514435\n",
      "Batch loss:  0.2302192896604538\n",
      "Batch loss:  0.09150780737400055\n",
      "Batch loss:  0.19234739243984222\n",
      "Batch loss:  0.17472463846206665\n",
      "Batch loss:  0.130939319729805\n",
      "Batch loss:  0.3495194911956787\n",
      "Batch loss:  0.2511885166168213\n",
      "Batch loss:  0.2547297179698944\n",
      "Batch loss:  0.1531669795513153\n",
      "Batch loss:  0.08645153790712357\n",
      "Batch loss:  0.19312645494937897\n",
      "Batch loss:  0.1279754936695099\n",
      "Batch loss:  0.35272151231765747\n",
      "Batch loss:  0.24615922570228577\n",
      "Batch loss:  0.25129806995391846\n",
      "Batch loss:  0.13124072551727295\n",
      "Batch loss:  0.14871466159820557\n",
      "Batch loss:  0.22020982205867767\n",
      "Batch loss:  0.18100419640541077\n",
      "Batch loss:  0.12995027005672455\n",
      "Current average loss: 0.2096897805343945\n",
      "Batch loss:  0.21084626019001007\n",
      "Batch loss:  0.11387170851230621\n",
      "Batch loss:  0.1311301290988922\n",
      "Batch loss:  0.19365999102592468\n",
      "Batch loss:  0.18256066739559174\n",
      "Batch loss:  0.11368947476148605\n",
      "Batch loss:  0.13998277485370636\n",
      "Batch loss:  0.16646243631839752\n",
      "Batch loss:  0.4068291485309601\n",
      "Batch loss:  0.12822213768959045\n",
      "Batch loss:  0.1906452625989914\n",
      "Batch loss:  0.21481971442699432\n",
      "Batch loss:  0.3390722870826721\n",
      "Batch loss:  0.1378737837076187\n",
      "Batch loss:  0.18659693002700806\n",
      "Batch loss:  0.11623797565698624\n",
      "Batch loss:  0.25563302636146545\n",
      "Batch loss:  0.1524115651845932\n",
      "Batch loss:  0.09385814517736435\n",
      "Batch loss:  0.21868428587913513\n",
      "Batch loss:  0.24803483486175537\n",
      "Batch loss:  0.07696598768234253\n",
      "Batch loss:  0.17203915119171143\n",
      "Batch loss:  0.18506097793579102\n",
      "Batch loss:  0.164436474442482\n",
      "Batch loss:  0.2451176792383194\n",
      "Batch loss:  0.18400824069976807\n",
      "Batch loss:  0.0759645402431488\n",
      "Batch loss:  0.2206985205411911\n",
      "Batch loss:  0.08887109905481339\n",
      "Batch loss:  0.22475968301296234\n",
      "Batch loss:  0.11081429570913315\n",
      "Batch loss:  0.08529861271381378\n",
      "Batch loss:  0.16485339403152466\n",
      "Batch loss:  0.1947370320558548\n",
      "Batch loss:  0.2259766012430191\n",
      "Batch loss:  0.11267005652189255\n",
      "Batch loss:  0.11242406815290451\n",
      "Batch loss:  0.1622319221496582\n",
      "Batch loss:  0.19177956879138947\n",
      "Batch loss:  0.22702035307884216\n",
      "Batch loss:  0.17320604622364044\n",
      "Batch loss:  0.35094597935676575\n",
      "Batch loss:  0.2842082381248474\n",
      "Batch loss:  0.26753512024879456\n",
      "Batch loss:  0.2254703789949417\n",
      "Batch loss:  0.13178691267967224\n",
      "Batch loss:  0.19157834351062775\n",
      "Batch loss:  0.19654399156570435\n",
      "Batch loss:  0.30476731061935425\n",
      "Batch loss:  0.14942286908626556\n",
      "Batch loss:  0.33158859610557556\n",
      "Batch loss:  0.26826271414756775\n",
      "Batch loss:  0.1060442328453064\n",
      "Batch loss:  0.21888156235218048\n",
      "Batch loss:  0.22188380360603333\n",
      "Batch loss:  0.13257096707820892\n",
      "Batch loss:  0.2581751048564911\n",
      "Batch loss:  0.16871054470539093\n",
      "Batch loss:  0.1751992404460907\n",
      "Batch loss:  0.22766917943954468\n",
      "Batch loss:  0.25804492831230164\n",
      "Batch loss:  0.2751551866531372\n",
      "Batch loss:  0.3516288101673126\n",
      "Batch loss:  0.061966974288225174\n",
      "Batch loss:  0.09391352534294128\n",
      "Batch loss:  0.16478876769542694\n",
      "Batch loss:  0.24885162711143494\n",
      "Batch loss:  0.12611143290996552\n",
      "Batch loss:  0.12257412821054459\n",
      "Batch loss:  0.09011085331439972\n",
      "Batch loss:  0.15845905244350433\n",
      "Batch loss:  0.1510172337293625\n",
      "Batch loss:  0.27985548973083496\n",
      "Batch loss:  0.1812244951725006\n",
      "Batch loss:  0.1948217749595642\n",
      "Batch loss:  0.1855984330177307\n",
      "Batch loss:  0.13014006614685059\n",
      "Batch loss:  0.1296655237674713\n",
      "Batch loss:  0.13946783542633057\n",
      "Batch loss:  0.25121864676475525\n",
      "Batch loss:  0.269688218832016\n",
      "Batch loss:  0.13830532133579254\n",
      "Batch loss:  0.13907837867736816\n",
      "Batch loss:  0.1574525088071823\n",
      "Batch loss:  0.20076079666614532\n",
      "Batch loss:  0.11392772197723389\n",
      "Batch loss:  0.21722401678562164\n",
      "Batch loss:  0.13839425146579742\n",
      "Batch loss:  0.16055943071842194\n",
      "Batch loss:  0.1481015682220459\n",
      "Batch loss:  0.11919617652893066\n",
      "Batch loss:  0.1899750530719757\n",
      "Batch loss:  0.14447718858718872\n",
      "Batch loss:  0.2725822329521179\n",
      "Batch loss:  0.16677893698215485\n",
      "Batch loss:  0.2702811658382416\n",
      "Batch loss:  0.15986941754817963\n",
      "Batch loss:  0.29106104373931885\n",
      "Batch loss:  0.08251368999481201\n",
      "Current average loss: 0.2093305930302452\n",
      "Batch loss:  0.16596364974975586\n",
      "Batch loss:  0.15206949412822723\n",
      "Batch loss:  0.1100085973739624\n",
      "Batch loss:  0.13703477382659912\n",
      "Batch loss:  0.19740496575832367\n",
      "Batch loss:  0.37722048163414\n",
      "Batch loss:  0.2239162027835846\n",
      "Batch loss:  0.1612183004617691\n",
      "Batch loss:  0.1722954511642456\n",
      "Batch loss:  0.11847997456789017\n",
      "Batch loss:  0.13767600059509277\n",
      "Batch loss:  0.2241787165403366\n",
      "Batch loss:  0.283908873796463\n",
      "Batch loss:  0.27415069937705994\n",
      "Batch loss:  0.22856387495994568\n",
      "Batch loss:  0.25885921716690063\n",
      "Batch loss:  0.14524856209754944\n",
      "Batch loss:  0.16596552729606628\n",
      "Batch loss:  0.14315165579319\n",
      "Batch loss:  0.2284860461950302\n",
      "Batch loss:  0.1545570194721222\n",
      "Batch loss:  0.21629810333251953\n",
      "Batch loss:  0.14861896634101868\n",
      "Batch loss:  0.24422499537467957\n",
      "Batch loss:  0.1762549877166748\n",
      "Batch loss:  0.13873940706253052\n",
      "Batch loss:  0.1530965119600296\n",
      "Batch loss:  0.30982664227485657\n",
      "Batch loss:  0.2611658573150635\n",
      "Batch loss:  0.09752210974693298\n",
      "Batch loss:  0.13214807212352753\n",
      "Batch loss:  0.18883948028087616\n",
      "Batch loss:  0.13046903908252716\n",
      "Batch loss:  0.43294069170951843\n",
      "Batch loss:  0.12343587726354599\n",
      "Batch loss:  0.1671152561903\n",
      "Batch loss:  0.1698664128780365\n",
      "Batch loss:  0.3217840790748596\n",
      "Batch loss:  0.32737475633621216\n",
      "Batch loss:  0.21609562635421753\n",
      "Batch loss:  0.27768465876579285\n",
      "Batch loss:  0.22418203949928284\n",
      "Batch loss:  0.30801525712013245\n",
      "Batch loss:  0.46669062972068787\n",
      "Batch loss:  0.21041424572467804\n",
      "Batch loss:  0.1964220255613327\n",
      "Batch loss:  0.22018276154994965\n",
      "Batch loss:  0.21423102915287018\n",
      "Batch loss:  0.15442627668380737\n",
      "Batch loss:  0.2956826686859131\n",
      "Batch loss:  0.14096631109714508\n",
      "Batch loss:  0.17192545533180237\n",
      "Batch loss:  0.06568174064159393\n",
      "Batch loss:  0.2307419329881668\n",
      "Batch loss:  0.3281216025352478\n",
      "Batch loss:  0.18067499995231628\n",
      "Batch loss:  0.3049030005931854\n",
      "Batch loss:  0.22178113460540771\n",
      "Batch loss:  0.18837036192417145\n",
      "Batch loss:  0.12614041566848755\n",
      "Batch loss:  0.18793053925037384\n",
      "Batch loss:  0.2501407563686371\n",
      "Batch loss:  0.11180142313241959\n",
      "Batch loss:  0.13162226974964142\n",
      "Batch loss:  0.18945522606372833\n",
      "Batch loss:  0.17090138792991638\n",
      "Batch loss:  0.25148171186447144\n",
      "Batch loss:  0.1509476900100708\n",
      "Batch loss:  0.16962464153766632\n",
      "Batch loss:  0.25903990864753723\n",
      "Batch loss:  0.3358409106731415\n",
      "Batch loss:  0.19841064512729645\n",
      "Batch loss:  0.09456772357225418\n",
      "Batch loss:  0.32903072237968445\n",
      "Batch loss:  0.0684124156832695\n",
      "Batch loss:  0.15179163217544556\n",
      "Batch loss:  0.16083894670009613\n",
      "Batch loss:  0.09881892800331116\n",
      "Batch loss:  0.309415727853775\n",
      "Batch loss:  0.16268865764141083\n",
      "Batch loss:  0.16267597675323486\n",
      "Batch loss:  0.29932621121406555\n",
      "Batch loss:  0.3347386419773102\n",
      "Batch loss:  0.10083304345607758\n",
      "Batch loss:  0.21959011256694794\n",
      "Batch loss:  0.09178663790225983\n",
      "Batch loss:  0.14120639860630035\n",
      "Batch loss:  0.20011857151985168\n",
      "Batch loss:  0.24716414511203766\n",
      "Batch loss:  0.26281267404556274\n",
      "Batch loss:  0.16994231939315796\n",
      "Batch loss:  0.25263965129852295\n",
      "Batch loss:  0.1856592893600464\n",
      "Batch loss:  0.335254043340683\n",
      "Batch loss:  0.12836407124996185\n",
      "Batch loss:  0.1441054344177246\n",
      "Batch loss:  0.2637172341346741\n",
      "Batch loss:  0.24306544661521912\n",
      "Batch loss:  0.3398357927799225\n",
      "Batch loss:  0.21653375029563904\n",
      "Current average loss: 0.20929182843269947\n",
      "Batch loss:  0.13444864749908447\n",
      "Batch loss:  0.25167226791381836\n",
      "Batch loss:  0.19189967215061188\n",
      "Batch loss:  0.22515258193016052\n",
      "Batch loss:  0.1913563460111618\n",
      "Batch loss:  0.278858482837677\n",
      "Batch loss:  0.30040067434310913\n",
      "Batch loss:  0.2932533323764801\n",
      "Batch loss:  0.12733308970928192\n",
      "Batch loss:  0.168974369764328\n",
      "Batch loss:  0.27015766501426697\n",
      "Batch loss:  0.27259472012519836\n",
      "Batch loss:  0.10431794822216034\n",
      "Batch loss:  0.16092003881931305\n",
      "Batch loss:  0.10877753049135208\n",
      "Batch loss:  0.125249445438385\n",
      "Batch loss:  0.1562010496854782\n",
      "Batch loss:  0.2927890419960022\n",
      "Batch loss:  0.22318628430366516\n",
      "Batch loss:  0.12142350524663925\n",
      "Batch loss:  0.08736187219619751\n",
      "Batch loss:  0.1557350605726242\n",
      "Batch loss:  0.36014899611473083\n",
      "Batch loss:  0.19730667769908905\n",
      "Batch loss:  0.11312777549028397\n",
      "Batch loss:  0.2092551589012146\n",
      "Batch loss:  0.26208385825157166\n",
      "Batch loss:  0.10174620151519775\n",
      "Batch loss:  0.3935290575027466\n",
      "Batch loss:  0.20720325410366058\n",
      "Batch loss:  0.22062000632286072\n",
      "Batch loss:  0.1923111230134964\n",
      "Batch loss:  0.1624131202697754\n",
      "Batch loss:  0.23411397635936737\n",
      "Batch loss:  0.2143840193748474\n",
      "Batch loss:  0.139825701713562\n",
      "Batch loss:  0.2121887505054474\n",
      "Batch loss:  0.22832761704921722\n",
      "Batch loss:  0.2973153591156006\n",
      "Batch loss:  0.19044937193393707\n",
      "Batch loss:  0.06996587663888931\n",
      "Batch loss:  0.09299907833337784\n",
      "Batch loss:  0.22650963068008423\n",
      "Batch loss:  0.3030940592288971\n",
      "Batch loss:  0.12978719174861908\n",
      "Batch loss:  0.32995665073394775\n",
      "Batch loss:  0.37624111771583557\n",
      "Batch loss:  0.2293824404478073\n",
      "Batch loss:  0.16423822939395905\n",
      "Batch loss:  0.10700523108243942\n",
      "Batch loss:  0.19234785437583923\n",
      "Batch loss:  0.10151752084493637\n",
      "Batch loss:  0.1179061084985733\n",
      "Batch loss:  0.19523388147354126\n",
      "Batch loss:  0.31820088624954224\n",
      "Batch loss:  0.28215155005455017\n",
      "Batch loss:  0.12996450066566467\n",
      "Batch loss:  0.17790666222572327\n",
      "Batch loss:  0.15574826300144196\n",
      "Batch loss:  0.1992770880460739\n",
      "Batch loss:  0.4278544485569\n",
      "Batch loss:  0.18844197690486908\n",
      "Batch loss:  0.21309104561805725\n",
      "Batch loss:  0.3662106692790985\n",
      "Batch loss:  0.23690049350261688\n",
      "Batch loss:  0.28364449739456177\n",
      "Batch loss:  0.15652410686016083\n",
      "Batch loss:  0.11899518966674805\n",
      "Batch loss:  0.202863410115242\n",
      "Batch loss:  0.18535371124744415\n",
      "Batch loss:  0.2383611798286438\n",
      "Batch loss:  0.2577311396598816\n",
      "Batch loss:  0.12100909650325775\n",
      "Batch loss:  0.10501320660114288\n",
      "Batch loss:  0.23685163259506226\n",
      "Batch loss:  0.11566749960184097\n",
      "Batch loss:  0.10144715756177902\n",
      "Batch loss:  0.09665082395076752\n",
      "Batch loss:  0.13388095796108246\n",
      "Batch loss:  0.21007421612739563\n",
      "Batch loss:  0.16822141408920288\n",
      "Batch loss:  0.20796479284763336\n",
      "Batch loss:  0.26575741171836853\n",
      "Batch loss:  0.27094605565071106\n",
      "Batch loss:  0.08262358605861664\n",
      "Batch loss:  0.2650410830974579\n",
      "Batch loss:  0.3107835650444031\n",
      "Batch loss:  0.08927391469478607\n",
      "Batch loss:  0.1858295202255249\n",
      "Batch loss:  0.22274905443191528\n",
      "Batch loss:  0.3023834228515625\n",
      "Batch loss:  0.2841229736804962\n",
      "Batch loss:  0.18457365036010742\n",
      "Batch loss:  0.20705103874206543\n",
      "Batch loss:  0.09913856536149979\n",
      "Batch loss:  0.0875370129942894\n",
      "Batch loss:  0.18837785720825195\n",
      "Batch loss:  0.12316326797008514\n",
      "Batch loss:  0.29010364413261414\n",
      "Batch loss:  0.15866486728191376\n",
      "Current average loss: 0.20916892281750613\n",
      "Batch loss:  0.2478291094303131\n",
      "Batch loss:  0.2338743954896927\n",
      "Batch loss:  0.15300734341144562\n",
      "Batch loss:  0.20440712571144104\n",
      "Batch loss:  0.19417865574359894\n",
      "Batch loss:  0.20883820950984955\n",
      "Batch loss:  0.2851375937461853\n",
      "Batch loss:  0.17808224260807037\n",
      "Batch loss:  0.4777480363845825\n",
      "Batch loss:  0.2075071632862091\n",
      "Batch loss:  0.2521021068096161\n",
      "Batch loss:  0.23794785141944885\n",
      "Batch loss:  0.34479936957359314\n",
      "Batch loss:  0.43036869168281555\n",
      "Batch loss:  0.15833422541618347\n",
      "Batch loss:  0.09804518520832062\n",
      "Batch loss:  0.10203617066144943\n",
      "Batch loss:  0.4200904369354248\n",
      "Batch loss:  0.23642924427986145\n",
      "Batch loss:  0.1669861078262329\n",
      "Batch loss:  0.2255753129720688\n",
      "Batch loss:  0.14963778853416443\n",
      "Batch loss:  0.14264211058616638\n",
      "Batch loss:  0.15851271152496338\n",
      "Batch loss:  0.16234520077705383\n",
      "Batch loss:  0.15161554515361786\n",
      "Batch loss:  0.05317803844809532\n",
      "Batch loss:  0.29547810554504395\n",
      "Batch loss:  0.19715061783790588\n",
      "Batch loss:  0.18448258936405182\n",
      "Batch loss:  0.17527085542678833\n",
      "Batch loss:  0.20052208006381989\n",
      "Batch loss:  0.09273312985897064\n",
      "Batch loss:  0.1668761819601059\n",
      "Batch loss:  0.25615444779396057\n",
      "Batch loss:  0.09351144731044769\n",
      "Batch loss:  0.16139857470989227\n",
      "Batch loss:  0.1852913498878479\n",
      "Batch loss:  0.1827833503484726\n",
      "Batch loss:  0.27246153354644775\n",
      "Batch loss:  0.12953393161296844\n",
      "Batch loss:  0.1005435585975647\n",
      "Batch loss:  0.3975258767604828\n",
      "Batch loss:  0.14663131535053253\n",
      "Batch loss:  0.14206288754940033\n",
      "Batch loss:  0.15528622269630432\n",
      "Batch loss:  0.17955826222896576\n",
      "Batch loss:  0.12645548582077026\n",
      "Batch loss:  0.1907764971256256\n",
      "Batch loss:  0.11839834600687027\n",
      "Batch loss:  0.14713558554649353\n",
      "Batch loss:  0.19342570006847382\n",
      "Batch loss:  0.12024839222431183\n",
      "Batch loss:  0.22937528789043427\n",
      "Batch loss:  0.18148285150527954\n",
      "Batch loss:  0.20023468136787415\n",
      "Batch loss:  0.40628743171691895\n",
      "Batch loss:  0.0898110568523407\n",
      "Batch loss:  0.21708078682422638\n",
      "Batch loss:  0.26873016357421875\n",
      "Batch loss:  0.13335424661636353\n",
      "Batch loss:  0.20225317776203156\n",
      "Batch loss:  0.5005003809928894\n",
      "Batch loss:  0.2881624698638916\n",
      "Batch loss:  0.21630661189556122\n",
      "Batch loss:  0.10086827725172043\n",
      "Batch loss:  0.1654970794916153\n",
      "Batch loss:  0.14403283596038818\n",
      "Batch loss:  0.15718978643417358\n",
      "Batch loss:  0.18655072152614594\n",
      "Batch loss:  0.206099271774292\n",
      "Batch loss:  0.3037206828594208\n",
      "Batch loss:  0.2601032853126526\n",
      "Batch loss:  0.28462696075439453\n",
      "Batch loss:  0.12937137484550476\n",
      "Batch loss:  0.14163926243782043\n",
      "Batch loss:  0.11494974792003632\n",
      "Batch loss:  0.2907637655735016\n",
      "Batch loss:  0.2065351903438568\n",
      "Batch loss:  0.14525046944618225\n",
      "Batch loss:  0.3044034242630005\n",
      "Batch loss:  0.18910662829875946\n",
      "Batch loss:  0.10655982792377472\n",
      "Batch loss:  0.2678201496601105\n",
      "Batch loss:  0.12401271611452103\n",
      "Batch loss:  0.35663825273513794\n",
      "Batch loss:  0.08757084608078003\n",
      "Batch loss:  0.16052816808223724\n",
      "Batch loss:  0.4589748680591583\n",
      "Batch loss:  0.09220780432224274\n",
      "Batch loss:  0.2209453582763672\n",
      "Batch loss:  0.1897779405117035\n",
      "Batch loss:  0.3298468589782715\n",
      "Batch loss:  0.14752081036567688\n",
      "Batch loss:  0.16684404015541077\n",
      "Batch loss:  0.16875557601451874\n",
      "Batch loss:  0.2732333838939667\n",
      "Batch loss:  0.36860623955726624\n",
      "Batch loss:  0.31786131858825684\n",
      "Batch loss:  0.29058724641799927\n",
      "Current average loss: 0.20916450644427917\n",
      "Batch loss:  0.15719331800937653\n",
      "Batch loss:  0.19663475453853607\n",
      "Batch loss:  0.33566591143608093\n",
      "Batch loss:  0.21519970893859863\n",
      "Batch loss:  0.3557176887989044\n",
      "Batch loss:  0.39636480808258057\n",
      "Batch loss:  0.20644423365592957\n",
      "Batch loss:  0.2796940803527832\n",
      "Batch loss:  0.13585175573825836\n",
      "Batch loss:  0.16224834322929382\n",
      "Batch loss:  0.3381834626197815\n",
      "Batch loss:  0.2703618109226227\n",
      "Batch loss:  0.12761850655078888\n",
      "Batch loss:  0.19041751325130463\n",
      "Batch loss:  0.2290792018175125\n",
      "Batch loss:  0.10827171802520752\n",
      "Batch loss:  0.27979934215545654\n",
      "Batch loss:  0.19738294184207916\n",
      "Batch loss:  0.2026098072528839\n",
      "Batch loss:  0.1945415586233139\n",
      "Batch loss:  0.11116106063127518\n",
      "Batch loss:  0.13772360980510712\n",
      "Batch loss:  0.12725338339805603\n",
      "Batch loss:  0.12194359302520752\n",
      "Batch loss:  0.09877822548151016\n",
      "Batch loss:  0.10993170738220215\n",
      "Batch loss:  0.22103939950466156\n",
      "Batch loss:  0.09648283571004868\n",
      "Batch loss:  0.1667189598083496\n",
      "Batch loss:  0.20485398173332214\n",
      "Batch loss:  0.2910896837711334\n",
      "Batch loss:  0.23891545832157135\n",
      "Batch loss:  0.09417826682329178\n",
      "Batch loss:  0.13919596374034882\n",
      "Batch loss:  0.2685684561729431\n",
      "Batch loss:  0.14902089536190033\n",
      "Batch loss:  0.18983718752861023\n",
      "Batch loss:  0.09824174642562866\n",
      "Batch loss:  0.1854524463415146\n",
      "Batch loss:  0.3010980784893036\n",
      "Batch loss:  0.14603151381015778\n",
      "Batch loss:  0.18900741636753082\n",
      "Batch loss:  0.10951381176710129\n",
      "Batch loss:  0.20220671594142914\n",
      "Batch loss:  0.14579664170742035\n",
      "Batch loss:  0.1736014485359192\n",
      "Batch loss:  0.16619566082954407\n",
      "Batch loss:  0.2191699743270874\n",
      "Batch loss:  0.3749438226222992\n",
      "Batch loss:  0.17959751188755035\n",
      "Batch loss:  0.08496924489736557\n",
      "Batch loss:  0.07537129521369934\n",
      "Batch loss:  0.15601415932178497\n",
      "Batch loss:  0.18721763789653778\n",
      "Batch loss:  0.14910192787647247\n",
      "Batch loss:  0.2107972949743271\n",
      "Batch loss:  0.1272740364074707\n",
      "Batch loss:  0.2372555285692215\n",
      "Batch loss:  0.29599660634994507\n",
      "Batch loss:  0.34370753169059753\n",
      "Batch loss:  0.1557581126689911\n",
      "Batch loss:  0.2585863769054413\n",
      "Batch loss:  0.11772344261407852\n",
      "Batch loss:  0.27953144907951355\n",
      "Batch loss:  0.328960120677948\n",
      "Batch loss:  0.16409622132778168\n",
      "Batch loss:  0.35476967692375183\n",
      "Batch loss:  0.15091943740844727\n",
      "Batch loss:  0.09239571541547775\n",
      "Batch loss:  0.20853766798973083\n",
      "Batch loss:  0.17925558984279633\n",
      "Batch loss:  0.23452195525169373\n",
      "Batch loss:  0.21659405529499054\n",
      "Batch loss:  0.17754022777080536\n",
      "Batch loss:  0.13314929604530334\n",
      "Batch loss:  0.1171262115240097\n",
      "Batch loss:  0.2668243944644928\n",
      "Batch loss:  0.24833782017230988\n",
      "Batch loss:  0.2523239552974701\n",
      "Batch loss:  0.10793104767799377\n",
      "Batch loss:  0.1784650683403015\n",
      "Batch loss:  0.19785845279693604\n",
      "Batch loss:  0.11600011587142944\n",
      "Batch loss:  0.07646320760250092\n",
      "Batch loss:  0.26184216141700745\n",
      "Batch loss:  0.13320481777191162\n",
      "Batch loss:  0.1936275213956833\n",
      "Batch loss:  0.21639414131641388\n",
      "Batch loss:  0.12732475996017456\n",
      "Batch loss:  0.18011392652988434\n",
      "Batch loss:  0.19906389713287354\n",
      "Batch loss:  0.2640587389469147\n",
      "Batch loss:  0.1517164260149002\n",
      "Batch loss:  0.5910079479217529\n",
      "Batch loss:  0.030908148735761642\n",
      "Batch loss:  0.1979798674583435\n",
      "Batch loss:  0.15660884976387024\n",
      "Batch loss:  0.3073742985725403\n",
      "Batch loss:  0.21983486413955688\n",
      "Batch loss:  0.08435229957103729\n",
      "Current average loss: 0.20898635991289125\n",
      "Batch loss:  0.24206168949604034\n",
      "Batch loss:  0.2002382129430771\n",
      "Batch loss:  0.3439635634422302\n",
      "Batch loss:  0.3460035026073456\n",
      "Batch loss:  0.11748446524143219\n",
      "Batch loss:  0.08298878371715546\n",
      "Batch loss:  0.14133206009864807\n",
      "Batch loss:  0.12971681356430054\n",
      "Batch loss:  0.34190040826797485\n",
      "Batch loss:  0.1379590928554535\n",
      "Batch loss:  0.1364045888185501\n",
      "Batch loss:  0.2099199742078781\n",
      "Batch loss:  0.15087935328483582\n",
      "Batch loss:  0.13728450238704681\n",
      "Batch loss:  0.20839713513851166\n",
      "Batch loss:  0.28935331106185913\n",
      "Batch loss:  0.10399754345417023\n",
      "Batch loss:  0.10164721310138702\n",
      "Batch loss:  0.23297467827796936\n",
      "Batch loss:  0.2531580626964569\n",
      "Batch loss:  0.08420390635728836\n",
      "Batch loss:  0.24298658967018127\n",
      "Batch loss:  0.22995123267173767\n",
      "Batch loss:  0.21284647285938263\n",
      "Batch loss:  0.3426018953323364\n",
      "Batch loss:  0.13323500752449036\n",
      "Batch loss:  0.17301733791828156\n",
      "Batch loss:  0.23698312044143677\n",
      "Batch loss:  0.22068330645561218\n",
      "Batch loss:  0.2609241008758545\n",
      "Batch loss:  0.12073107808828354\n",
      "Batch loss:  0.2672894299030304\n",
      "Batch loss:  0.1577734649181366\n",
      "Batch loss:  0.18920202553272247\n",
      "Batch loss:  0.1806483268737793\n",
      "Batch loss:  0.15323437750339508\n",
      "Batch loss:  0.20579461753368378\n",
      "Batch loss:  0.14836598932743073\n",
      "Batch loss:  0.20563319325447083\n",
      "Batch loss:  0.17649972438812256\n",
      "Batch loss:  0.13138821721076965\n",
      "Batch loss:  0.23351693153381348\n",
      "Batch loss:  0.10322405397891998\n",
      "Batch loss:  0.17686644196510315\n",
      "Batch loss:  0.22105003893375397\n",
      "Batch loss:  0.18327008187770844\n",
      "Batch loss:  0.13644550740718842\n",
      "Batch loss:  0.22763724625110626\n",
      "Batch loss:  0.15644001960754395\n",
      "Batch loss:  0.1155359223484993\n",
      "Batch loss:  0.23133306205272675\n",
      "Batch loss:  0.09490366280078888\n",
      "Batch loss:  0.2652546763420105\n",
      "Batch loss:  0.1718938797712326\n",
      "Batch loss:  0.3087210953235626\n",
      "Batch loss:  0.16126824915409088\n",
      "Batch loss:  0.20046819746494293\n",
      "Batch loss:  0.127212792634964\n",
      "Batch loss:  0.13430604338645935\n",
      "Batch loss:  0.18144050240516663\n",
      "Batch loss:  0.19201408326625824\n",
      "Batch loss:  0.15000972151756287\n",
      "Batch loss:  0.10060077160596848\n",
      "Batch loss:  0.16007566452026367\n",
      "Batch loss:  0.0469832718372345\n",
      "Batch loss:  0.12529776990413666\n",
      "Batch loss:  0.12100637704133987\n",
      "Batch loss:  0.3255966305732727\n",
      "Batch loss:  0.2567965090274811\n",
      "Batch loss:  0.1727929562330246\n",
      "Batch loss:  0.269704669713974\n",
      "Batch loss:  0.06518110632896423\n",
      "Batch loss:  0.19757743179798126\n",
      "Batch loss:  0.27856025099754333\n",
      "Batch loss:  0.1906752586364746\n",
      "Batch loss:  0.15976488590240479\n",
      "Batch loss:  0.25803452730178833\n",
      "Batch loss:  0.37089887261390686\n",
      "Batch loss:  0.12009734660387039\n",
      "Batch loss:  0.1587926298379898\n",
      "Batch loss:  0.2918705940246582\n",
      "Batch loss:  0.20475655794143677\n",
      "Batch loss:  0.1876910924911499\n",
      "Batch loss:  0.08561591804027557\n",
      "Batch loss:  0.13921421766281128\n",
      "Batch loss:  0.10874195396900177\n",
      "Batch loss:  0.13753779232501984\n",
      "Batch loss:  0.2314201295375824\n",
      "Batch loss:  0.13039231300354004\n",
      "Batch loss:  0.2076161503791809\n",
      "Batch loss:  0.1458507478237152\n",
      "Batch loss:  0.11468210071325302\n",
      "Batch loss:  0.21466682851314545\n",
      "Batch loss:  0.25869882106781006\n",
      "Batch loss:  0.2823626697063446\n",
      "Batch loss:  0.1083974689245224\n",
      "Batch loss:  0.30771321058273315\n",
      "Batch loss:  0.14006608724594116\n",
      "Batch loss:  0.2386627495288849\n",
      "Batch loss:  0.21485847234725952\n",
      "Current average loss: 0.20871038229261224\n",
      "Batch loss:  0.22331416606903076\n",
      "Batch loss:  0.08499398082494736\n",
      "Batch loss:  0.2690580189228058\n",
      "Batch loss:  0.11177733540534973\n",
      "Batch loss:  0.16889534890651703\n",
      "Batch loss:  0.1694595366716385\n",
      "Batch loss:  0.14109522104263306\n",
      "Batch loss:  0.18770697712898254\n",
      "Batch loss:  0.1360487937927246\n",
      "Batch loss:  0.25064149498939514\n",
      "Batch loss:  0.2353277951478958\n",
      "Batch loss:  0.22793832421302795\n",
      "Batch loss:  0.17828860878944397\n",
      "Batch loss:  0.11164113879203796\n",
      "Batch loss:  0.18831653892993927\n",
      "Batch loss:  0.22127331793308258\n",
      "Batch loss:  0.2571779787540436\n",
      "Batch loss:  0.17189297080039978\n",
      "Batch loss:  0.11578533053398132\n",
      "Batch loss:  0.22866880893707275\n",
      "Batch loss:  0.3198506832122803\n",
      "Batch loss:  0.10082133859395981\n",
      "Batch loss:  0.3257562518119812\n",
      "Batch loss:  0.2121603637933731\n",
      "Batch loss:  0.09848792105913162\n",
      "Batch loss:  0.2727632522583008\n",
      "Batch loss:  0.15577523410320282\n",
      "Batch loss:  0.16395923495292664\n",
      "Batch loss:  0.05781315267086029\n",
      "Batch loss:  0.127198725938797\n",
      "Batch loss:  0.18464110791683197\n",
      "Batch loss:  0.05338199809193611\n",
      "Batch loss:  0.19065126776695251\n",
      "Batch loss:  0.09650018811225891\n",
      "Batch loss:  0.11982524394989014\n",
      "Batch loss:  0.2582342326641083\n",
      "Batch loss:  0.22669227421283722\n",
      "Batch loss:  0.18513065576553345\n",
      "Batch loss:  0.2298913598060608\n",
      "Batch loss:  0.10254936665296555\n",
      "Batch loss:  0.11327963322401047\n",
      "Batch loss:  0.1071501076221466\n",
      "Batch loss:  0.09884827584028244\n",
      "Batch loss:  0.2735973298549652\n",
      "Batch loss:  0.2579708397388458\n",
      "Batch loss:  0.27757206559181213\n",
      "Batch loss:  0.4298059046268463\n",
      "Batch loss:  0.2135922908782959\n",
      "Batch loss:  0.18190984427928925\n",
      "Batch loss:  0.27466607093811035\n",
      "Batch loss:  0.11250271648168564\n",
      "Batch loss:  0.26981475949287415\n",
      "Batch loss:  0.06771904230117798\n",
      "Batch loss:  0.24737121164798737\n",
      "Batch loss:  0.11425520479679108\n",
      "Batch loss:  0.07782337069511414\n",
      "Batch loss:  0.24256326258182526\n",
      "Batch loss:  0.09608925133943558\n",
      "Batch loss:  0.1873222440481186\n",
      "Batch loss:  0.10268203169107437\n",
      "Batch loss:  0.26078516244888306\n",
      "Batch loss:  0.3483413755893707\n",
      "Batch loss:  0.22985319793224335\n",
      "Batch loss:  0.13170436024665833\n",
      "Batch loss:  0.18820106983184814\n",
      "Batch loss:  0.1495112031698227\n",
      "Batch loss:  0.2147652953863144\n",
      "Batch loss:  0.16418865323066711\n",
      "Batch loss:  0.12001696974039078\n",
      "Batch loss:  0.11213991045951843\n",
      "Batch loss:  0.21562747657299042\n",
      "Batch loss:  0.17772528529167175\n",
      "Batch loss:  0.10872584581375122\n",
      "Batch loss:  0.19574497640132904\n",
      "Batch loss:  0.18161897361278534\n",
      "Batch loss:  0.10265412926673889\n",
      "Batch loss:  0.2489335685968399\n",
      "Batch loss:  0.2891252636909485\n",
      "Batch loss:  0.2717537581920624\n",
      "Batch loss:  0.07810540497303009\n",
      "Batch loss:  0.2703878879547119\n",
      "Batch loss:  0.09867103397846222\n",
      "Batch loss:  0.22732320427894592\n",
      "Batch loss:  0.17750677466392517\n",
      "Batch loss:  0.16312265396118164\n",
      "Batch loss:  0.45049193501472473\n",
      "Batch loss:  0.19648344814777374\n",
      "Batch loss:  0.17511150240898132\n",
      "Batch loss:  0.38243716955184937\n",
      "Batch loss:  0.21189527213573456\n",
      "Batch loss:  0.11888468265533447\n",
      "Batch loss:  0.18730401992797852\n",
      "Batch loss:  0.5097110867500305\n",
      "Batch loss:  0.2752903401851654\n",
      "Batch loss:  0.21880687773227692\n",
      "Batch loss:  0.12743425369262695\n",
      "Batch loss:  0.13438384234905243\n",
      "Batch loss:  0.15023784339427948\n",
      "Batch loss:  0.10611727088689804\n",
      "Batch loss:  0.28800004720687866\n",
      "Current average loss: 0.2084801439188321\n",
      "Batch loss:  0.17368514835834503\n",
      "Batch loss:  0.22598442435264587\n",
      "Batch loss:  0.10452825576066971\n",
      "Batch loss:  0.20504707098007202\n",
      "Batch loss:  0.14914746582508087\n",
      "Batch loss:  0.26792827248573303\n",
      "Batch loss:  0.19465100765228271\n",
      "Batch loss:  0.16623543202877045\n",
      "Batch loss:  0.14525118470191956\n",
      "Batch loss:  0.10947045683860779\n",
      "Batch loss:  0.200171560049057\n",
      "Batch loss:  0.45867544412612915\n",
      "Batch loss:  0.3515734374523163\n",
      "Batch loss:  0.25273463129997253\n",
      "Batch loss:  0.2691788971424103\n",
      "Batch loss:  0.38176611065864563\n",
      "Batch loss:  0.13297578692436218\n",
      "Batch loss:  0.18096254765987396\n",
      "Batch loss:  0.15094713866710663\n",
      "Batch loss:  0.22607338428497314\n",
      "Batch loss:  0.41746193170547485\n",
      "Batch loss:  0.26386064291000366\n",
      "Batch loss:  0.08940383791923523\n",
      "Batch loss:  0.37951385974884033\n",
      "Batch loss:  0.2981266379356384\n",
      "Batch loss:  0.1951419711112976\n",
      "Batch loss:  0.1501910388469696\n",
      "Batch loss:  0.17615780234336853\n",
      "Batch loss:  0.174700528383255\n",
      "Batch loss:  0.10816081613302231\n",
      "Batch loss:  0.23927204310894012\n",
      "Batch loss:  0.10338107496500015\n",
      "Batch loss:  0.2604231536388397\n",
      "Batch loss:  0.17706149816513062\n",
      "Batch loss:  0.24950377643108368\n",
      "Batch loss:  0.1158396527171135\n",
      "Batch loss:  0.23022279143333435\n",
      "Batch loss:  0.24181848764419556\n",
      "Batch loss:  0.23828698694705963\n",
      "Batch loss:  0.37182632088661194\n",
      "Batch loss:  0.3576242923736572\n",
      "Batch loss:  0.21819664537906647\n",
      "Batch loss:  0.28656306862831116\n",
      "Batch loss:  0.15286971628665924\n",
      "Batch loss:  0.12482104450464249\n",
      "Batch loss:  0.16325068473815918\n",
      "Batch loss:  0.2171032875776291\n",
      "Batch loss:  0.11955416947603226\n",
      "Batch loss:  0.206583172082901\n",
      "Batch loss:  0.1419258713722229\n",
      "Batch loss:  0.14137381315231323\n",
      "Batch loss:  0.124192014336586\n",
      "Batch loss:  0.2303961217403412\n",
      "Batch loss:  0.16441898047924042\n",
      "Batch loss:  0.37431928515434265\n",
      "Batch loss:  0.23906080424785614\n",
      "Batch loss:  0.1603672057390213\n",
      "Batch loss:  0.4639217257499695\n",
      "Batch loss:  0.2169477641582489\n",
      "Batch loss:  0.1420230120420456\n",
      "Batch loss:  0.1240556538105011\n",
      "Batch loss:  0.27569302916526794\n",
      "Batch loss:  0.21823804080486298\n",
      "Batch loss:  0.21278418600559235\n",
      "Batch loss:  0.20873528718948364\n",
      "Batch loss:  0.4171706438064575\n",
      "Batch loss:  0.15176180005073547\n",
      "Batch loss:  0.1425294578075409\n",
      "Batch loss:  0.1940154731273651\n",
      "Batch loss:  0.14268556237220764\n",
      "Batch loss:  0.17654265463352203\n",
      "Batch loss:  0.16565895080566406\n",
      "Batch loss:  0.15816713869571686\n",
      "Batch loss:  0.1446591317653656\n",
      "Batch loss:  0.19620540738105774\n",
      "Batch loss:  0.1640007644891739\n",
      "Batch loss:  0.10969332605600357\n",
      "Batch loss:  0.22930917143821716\n",
      "Batch loss:  0.13174274563789368\n",
      "Batch loss:  0.30126628279685974\n",
      "Batch loss:  0.16502155363559723\n",
      "Batch loss:  0.27674686908721924\n",
      "Batch loss:  0.20746442675590515\n",
      "Batch loss:  0.1588871031999588\n",
      "Batch loss:  0.33483099937438965\n",
      "Batch loss:  0.1908571720123291\n",
      "Batch loss:  0.32450243830680847\n",
      "Batch loss:  0.271782249212265\n",
      "Batch loss:  0.22255218029022217\n",
      "Batch loss:  0.23795977234840393\n",
      "Batch loss:  0.17353050410747528\n",
      "Batch loss:  0.31545478105545044\n",
      "Batch loss:  0.1598455011844635\n",
      "Batch loss:  0.26690155267715454\n",
      "Batch loss:  0.22684210538864136\n",
      "Batch loss:  0.26604488492012024\n",
      "Batch loss:  0.17346103489398956\n",
      "Batch loss:  0.34547603130340576\n",
      "Batch loss:  0.19199246168136597\n",
      "Batch loss:  0.24729403853416443\n",
      "Current average loss: 0.20859308928149953\n",
      "Batch loss:  0.2001422494649887\n",
      "Batch loss:  0.32496893405914307\n",
      "Batch loss:  0.11318176984786987\n",
      "Batch loss:  0.19181512296199799\n",
      "Batch loss:  0.12350910902023315\n",
      "Batch loss:  0.2065812200307846\n",
      "Batch loss:  0.2938438653945923\n",
      "Batch loss:  0.14877349138259888\n",
      "Batch loss:  0.13864390552043915\n",
      "Batch loss:  0.23554888367652893\n",
      "Batch loss:  0.18775574862957\n",
      "Batch loss:  0.11813224107027054\n",
      "Batch loss:  0.25301748514175415\n",
      "Batch loss:  0.23235705494880676\n",
      "Batch loss:  0.14583534002304077\n",
      "Batch loss:  0.21379117667675018\n",
      "Batch loss:  0.14672251045703888\n",
      "Batch loss:  0.2305227816104889\n",
      "Batch loss:  0.2115916907787323\n",
      "Batch loss:  0.12178072333335876\n",
      "Batch loss:  0.1395522505044937\n",
      "Batch loss:  0.22113369405269623\n",
      "Batch loss:  0.22594109177589417\n",
      "Batch loss:  0.13754630088806152\n",
      "Batch loss:  0.11357193440198898\n",
      "Batch loss:  0.160782128572464\n",
      "Batch loss:  0.2908845841884613\n",
      "Batch loss:  0.206613227725029\n",
      "Batch loss:  0.21823304891586304\n",
      "Batch loss:  0.23937243223190308\n",
      "Batch loss:  0.20858153700828552\n",
      "Batch loss:  0.10881168395280838\n",
      "Batch loss:  0.15938928723335266\n",
      "Batch loss:  0.12965738773345947\n",
      "Batch loss:  0.06448132544755936\n",
      "Batch loss:  0.18341481685638428\n",
      "Batch loss:  0.18307478725910187\n",
      "Batch loss:  0.1411406546831131\n",
      "Batch loss:  0.1403481811285019\n",
      "Batch loss:  0.21214787662029266\n",
      "Batch loss:  0.17420582473278046\n",
      "Batch loss:  0.20664052665233612\n",
      "Batch loss:  0.10566814243793488\n",
      "Batch loss:  0.28611794114112854\n",
      "Batch loss:  0.15408428013324738\n",
      "Batch loss:  0.12998919188976288\n",
      "Batch loss:  0.19665317237377167\n",
      "Batch loss:  0.10482120513916016\n",
      "Batch loss:  0.19654899835586548\n",
      "Batch loss:  0.24387797713279724\n",
      "Batch loss:  0.11357557773590088\n",
      "Batch loss:  0.11800850927829742\n",
      "Batch loss:  0.12164502590894699\n",
      "Batch loss:  0.1735811084508896\n",
      "Batch loss:  0.16106794774532318\n",
      "Batch loss:  0.18608860671520233\n",
      "Batch loss:  0.2916417121887207\n",
      "Batch loss:  0.17528638243675232\n",
      "Batch loss:  0.20978261530399323\n",
      "Batch loss:  0.11338627338409424\n",
      "Batch loss:  0.20005205273628235\n",
      "Batch loss:  0.20028433203697205\n",
      "Batch loss:  0.30578839778900146\n",
      "Batch loss:  0.2446209192276001\n",
      "Batch loss:  0.14535139501094818\n",
      "Batch loss:  0.23765189945697784\n",
      "Batch loss:  0.07838232815265656\n",
      "Batch loss:  0.17216378450393677\n",
      "Batch loss:  0.10844578593969345\n",
      "Batch loss:  0.1732877492904663\n",
      "Batch loss:  0.1965770423412323\n",
      "Batch loss:  0.246634379029274\n",
      "Batch loss:  0.18057005107402802\n",
      "Batch loss:  0.164781391620636\n",
      "Batch loss:  0.12897080183029175\n",
      "Batch loss:  0.23741982877254486\n",
      "Batch loss:  0.22396571934223175\n",
      "Batch loss:  0.16617080569267273\n",
      "Batch loss:  0.31608662009239197\n",
      "Batch loss:  0.11309286952018738\n",
      "Batch loss:  0.12302588671445847\n",
      "Batch loss:  0.12637387216091156\n",
      "Batch loss:  0.22846932709217072\n",
      "Batch loss:  0.13255353271961212\n",
      "Batch loss:  0.12868724763393402\n",
      "Batch loss:  0.17598290741443634\n",
      "Batch loss:  0.12263770401477814\n",
      "Batch loss:  0.21260479092597961\n",
      "Batch loss:  0.17215557396411896\n",
      "Batch loss:  0.29486221075057983\n",
      "Batch loss:  0.178645521402359\n",
      "Batch loss:  0.3432832658290863\n",
      "Batch loss:  0.18385881185531616\n",
      "Batch loss:  0.1573728621006012\n",
      "Batch loss:  0.14267301559448242\n",
      "Batch loss:  0.2108382135629654\n",
      "Batch loss:  0.31001272797584534\n",
      "Batch loss:  0.17576752603054047\n",
      "Batch loss:  0.10738390684127808\n",
      "Batch loss:  0.17132523655891418\n",
      "Current average loss: 0.20825962893691302\n",
      "Batch loss:  0.10708052664995193\n",
      "Batch loss:  0.1683465838432312\n",
      "Batch loss:  0.2382165938615799\n",
      "Batch loss:  0.2449318766593933\n",
      "Batch loss:  0.18057994544506073\n",
      "Batch loss:  0.18071287870407104\n",
      "Batch loss:  0.2525347173213959\n",
      "Batch loss:  0.16078589856624603\n",
      "Batch loss:  0.249041348695755\n",
      "Batch loss:  0.089029461145401\n",
      "Batch loss:  0.20620912313461304\n",
      "Batch loss:  0.20517046749591827\n",
      "Batch loss:  0.18489043414592743\n",
      "Batch loss:  0.19831891357898712\n",
      "Batch loss:  0.2285335212945938\n",
      "Batch loss:  0.17388345301151276\n",
      "Batch loss:  0.2146134376525879\n",
      "Batch loss:  0.19009582698345184\n",
      "Batch loss:  0.1472359597682953\n",
      "Batch loss:  0.26998502016067505\n",
      "Batch loss:  0.17853301763534546\n",
      "Batch loss:  0.1264439821243286\n",
      "Batch loss:  0.24803923070430756\n",
      "Batch loss:  0.26760539412498474\n",
      "Batch loss:  0.43555375933647156\n",
      "Batch loss:  0.18460185825824738\n",
      "Batch loss:  0.2071249932050705\n",
      "Batch loss:  0.16985508799552917\n",
      "Batch loss:  0.17430557310581207\n",
      "Batch loss:  0.1483892798423767\n",
      "Batch loss:  0.16696199774742126\n",
      "Batch loss:  0.15779583156108856\n",
      "Batch loss:  0.18245913088321686\n",
      "Batch loss:  0.24422208964824677\n",
      "Batch loss:  0.15198643505573273\n",
      "Batch loss:  0.23812653124332428\n",
      "Batch loss:  0.10781756043434143\n",
      "Batch loss:  0.11510627716779709\n",
      "Batch loss:  0.25992971658706665\n",
      "Batch loss:  0.19354207813739777\n",
      "Batch loss:  0.1431153416633606\n",
      "Batch loss:  0.14117765426635742\n",
      "Batch loss:  0.22191974520683289\n",
      "Batch loss:  0.13968782126903534\n",
      "Batch loss:  0.10658330470323563\n",
      "Batch loss:  0.09419988095760345\n",
      "Batch loss:  0.1805904060602188\n",
      "Batch loss:  0.3313359022140503\n",
      "Batch loss:  0.32452520728111267\n",
      "Batch loss:  0.232612743973732\n",
      "Batch loss:  0.19385503232479095\n",
      "Batch loss:  0.25386667251586914\n",
      "Batch loss:  0.16811755299568176\n",
      "Batch loss:  0.16953438520431519\n",
      "Batch loss:  0.1563824564218521\n",
      "Batch loss:  0.17026005685329437\n",
      "Batch loss:  0.20475919544696808\n",
      "Batch loss:  0.15264208614826202\n",
      "Batch loss:  0.20337700843811035\n",
      "Batch loss:  0.2231481671333313\n",
      "Batch loss:  0.41820961236953735\n",
      "Batch loss:  0.20423643290996552\n",
      "Batch loss:  0.20405282080173492\n",
      "Batch loss:  0.1018197238445282\n",
      "Batch loss:  0.1967131495475769\n",
      "Batch loss:  0.2778756618499756\n",
      "Batch loss:  0.3141176104545593\n",
      "Batch loss:  0.10995495319366455\n",
      "Batch loss:  0.15905995666980743\n",
      "Batch loss:  0.11283527314662933\n",
      "Batch loss:  0.3758447468280792\n",
      "Batch loss:  0.16566616296768188\n",
      "Batch loss:  0.1360253095626831\n",
      "Batch loss:  0.12105126678943634\n",
      "Batch loss:  0.22366659343242645\n",
      "Batch loss:  0.18221579492092133\n",
      "Batch loss:  0.11729088425636292\n",
      "Batch loss:  0.10834789276123047\n",
      "Batch loss:  0.14403817057609558\n",
      "Batch loss:  0.17136231064796448\n",
      "Batch loss:  0.17495429515838623\n",
      "Batch loss:  0.1434159129858017\n",
      "Batch loss:  0.14954008162021637\n",
      "Batch loss:  0.06041869521141052\n",
      "Batch loss:  0.21494607627391815\n",
      "Batch loss:  0.17547887563705444\n",
      "Batch loss:  0.11859586834907532\n",
      "Batch loss:  0.10361269861459732\n",
      "Batch loss:  0.13757026195526123\n",
      "Batch loss:  0.1439543217420578\n",
      "Batch loss:  0.15824294090270996\n",
      "Batch loss:  0.19135944545269012\n",
      "Batch loss:  0.2290945202112198\n",
      "Batch loss:  0.45515623688697815\n",
      "Batch loss:  0.10203437507152557\n",
      "Batch loss:  0.10606222599744797\n",
      "Batch loss:  0.26341474056243896\n",
      "Batch loss:  0.24603569507598877\n",
      "Batch loss:  0.15621697902679443\n",
      "Batch loss:  0.14823873341083527\n",
      "Current average loss: 0.20802031597080223\n",
      "Batch loss:  0.08195296674966812\n",
      "Batch loss:  0.14907881617546082\n",
      "Batch loss:  0.31649014353752136\n",
      "Batch loss:  0.11898022145032883\n",
      "Batch loss:  0.15607281029224396\n",
      "Batch loss:  0.1689402461051941\n",
      "Batch loss:  0.12322015315294266\n",
      "Batch loss:  0.41145506501197815\n",
      "Batch loss:  0.12125005573034286\n",
      "Batch loss:  0.2109723836183548\n",
      "Batch loss:  0.1986793428659439\n",
      "Batch loss:  0.2944300174713135\n",
      "Batch loss:  0.2997368574142456\n",
      "Batch loss:  0.10502241551876068\n",
      "Batch loss:  0.15885260701179504\n",
      "Batch loss:  0.300675630569458\n",
      "Batch loss:  0.18218596279621124\n",
      "Batch loss:  0.34294864535331726\n",
      "Batch loss:  0.23806658387184143\n",
      "Batch loss:  0.3202340602874756\n",
      "Batch loss:  0.1543780416250229\n",
      "Batch loss:  0.33106985688209534\n",
      "Batch loss:  0.2716052234172821\n",
      "Batch loss:  0.13399232923984528\n",
      "Batch loss:  0.3451930582523346\n",
      "Batch loss:  0.09836459904909134\n",
      "Batch loss:  0.11933570355176926\n",
      "Batch loss:  0.2900436818599701\n",
      "Batch loss:  0.1739615499973297\n",
      "Batch loss:  0.12240586429834366\n",
      "Batch loss:  0.23125894367694855\n",
      "Batch loss:  0.24366115033626556\n",
      "Batch loss:  0.11441255360841751\n",
      "Batch loss:  0.19652527570724487\n",
      "Batch loss:  0.17753534018993378\n",
      "Batch loss:  0.10463199019432068\n",
      "Batch loss:  0.14724455773830414\n",
      "Batch loss:  0.22329357266426086\n",
      "Batch loss:  0.1494944542646408\n",
      "Batch loss:  0.19379428029060364\n",
      "Batch loss:  0.16842812299728394\n",
      "Batch loss:  0.1628497689962387\n",
      "Batch loss:  0.16745196282863617\n",
      "Batch loss:  0.07940651476383209\n",
      "Batch loss:  0.2735700309276581\n",
      "Batch loss:  0.16964004933834076\n",
      "Batch loss:  0.41432684659957886\n",
      "Batch loss:  0.10252217948436737\n",
      "Batch loss:  0.261854350566864\n",
      "Batch loss:  0.14087168872356415\n",
      "Batch loss:  0.15018033981323242\n",
      "Batch loss:  0.24783436954021454\n",
      "Batch loss:  0.1567765325307846\n",
      "Batch loss:  0.1891494244337082\n",
      "Batch loss:  0.0996987596154213\n",
      "Batch loss:  0.18307939171791077\n",
      "Batch loss:  0.08225509524345398\n",
      "Batch loss:  0.12787982821464539\n",
      "Batch loss:  0.27027231454849243\n",
      "Batch loss:  0.39256858825683594\n",
      "Batch loss:  0.23032939434051514\n",
      "Batch loss:  0.13701476156711578\n",
      "Batch loss:  0.23253346979618073\n",
      "Batch loss:  0.11241235584020615\n",
      "Batch loss:  0.19692131876945496\n",
      "Batch loss:  0.08113077282905579\n",
      "Batch loss:  0.1241963729262352\n",
      "Batch loss:  0.2012232542037964\n",
      "Batch loss:  0.15148954093456268\n",
      "Batch loss:  0.1406451314687729\n",
      "Batch loss:  0.34519919753074646\n",
      "Batch loss:  0.1452089250087738\n",
      "Batch loss:  0.17775534093379974\n",
      "Batch loss:  0.0981871709227562\n",
      "Batch loss:  0.18761031329631805\n",
      "Batch loss:  0.04653646796941757\n",
      "Batch loss:  0.14586621522903442\n",
      "Batch loss:  0.23943160474300385\n",
      "Batch loss:  0.24513274431228638\n",
      "Batch loss:  0.13729090988636017\n",
      "Batch loss:  0.16892290115356445\n",
      "Batch loss:  0.10692844539880753\n",
      "Batch loss:  0.26370763778686523\n",
      "Batch loss:  0.3033103346824646\n",
      "Batch loss:  0.26042574644088745\n",
      "Batch loss:  0.19860710203647614\n",
      "Batch loss:  0.07985205203294754\n",
      "Batch loss:  0.13832321763038635\n",
      "Batch loss:  0.09415490925312042\n",
      "Batch loss:  0.23692306876182556\n",
      "Batch loss:  0.2263432890176773\n",
      "Batch loss:  0.3678053021430969\n",
      "Batch loss:  0.17710500955581665\n",
      "Batch loss:  0.262608140707016\n",
      "Batch loss:  0.21229560673236847\n",
      "Batch loss:  0.21902808547019958\n",
      "Batch loss:  0.10168375074863434\n",
      "Batch loss:  0.20734788477420807\n",
      "Batch loss:  0.14927321672439575\n",
      "Batch loss:  0.19283203780651093\n",
      "Current average loss: 0.20782849755973376\n",
      "Batch loss:  0.21219119429588318\n",
      "Batch loss:  0.4139794111251831\n",
      "Batch loss:  0.2697805166244507\n",
      "Batch loss:  0.14038856327533722\n",
      "Batch loss:  0.16213636100292206\n",
      "Batch loss:  0.1357833743095398\n",
      "Batch loss:  0.17961931228637695\n",
      "Batch loss:  0.21832647919654846\n",
      "Batch loss:  0.3661308288574219\n",
      "Batch loss:  0.24440361559391022\n",
      "Batch loss:  0.16559261083602905\n",
      "Batch loss:  0.27571430802345276\n",
      "Batch loss:  0.08430373668670654\n",
      "Batch loss:  0.29049497842788696\n",
      "Batch loss:  0.12135756015777588\n",
      "Batch loss:  0.19238539040088654\n",
      "Batch loss:  0.27498143911361694\n",
      "Batch loss:  0.2786666750907898\n",
      "Batch loss:  0.1222880631685257\n",
      "Batch loss:  0.1766728013753891\n",
      "Batch loss:  0.32294008135795593\n",
      "Batch loss:  0.27401164174079895\n",
      "Batch loss:  0.1219148337841034\n",
      "Batch loss:  0.27431201934814453\n",
      "Batch loss:  0.11009449511766434\n",
      "Batch loss:  0.2675253748893738\n",
      "Batch loss:  0.3038421869277954\n",
      "Batch loss:  0.1913398951292038\n",
      "Batch loss:  0.27350518107414246\n",
      "Batch loss:  0.2133307009935379\n",
      "Batch loss:  0.31430816650390625\n",
      "Batch loss:  0.19578473269939423\n",
      "Batch loss:  0.28715333342552185\n",
      "Batch loss:  0.17351515591144562\n",
      "Batch loss:  0.10134389251470566\n",
      "Batch loss:  0.25487038493156433\n",
      "Batch loss:  0.22541354596614838\n",
      "Batch loss:  0.2151055634021759\n",
      "Batch loss:  0.1662878394126892\n",
      "Batch loss:  0.24583329260349274\n",
      "Batch loss:  0.15509340167045593\n",
      "Batch loss:  0.15539516508579254\n",
      "Batch loss:  0.34016185998916626\n",
      "Batch loss:  0.17978419363498688\n",
      "Batch loss:  0.2764495015144348\n",
      "Batch loss:  0.20120152831077576\n",
      "Batch loss:  0.1379033923149109\n",
      "Batch loss:  0.21158768236637115\n",
      "Batch loss:  0.18757927417755127\n",
      "Batch loss:  0.10016695410013199\n",
      "Batch loss:  0.18034589290618896\n",
      "Batch loss:  0.12597370147705078\n",
      "Batch loss:  0.11718548834323883\n",
      "Batch loss:  0.3132847249507904\n",
      "Batch loss:  0.185328871011734\n",
      "Batch loss:  0.22359409928321838\n",
      "Batch loss:  0.22068333625793457\n",
      "Batch loss:  0.12789471447467804\n",
      "Batch loss:  0.1588796079158783\n",
      "Batch loss:  0.2346848100423813\n",
      "Batch loss:  0.14054681360721588\n",
      "Batch loss:  0.16737699508666992\n",
      "Batch loss:  0.27021437883377075\n",
      "Batch loss:  0.29627951979637146\n",
      "Batch loss:  0.27956271171569824\n",
      "Batch loss:  0.11546345800161362\n",
      "Batch loss:  0.23571865260601044\n",
      "Batch loss:  0.17679762840270996\n",
      "Batch loss:  0.24578173458576202\n",
      "Batch loss:  0.11212629079818726\n",
      "Batch loss:  0.16532842814922333\n",
      "Batch loss:  0.1740977168083191\n",
      "Batch loss:  0.32399699091911316\n",
      "Batch loss:  0.12829336524009705\n",
      "Batch loss:  0.2501247823238373\n",
      "Batch loss:  0.12324760109186172\n",
      "Batch loss:  0.285224050283432\n",
      "Batch loss:  0.38480839133262634\n",
      "Batch loss:  0.11717581748962402\n",
      "Batch loss:  0.2599199414253235\n",
      "Batch loss:  0.13159728050231934\n",
      "Batch loss:  0.1769293248653412\n",
      "Batch loss:  0.30488428473472595\n",
      "Batch loss:  0.3340686559677124\n",
      "Batch loss:  0.25537148118019104\n",
      "Batch loss:  0.1003599762916565\n",
      "Batch loss:  0.15694157779216766\n",
      "Batch loss:  0.11564348638057709\n",
      "Batch loss:  0.14810653030872345\n",
      "Batch loss:  0.22900649905204773\n",
      "Batch loss:  0.19295436143875122\n",
      "Batch loss:  0.18139594793319702\n",
      "Batch loss:  0.14400967955589294\n",
      "Batch loss:  0.17138953506946564\n",
      "Batch loss:  0.1829269528388977\n",
      "Batch loss:  0.29665178060531616\n",
      "Batch loss:  0.40248045325279236\n",
      "Batch loss:  0.23235540091991425\n",
      "Batch loss:  0.19849471747875214\n",
      "Batch loss:  0.21197865903377533\n",
      "Current average loss: 0.207869959379141\n",
      "Batch loss:  0.15536196529865265\n",
      "Batch loss:  0.28292524814605713\n",
      "Batch loss:  0.15178035199642181\n",
      "Batch loss:  0.257475882768631\n",
      "Batch loss:  0.24093453586101532\n",
      "Batch loss:  0.13156545162200928\n",
      "Batch loss:  0.22690747678279877\n",
      "Batch loss:  0.15157368779182434\n",
      "Batch loss:  0.15871170163154602\n",
      "Batch loss:  0.14401626586914062\n",
      "Batch loss:  0.09214011579751968\n",
      "Batch loss:  0.2031814157962799\n",
      "Batch loss:  0.1421077847480774\n",
      "Batch loss:  0.3033672571182251\n",
      "Batch loss:  0.1083739697933197\n",
      "Batch loss:  0.2535836100578308\n",
      "Batch loss:  0.13200731575489044\n",
      "Batch loss:  0.21646882593631744\n",
      "Batch loss:  0.12913061678409576\n",
      "Batch loss:  0.2572180926799774\n",
      "Batch loss:  0.19973446428775787\n",
      "Batch loss:  0.16316841542720795\n",
      "Batch loss:  0.1579189896583557\n",
      "Batch loss:  0.09759362041950226\n",
      "Batch loss:  0.19664514064788818\n",
      "Batch loss:  0.11238664388656616\n",
      "Batch loss:  0.17668555676937103\n",
      "Batch loss:  0.1547539234161377\n",
      "Batch loss:  0.19111347198486328\n",
      "Batch loss:  0.16183748841285706\n",
      "Batch loss:  0.2115224152803421\n",
      "Batch loss:  0.1629941165447235\n",
      "Batch loss:  0.16476450860500336\n",
      "Batch loss:  0.2347242832183838\n",
      "Batch loss:  0.24362044036388397\n",
      "Batch loss:  0.15059427917003632\n",
      "Batch loss:  0.23064804077148438\n",
      "Batch loss:  0.18439331650733948\n",
      "Batch loss:  0.1857227385044098\n",
      "Batch loss:  0.09549527615308762\n",
      "Batch loss:  0.1926346868276596\n",
      "Batch loss:  0.18307803571224213\n",
      "Batch loss:  0.18636101484298706\n",
      "Batch loss:  0.1870241016149521\n",
      "Batch loss:  0.13758507370948792\n",
      "Batch loss:  0.381106972694397\n",
      "Batch loss:  0.2667759656906128\n",
      "Batch loss:  0.10843577235937119\n",
      "Batch loss:  0.23562441766262054\n",
      "Batch loss:  0.31803345680236816\n",
      "Batch loss:  0.14105619490146637\n",
      "Batch loss:  0.1421651989221573\n",
      "Batch loss:  0.11181928217411041\n",
      "Batch loss:  0.13698279857635498\n",
      "Batch loss:  0.21500883996486664\n",
      "Batch loss:  0.36578401923179626\n",
      "Batch loss:  0.09822655469179153\n",
      "Batch loss:  0.33912035822868347\n",
      "Batch loss:  0.16485515236854553\n",
      "Batch loss:  0.23342613875865936\n",
      "Batch loss:  0.388400137424469\n",
      "Batch loss:  0.1478269249200821\n",
      "Batch loss:  0.15673163533210754\n",
      "Batch loss:  0.15960240364074707\n",
      "Batch loss:  0.18927805125713348\n",
      "Batch loss:  0.0582076758146286\n",
      "Batch loss:  0.30131950974464417\n",
      "Batch loss:  0.3103874623775482\n",
      "Batch loss:  0.0738205835223198\n",
      "Batch loss:  0.2795605957508087\n",
      "Batch loss:  0.15951377153396606\n",
      "Batch loss:  0.3636263310909271\n",
      "Batch loss:  0.19219788908958435\n",
      "Batch loss:  0.12490910291671753\n",
      "Batch loss:  0.17474651336669922\n",
      "Batch loss:  0.155324786901474\n",
      "Batch loss:  0.2302340567111969\n",
      "Batch loss:  0.40560322999954224\n",
      "Batch loss:  0.11499132215976715\n",
      "Batch loss:  0.3548123836517334\n",
      "Batch loss:  0.09349536150693893\n",
      "Batch loss:  0.16341662406921387\n",
      "Batch loss:  0.17444750666618347\n",
      "Batch loss:  0.14999745786190033\n",
      "Batch loss:  0.3500392735004425\n",
      "Batch loss:  0.1655624359846115\n",
      "Batch loss:  0.16599206626415253\n",
      "Batch loss:  0.31396567821502686\n",
      "Batch loss:  0.25798487663269043\n",
      "Batch loss:  0.5402881503105164\n",
      "Batch loss:  0.16481110453605652\n",
      "Batch loss:  0.13276463747024536\n",
      "Batch loss:  0.09427371621131897\n",
      "Batch loss:  0.15988555550575256\n",
      "Batch loss:  0.13991855084896088\n",
      "Batch loss:  0.23229584097862244\n",
      "Batch loss:  0.13629716634750366\n",
      "Batch loss:  0.09412728995084763\n",
      "Batch loss:  0.08715096116065979\n",
      "Batch loss:  0.17777612805366516\n",
      "Current average loss: 0.20770858474464107\n",
      "Batch loss:  0.2147517204284668\n",
      "Batch loss:  0.20553943514823914\n",
      "Batch loss:  0.35453882813453674\n",
      "Batch loss:  0.20137645304203033\n",
      "Batch loss:  0.16173012554645538\n",
      "Batch loss:  0.29881155490875244\n",
      "Batch loss:  0.23792503774166107\n",
      "Batch loss:  0.14882566034793854\n",
      "Batch loss:  0.10094576328992844\n",
      "Batch loss:  0.1380450278520584\n",
      "Batch loss:  0.17917859554290771\n",
      "Batch loss:  0.1594466269016266\n",
      "Batch loss:  0.12221524864435196\n",
      "Batch loss:  0.09096745401620865\n",
      "Batch loss:  0.2323988676071167\n",
      "Batch loss:  0.0664733499288559\n",
      "Batch loss:  0.17236576974391937\n",
      "Batch loss:  0.20924311876296997\n",
      "Batch loss:  0.13321103155612946\n",
      "Batch loss:  0.10782638937234879\n",
      "Batch loss:  0.13492855429649353\n",
      "Batch loss:  0.11027172952890396\n",
      "Batch loss:  0.34986844658851624\n",
      "Batch loss:  0.21343478560447693\n",
      "Batch loss:  0.14802567660808563\n",
      "Batch loss:  0.1820891946554184\n",
      "Batch loss:  0.1787916123867035\n",
      "Batch loss:  0.10208401829004288\n",
      "Batch loss:  0.24729345738887787\n",
      "Batch loss:  0.15414102375507355\n",
      "Batch loss:  0.1297391951084137\n",
      "Batch loss:  0.2940477132797241\n",
      "Batch loss:  0.2459414303302765\n",
      "Batch loss:  0.18908600509166718\n",
      "Batch loss:  0.15854807198047638\n",
      "Batch loss:  0.11581844836473465\n",
      "Batch loss:  0.1850743442773819\n",
      "Batch loss:  0.06910277903079987\n",
      "Batch loss:  0.160521999001503\n",
      "Batch loss:  0.17988015711307526\n",
      "Batch loss:  0.38570094108581543\n",
      "Batch loss:  0.19494448602199554\n",
      "Batch loss:  0.10836907476186752\n",
      "Batch loss:  0.14685280621051788\n",
      "Batch loss:  0.2359027862548828\n",
      "Batch loss:  0.2127394676208496\n",
      "Batch loss:  0.2077113538980484\n",
      "Batch loss:  0.2241823375225067\n",
      "Batch loss:  0.10226067900657654\n",
      "Batch loss:  0.25388944149017334\n",
      "Batch loss:  0.1696396917104721\n",
      "Batch loss:  0.1741512268781662\n",
      "Batch loss:  0.21881501376628876\n",
      "Batch loss:  0.3454117476940155\n",
      "Batch loss:  0.25646281242370605\n",
      "Batch loss:  0.19295614957809448\n",
      "Batch loss:  0.2498008906841278\n",
      "Batch loss:  0.17060743272304535\n",
      "Batch loss:  0.27167952060699463\n",
      "Batch loss:  0.08977589011192322\n",
      "Batch loss:  0.1716456264257431\n",
      "Batch loss:  0.1291494369506836\n",
      "Batch loss:  0.13631580770015717\n",
      "Batch loss:  0.30125755071640015\n",
      "Batch loss:  0.2744801342487335\n",
      "Batch loss:  0.257214218378067\n",
      "Batch loss:  0.33107513189315796\n",
      "Batch loss:  0.2680753469467163\n",
      "Batch loss:  0.1357572376728058\n",
      "Batch loss:  0.13783271610736847\n",
      "Batch loss:  0.17095163464546204\n",
      "Batch loss:  0.22117237746715546\n",
      "Batch loss:  0.12435953319072723\n",
      "Batch loss:  0.24071569740772247\n",
      "Batch loss:  0.2744917869567871\n",
      "Batch loss:  0.19972488284111023\n",
      "Batch loss:  0.2568739950656891\n",
      "Batch loss:  0.192453995347023\n",
      "Batch loss:  0.11274240911006927\n",
      "Batch loss:  0.09215640276670456\n",
      "Batch loss:  0.09846139699220657\n",
      "Batch loss:  0.1333499252796173\n",
      "Batch loss:  0.28214046359062195\n",
      "Batch loss:  0.23294122517108917\n",
      "Batch loss:  0.28400442004203796\n",
      "Batch loss:  0.0867186039686203\n",
      "Batch loss:  0.14708110690116882\n",
      "Batch loss:  0.15707559883594513\n",
      "Batch loss:  0.1577126830816269\n",
      "Batch loss:  0.1535140722990036\n",
      "Batch loss:  0.18345223367214203\n",
      "Batch loss:  0.11197590082883835\n",
      "Batch loss:  0.2958534061908722\n",
      "Batch loss:  0.06926792114973068\n",
      "Batch loss:  0.1458061784505844\n",
      "Batch loss:  0.20000532269477844\n",
      "Batch loss:  0.3640120327472687\n",
      "Batch loss:  0.20643150806427002\n",
      "Batch loss:  0.2450404167175293\n",
      "Batch loss:  0.16933251917362213\n",
      "Current average loss: 0.2074960335576855\n",
      "Batch loss:  0.1532086730003357\n",
      "Batch loss:  0.20258083939552307\n",
      "Batch loss:  0.13090291619300842\n",
      "Batch loss:  0.16488313674926758\n",
      "Batch loss:  0.24977166950702667\n",
      "Batch loss:  0.21261437237262726\n",
      "Batch loss:  0.16141027212142944\n",
      "Batch loss:  0.13590069115161896\n",
      "Batch loss:  0.21774537861347198\n",
      "Batch loss:  0.14736512303352356\n",
      "Batch loss:  0.22508253157138824\n",
      "Batch loss:  0.16862738132476807\n",
      "Batch loss:  0.23910517990589142\n",
      "Batch loss:  0.17552383244037628\n",
      "Batch loss:  0.12547209858894348\n",
      "Batch loss:  0.33956262469291687\n",
      "Batch loss:  0.16659414768218994\n",
      "Batch loss:  0.20693045854568481\n",
      "Batch loss:  0.10357467085123062\n",
      "Batch loss:  0.17370186746120453\n",
      "Batch loss:  0.12033718079328537\n",
      "Batch loss:  0.26024630665779114\n",
      "Batch loss:  0.16390323638916016\n",
      "Batch loss:  0.107661172747612\n",
      "Batch loss:  0.15672916173934937\n",
      "Batch loss:  0.1337142288684845\n",
      "Batch loss:  0.08543151617050171\n",
      "Batch loss:  0.20540344715118408\n",
      "Batch loss:  0.10466738045215607\n",
      "Batch loss:  0.2791694402694702\n",
      "Batch loss:  0.07137994468212128\n",
      "Batch loss:  0.18823356926441193\n",
      "Batch loss:  0.17041806876659393\n",
      "Batch loss:  0.29747068881988525\n",
      "Batch loss:  0.18662522733211517\n",
      "Batch loss:  0.28132113814353943\n",
      "Batch loss:  0.14878535270690918\n",
      "Batch loss:  0.1428629457950592\n",
      "Batch loss:  0.21516470611095428\n",
      "Batch loss:  0.2450169324874878\n",
      "Batch loss:  0.17341917753219604\n",
      "Batch loss:  0.11878950893878937\n",
      "Batch loss:  0.11935259401798248\n",
      "Batch loss:  0.25090309977531433\n",
      "Batch loss:  0.1749640256166458\n",
      "Batch loss:  0.37119078636169434\n",
      "Batch loss:  0.35656875371932983\n",
      "Batch loss:  0.17338216304779053\n",
      "Batch loss:  0.2759566307067871\n",
      "Batch loss:  0.46168601512908936\n",
      "Batch loss:  0.20965802669525146\n",
      "Batch loss:  0.13014480471611023\n",
      "Batch loss:  0.14973782002925873\n",
      "Batch loss:  0.08461512625217438\n",
      "Batch loss:  0.280059814453125\n",
      "Batch loss:  0.25898969173431396\n",
      "Batch loss:  0.13904663920402527\n",
      "Batch loss:  0.16580721735954285\n",
      "Batch loss:  0.12525144219398499\n",
      "Batch loss:  0.18709971010684967\n",
      "Batch loss:  0.295563668012619\n",
      "Batch loss:  0.17906714975833893\n",
      "Batch loss:  0.21177203953266144\n",
      "Batch loss:  0.10569734871387482\n",
      "Batch loss:  0.0792202576994896\n",
      "Batch loss:  0.1445406824350357\n",
      "Batch loss:  0.22543877363204956\n",
      "Batch loss:  0.10316651314496994\n",
      "Batch loss:  0.07872854173183441\n",
      "Batch loss:  0.21056805551052094\n",
      "Batch loss:  0.14712166786193848\n",
      "Batch loss:  0.1293056160211563\n",
      "Batch loss:  0.14579831063747406\n",
      "Batch loss:  0.10307380557060242\n",
      "Batch loss:  0.13938285410404205\n",
      "Batch loss:  0.23522327840328217\n",
      "Batch loss:  0.09966325014829636\n",
      "Batch loss:  0.19568920135498047\n",
      "Batch loss:  0.3086989223957062\n",
      "Batch loss:  0.17608030140399933\n",
      "Batch loss:  0.33321627974510193\n",
      "Batch loss:  0.2160388082265854\n",
      "Batch loss:  0.18402260541915894\n",
      "Batch loss:  0.18139471113681793\n",
      "Batch loss:  0.12013143301010132\n",
      "Batch loss:  0.1992848515510559\n",
      "Batch loss:  0.18937791883945465\n",
      "Batch loss:  0.145891010761261\n",
      "Batch loss:  0.17701101303100586\n",
      "Batch loss:  0.16778740286827087\n",
      "Batch loss:  0.1852148175239563\n",
      "Batch loss:  0.10083479434251785\n",
      "Batch loss:  0.1318681240081787\n",
      "Batch loss:  0.19444027543067932\n",
      "Batch loss:  0.19720961153507233\n",
      "Batch loss:  0.27958810329437256\n",
      "Batch loss:  0.2085411250591278\n",
      "Batch loss:  0.08857754617929459\n",
      "Batch loss:  0.18913988769054413\n",
      "Batch loss:  0.23293930292129517\n",
      "Current average loss: 0.2072227045810996\n",
      "Batch loss:  0.13096332550048828\n",
      "Batch loss:  0.12398698180913925\n",
      "Batch loss:  0.13629889488220215\n",
      "Batch loss:  0.1269546002149582\n",
      "Batch loss:  0.16543763875961304\n",
      "Batch loss:  0.29197996854782104\n",
      "Batch loss:  0.08849699795246124\n",
      "Batch loss:  0.22362633049488068\n",
      "Batch loss:  0.09856939315795898\n",
      "Batch loss:  0.40895915031433105\n",
      "Batch loss:  0.333383172750473\n",
      "Batch loss:  0.15088017284870148\n",
      "Batch loss:  0.20892192423343658\n",
      "Batch loss:  0.2147466540336609\n",
      "Batch loss:  0.09450032562017441\n",
      "Batch loss:  0.3387841582298279\n",
      "Batch loss:  0.21106354892253876\n",
      "Batch loss:  0.21604569256305695\n",
      "Batch loss:  0.15934476256370544\n",
      "Batch loss:  0.22377943992614746\n",
      "Batch loss:  0.26606258749961853\n",
      "Batch loss:  0.3082555830478668\n",
      "Batch loss:  0.07441359758377075\n",
      "Batch loss:  0.21059313416481018\n",
      "Batch loss:  0.30884844064712524\n",
      "Batch loss:  0.09634643793106079\n",
      "Batch loss:  0.2883893549442291\n",
      "Batch loss:  0.1472141444683075\n",
      "Batch loss:  0.13623349368572235\n",
      "Batch loss:  0.1230303943157196\n",
      "Batch loss:  0.15420472621917725\n",
      "Batch loss:  0.1952534019947052\n",
      "Batch loss:  0.09358029067516327\n",
      "Batch loss:  0.18469823896884918\n",
      "Batch loss:  0.08105285465717316\n",
      "Batch loss:  0.16169248521327972\n",
      "Batch loss:  0.20611074566841125\n",
      "Batch loss:  0.11578227579593658\n",
      "Batch loss:  0.24065563082695007\n",
      "Batch loss:  0.30079665780067444\n",
      "Batch loss:  0.17900893092155457\n",
      "Batch loss:  0.06232234835624695\n",
      "Batch loss:  0.22225528955459595\n",
      "Batch loss:  0.21733447909355164\n",
      "Batch loss:  0.11576776951551437\n",
      "Batch loss:  0.45986035466194153\n",
      "Batch loss:  0.1084270104765892\n",
      "Batch loss:  0.16904018819332123\n",
      "Batch loss:  0.20419660210609436\n",
      "Batch loss:  0.15663208067417145\n",
      "Batch loss:  0.10028663277626038\n",
      "Batch loss:  0.1315617710351944\n",
      "Batch loss:  0.1934197098016739\n",
      "Batch loss:  0.20548057556152344\n",
      "Batch loss:  0.07318972796201706\n",
      "Batch loss:  0.2655187249183655\n",
      "Batch loss:  0.2532629370689392\n",
      "Batch loss:  0.1797112673521042\n",
      "Batch loss:  0.22488372027873993\n",
      "Batch loss:  0.5292328000068665\n",
      "Batch loss:  0.12361124157905579\n",
      "Batch loss:  0.23731346428394318\n",
      "Batch loss:  0.5160176157951355\n",
      "Batch loss:  0.12362079322338104\n",
      "Batch loss:  0.1623440831899643\n",
      "Batch loss:  0.2218044251203537\n",
      "Batch loss:  0.21653753519058228\n",
      "Batch loss:  0.19390735030174255\n",
      "Batch loss:  0.18636058270931244\n",
      "Batch loss:  0.10233502089977264\n",
      "Batch loss:  0.46359267830848694\n",
      "Batch loss:  0.1535588800907135\n",
      "Batch loss:  0.14790283143520355\n",
      "Batch loss:  0.3589133322238922\n",
      "Batch loss:  0.1722012758255005\n",
      "Batch loss:  0.24755354225635529\n",
      "Batch loss:  0.12809287011623383\n",
      "Batch loss:  0.2480453997850418\n",
      "Batch loss:  0.22908984124660492\n",
      "Batch loss:  0.2195873260498047\n",
      "Batch loss:  0.25317665934562683\n",
      "Batch loss:  0.1690627485513687\n",
      "Batch loss:  0.40645819902420044\n",
      "Batch loss:  0.32639411091804504\n",
      "Batch loss:  0.18212799727916718\n",
      "Batch loss:  0.20937961339950562\n",
      "Batch loss:  0.13818152248859406\n",
      "Batch loss:  0.14194951951503754\n",
      "Batch loss:  0.295479953289032\n",
      "Batch loss:  0.24210414290428162\n",
      "Batch loss:  0.5027188658714294\n",
      "Batch loss:  0.2065625786781311\n",
      "Batch loss:  0.20594212412834167\n",
      "Batch loss:  0.31799980998039246\n",
      "Batch loss:  0.15607424080371857\n",
      "Batch loss:  0.2404736429452896\n",
      "Batch loss:  0.30818355083465576\n",
      "Batch loss:  0.08499506115913391\n",
      "Batch loss:  0.15553328394889832\n",
      "Batch loss:  0.15103985369205475\n",
      "Current average loss: 0.20723659298490968\n",
      "Batch loss:  0.1416044384241104\n",
      "Batch loss:  0.10089292377233505\n",
      "Batch loss:  0.1994633674621582\n",
      "Batch loss:  0.24891036748886108\n",
      "Batch loss:  0.27977147698402405\n",
      "Batch loss:  0.16208763420581818\n",
      "Batch loss:  0.12601681053638458\n",
      "Batch loss:  0.09122096002101898\n",
      "Batch loss:  0.2515469491481781\n",
      "Batch loss:  0.18838657438755035\n",
      "Batch loss:  0.2103719860315323\n",
      "Batch loss:  0.31746262311935425\n",
      "Batch loss:  0.26721182465553284\n",
      "Batch loss:  0.23920795321464539\n",
      "Batch loss:  0.23312997817993164\n",
      "Batch loss:  0.21869532763957977\n",
      "Batch loss:  0.09997575730085373\n",
      "Batch loss:  0.15143154561519623\n",
      "Batch loss:  0.15740378201007843\n",
      "Batch loss:  0.14919792115688324\n",
      "Batch loss:  0.11094990372657776\n",
      "Batch loss:  0.18057003617286682\n",
      "Batch loss:  0.23516523838043213\n",
      "Batch loss:  0.13086654245853424\n",
      "Batch loss:  0.24913398921489716\n",
      "Batch loss:  0.16952449083328247\n",
      "Batch loss:  0.1593085080385208\n",
      "Batch loss:  0.2024248093366623\n",
      "Batch loss:  0.09055186808109283\n",
      "Batch loss:  0.13028964400291443\n",
      "Batch loss:  0.15745101869106293\n",
      "Batch loss:  0.2204028218984604\n",
      "Batch loss:  0.23456773161888123\n",
      "Batch loss:  0.42755889892578125\n",
      "Batch loss:  0.36389628052711487\n",
      "Batch loss:  0.12814435362815857\n",
      "Batch loss:  0.29389744997024536\n",
      "Batch loss:  0.18179762363433838\n",
      "Batch loss:  0.20080024003982544\n",
      "Batch loss:  0.17358051240444183\n",
      "Batch loss:  0.2118878960609436\n",
      "Batch loss:  0.38164156675338745\n",
      "Batch loss:  0.07886414974927902\n",
      "Batch loss:  0.16639924049377441\n",
      "Batch loss:  0.19854380190372467\n",
      "Batch loss:  0.23740185797214508\n",
      "Batch loss:  0.13196399807929993\n",
      "Batch loss:  0.2587199807167053\n",
      "Batch loss:  0.1661929190158844\n",
      "Batch loss:  0.17723001539707184\n",
      "Batch loss:  0.6738301515579224\n",
      "Batch loss:  0.14522668719291687\n",
      "Batch loss:  0.10466503351926804\n",
      "Batch loss:  0.3412311375141144\n",
      "Batch loss:  0.26777225732803345\n",
      "Batch loss:  0.2423488348722458\n",
      "Batch loss:  0.28203561902046204\n",
      "Batch loss:  0.27411097288131714\n",
      "Batch loss:  0.16748587787151337\n",
      "Batch loss:  0.19030649960041046\n",
      "Batch loss:  0.111496202647686\n",
      "Batch loss:  0.1513890027999878\n",
      "Batch loss:  0.14178721606731415\n",
      "Batch loss:  0.10449172556400299\n",
      "Batch loss:  0.11222433298826218\n",
      "Batch loss:  0.10009743273258209\n",
      "Batch loss:  0.2711670696735382\n",
      "Batch loss:  0.1636658012866974\n",
      "Batch loss:  0.19558770954608917\n",
      "Batch loss:  0.18511031568050385\n",
      "Batch loss:  0.12323543429374695\n",
      "Batch loss:  0.36402854323387146\n",
      "Batch loss:  0.31987887620925903\n",
      "Batch loss:  0.5796604752540588\n",
      "Batch loss:  0.1704629361629486\n",
      "Batch loss:  0.23574820160865784\n",
      "Batch loss:  0.07496272772550583\n",
      "Batch loss:  0.22219492495059967\n",
      "Batch loss:  0.23218289017677307\n",
      "Batch loss:  0.19739779829978943\n",
      "Batch loss:  0.14913107454776764\n",
      "Batch loss:  0.18810319900512695\n",
      "Batch loss:  0.17496952414512634\n",
      "Batch loss:  0.11868245899677277\n",
      "Batch loss:  0.24825789034366608\n",
      "Batch loss:  0.13103212416172028\n",
      "Batch loss:  0.13526581227779388\n",
      "Batch loss:  0.11746009439229965\n",
      "Batch loss:  0.19101789593696594\n",
      "Batch loss:  0.25959599018096924\n",
      "Batch loss:  0.12473098188638687\n",
      "Batch loss:  0.10563507676124573\n",
      "Batch loss:  0.1250864416360855\n",
      "Batch loss:  0.22681209444999695\n",
      "Batch loss:  0.20572529733181\n",
      "Batch loss:  0.13888822495937347\n",
      "Batch loss:  0.09143982082605362\n",
      "Batch loss:  0.14110971987247467\n",
      "Batch loss:  0.27061012387275696\n",
      "Batch loss:  0.2741982638835907\n",
      "Current average loss: 0.20715607793769117\n",
      "Batch loss:  0.3858110308647156\n",
      "Batch loss:  0.13624009490013123\n",
      "Batch loss:  0.2112164944410324\n",
      "Batch loss:  0.19273656606674194\n",
      "Batch loss:  0.1676933467388153\n",
      "Batch loss:  0.1484874188899994\n",
      "Batch loss:  0.10843725502490997\n",
      "Batch loss:  0.32961416244506836\n",
      "Batch loss:  0.09076707065105438\n",
      "Batch loss:  0.21130293607711792\n",
      "Batch loss:  0.09235857427120209\n",
      "Batch loss:  0.16564111411571503\n",
      "Batch loss:  0.2074623703956604\n",
      "Batch loss:  0.22718127071857452\n",
      "Batch loss:  0.20916704833507538\n",
      "Batch loss:  0.06903742998838425\n",
      "Batch loss:  0.17387720942497253\n",
      "Batch loss:  0.15212450921535492\n",
      "Batch loss:  0.14009609818458557\n",
      "Batch loss:  0.16565153002738953\n",
      "Batch loss:  0.05675414577126503\n",
      "Batch loss:  0.34872689843177795\n",
      "Batch loss:  0.23845672607421875\n",
      "Batch loss:  0.2094125896692276\n",
      "Batch loss:  0.1818435937166214\n",
      "Batch loss:  0.2085379958152771\n",
      "Batch loss:  0.12348439544439316\n",
      "Batch loss:  0.1690642237663269\n",
      "Batch loss:  0.1650976687669754\n",
      "Batch loss:  0.14767229557037354\n",
      "Batch loss:  0.0756157860159874\n",
      "Batch loss:  0.11519335955381393\n",
      "Batch loss:  0.20028196275234222\n",
      "Batch loss:  0.17093156278133392\n",
      "Batch loss:  0.20780439674854279\n",
      "Batch loss:  0.08108402043581009\n",
      "Batch loss:  0.3535369038581848\n",
      "Batch loss:  0.14994893968105316\n",
      "Batch loss:  0.20016933977603912\n",
      "Batch loss:  0.14840130507946014\n",
      "Batch loss:  0.5568137168884277\n",
      "Batch loss:  0.35461413860321045\n",
      "Batch loss:  0.26176247000694275\n",
      "Batch loss:  0.20575062930583954\n",
      "Batch loss:  0.2687026858329773\n",
      "Batch loss:  0.15012440085411072\n",
      "Batch loss:  0.11278863996267319\n",
      "Batch loss:  0.15890152752399445\n",
      "Batch loss:  0.15743711590766907\n",
      "Batch loss:  0.10847269743680954\n",
      "Batch loss:  0.1794506013393402\n",
      "Batch loss:  0.1363084316253662\n",
      "Batch loss:  0.23242388665676117\n",
      "Batch loss:  0.09939347952604294\n",
      "Batch loss:  0.15540063381195068\n",
      "Batch loss:  0.10789041221141815\n",
      "Batch loss:  0.1762816309928894\n",
      "Batch loss:  0.05499272048473358\n",
      "Batch loss:  0.18179938197135925\n",
      "Batch loss:  0.2551387548446655\n",
      "Batch loss:  0.39259523153305054\n",
      "Batch loss:  0.2950496971607208\n",
      "Batch loss:  0.23992769420146942\n",
      "Batch loss:  0.12418244779109955\n",
      "Batch loss:  0.14411255717277527\n",
      "Batch loss:  0.12080568075180054\n",
      "Batch loss:  0.2838481366634369\n",
      "Batch loss:  0.28953230381011963\n",
      "Batch loss:  0.10637133568525314\n",
      "Batch loss:  0.30845001339912415\n",
      "Batch loss:  0.09040608257055283\n",
      "Batch loss:  0.3934851288795471\n",
      "Batch loss:  0.23190025985240936\n",
      "Batch loss:  0.07825823873281479\n",
      "Batch loss:  0.12850137054920197\n",
      "Batch loss:  0.06536725908517838\n",
      "Batch loss:  0.1406339704990387\n",
      "Batch loss:  0.09622815996408463\n",
      "Batch loss:  0.20727209746837616\n",
      "Batch loss:  0.09034096449613571\n",
      "Batch loss:  0.3629758656024933\n",
      "Batch loss:  0.18980014324188232\n",
      "Batch loss:  0.12502709031105042\n",
      "Batch loss:  0.13910087943077087\n",
      "Batch loss:  0.3333364427089691\n",
      "Batch loss:  0.14816142618656158\n",
      "Batch loss:  0.17581823468208313\n",
      "Batch loss:  0.12541969120502472\n",
      "Batch loss:  0.11967784911394119\n",
      "Batch loss:  0.13617180287837982\n",
      "Batch loss:  0.09686535596847534\n",
      "Batch loss:  0.1442510485649109\n",
      "Batch loss:  0.3565935790538788\n",
      "Batch loss:  0.13813969492912292\n",
      "Batch loss:  0.11365614086389542\n",
      "Batch loss:  0.3062451481819153\n",
      "Batch loss:  0.17342205345630646\n",
      "Batch loss:  0.189157173037529\n",
      "Batch loss:  0.15098966658115387\n",
      "Batch loss:  0.08666209876537323\n",
      "Current average loss: 0.20689428542122645\n",
      "Batch loss:  0.12116704136133194\n",
      "Batch loss:  0.16839031875133514\n",
      "Batch loss:  0.24975603818893433\n",
      "Batch loss:  0.11051247268915176\n",
      "Batch loss:  0.1310490518808365\n",
      "Batch loss:  0.23028549551963806\n",
      "Batch loss:  0.25474029779434204\n",
      "Batch loss:  0.17292170226573944\n",
      "Batch loss:  0.15800833702087402\n",
      "Batch loss:  0.12957632541656494\n",
      "Batch loss:  0.13507188856601715\n",
      "Batch loss:  0.13466031849384308\n",
      "Batch loss:  0.10990990698337555\n",
      "Batch loss:  0.23202721774578094\n",
      "Batch loss:  0.24939465522766113\n",
      "Batch loss:  0.208882674574852\n",
      "Batch loss:  0.2614923417568207\n",
      "Batch loss:  0.10995561629533768\n",
      "Batch loss:  0.13801443576812744\n",
      "Batch loss:  0.357969731092453\n",
      "Batch loss:  0.2419290691614151\n",
      "Batch loss:  0.11337393522262573\n",
      "Batch loss:  0.17845912277698517\n",
      "Batch loss:  0.23689818382263184\n",
      "Batch loss:  0.11771814525127411\n",
      "Batch loss:  0.12223739922046661\n",
      "Batch loss:  0.32986459136009216\n",
      "Batch loss:  0.22402499616146088\n",
      "Batch loss:  0.13554087281227112\n",
      "Batch loss:  0.17610129714012146\n",
      "Batch loss:  0.3060271143913269\n",
      "Batch loss:  0.20405565202236176\n",
      "Batch loss:  0.2050524801015854\n",
      "Batch loss:  0.2612071931362152\n",
      "Batch loss:  0.19003960490226746\n",
      "Batch loss:  0.2508432865142822\n",
      "Batch loss:  0.1734229475259781\n",
      "Batch loss:  0.27823081612586975\n",
      "Batch loss:  0.14496055245399475\n",
      "Batch loss:  0.09310983121395111\n",
      "Batch loss:  0.3031718134880066\n",
      "Batch loss:  0.3494303524494171\n",
      "Batch loss:  0.12396623939275742\n",
      "Batch loss:  0.12800799310207367\n",
      "Batch loss:  0.3599700331687927\n",
      "Batch loss:  0.17955128848552704\n",
      "Batch loss:  0.24030078947544098\n",
      "Batch loss:  0.21245083212852478\n",
      "Batch loss:  0.14627006649971008\n",
      "Batch loss:  0.18240612745285034\n",
      "Batch loss:  0.28814956545829773\n",
      "Batch loss:  0.14417876303195953\n",
      "Batch loss:  0.13364271819591522\n",
      "Batch loss:  0.24298229813575745\n",
      "Batch loss:  0.15795285999774933\n",
      "Batch loss:  0.1527978777885437\n",
      "Batch loss:  0.08077355474233627\n",
      "Batch loss:  0.12387950718402863\n",
      "Batch loss:  0.22997359931468964\n",
      "Batch loss:  0.3270759582519531\n",
      "Batch loss:  0.1290789544582367\n",
      "Batch loss:  0.2175990343093872\n",
      "Batch loss:  0.23069240152835846\n",
      "Batch loss:  0.09794988483190536\n",
      "Batch loss:  0.14869852364063263\n",
      "Batch loss:  0.15995509922504425\n",
      "Batch loss:  0.13041901588439941\n",
      "Batch loss:  0.11865376681089401\n",
      "Batch loss:  0.16535191237926483\n",
      "Batch loss:  0.33458593487739563\n",
      "Batch loss:  0.2528942823410034\n",
      "Batch loss:  0.24687732756137848\n",
      "Batch loss:  0.10757394134998322\n",
      "Batch loss:  0.11617171764373779\n",
      "Batch loss:  0.10582982748746872\n",
      "Batch loss:  0.08651912957429886\n",
      "Batch loss:  0.13987179100513458\n",
      "Batch loss:  0.18484769761562347\n",
      "Batch loss:  0.20180578529834747\n",
      "Batch loss:  0.15025098621845245\n",
      "Batch loss:  0.3089855909347534\n",
      "Batch loss:  0.1828971654176712\n",
      "Batch loss:  0.14442244172096252\n",
      "Batch loss:  0.21297064423561096\n",
      "Batch loss:  0.21337850391864777\n",
      "Batch loss:  0.2081485539674759\n",
      "Batch loss:  0.1185019388794899\n",
      "Batch loss:  0.373977929353714\n",
      "Batch loss:  0.23523962497711182\n",
      "Batch loss:  0.30575263500213623\n",
      "Batch loss:  0.163347527384758\n",
      "Batch loss:  0.20807600021362305\n",
      "Batch loss:  0.22488123178482056\n",
      "Batch loss:  0.1682671755552292\n",
      "Batch loss:  0.1220860555768013\n",
      "Batch loss:  0.10167334228754044\n",
      "Batch loss:  0.1519767791032791\n",
      "Batch loss:  0.2812460958957672\n",
      "Batch loss:  0.14150777459144592\n",
      "Batch loss:  0.1232791468501091\n",
      "Current average loss: 0.2067060084564577\n",
      "Batch loss:  0.21933557093143463\n",
      "Batch loss:  0.21079906821250916\n",
      "Batch loss:  0.24998299777507782\n",
      "Batch loss:  0.18684957921504974\n",
      "Batch loss:  0.15857557952404022\n",
      "Batch loss:  0.20400778949260712\n",
      "Batch loss:  0.2134988009929657\n",
      "Batch loss:  0.22650380432605743\n",
      "Batch loss:  0.09567449241876602\n",
      "Batch loss:  0.19788281619548798\n",
      "Batch loss:  0.11599572002887726\n",
      "Batch loss:  0.17172810435295105\n",
      "Batch loss:  0.135005921125412\n",
      "Batch loss:  0.18014396727085114\n",
      "Batch loss:  0.22476738691329956\n",
      "Batch loss:  0.10655588656663895\n",
      "Batch loss:  0.15592406690120697\n",
      "Batch loss:  0.13369092345237732\n",
      "Batch loss:  0.1370486617088318\n",
      "Batch loss:  0.12110495567321777\n",
      "Batch loss:  0.08073333650827408\n",
      "Batch loss:  0.1679539531469345\n",
      "Batch loss:  0.2126348316669464\n",
      "Batch loss:  0.23844151198863983\n",
      "Batch loss:  0.21350647509098053\n",
      "Batch loss:  0.2022022157907486\n",
      "Batch loss:  0.32802408933639526\n",
      "Batch loss:  0.25199952721595764\n",
      "Batch loss:  0.14335235953330994\n",
      "Batch loss:  0.2546759247779846\n",
      "Batch loss:  0.3124217689037323\n",
      "Batch loss:  0.30898594856262207\n",
      "Batch loss:  0.09174595773220062\n",
      "Batch loss:  0.08627671748399734\n",
      "Batch loss:  0.16519923508167267\n",
      "Batch loss:  0.2873367667198181\n",
      "Batch loss:  0.12078377604484558\n",
      "Batch loss:  0.3148204982280731\n",
      "Batch loss:  0.1491554081439972\n",
      "Batch loss:  0.22081486880779266\n",
      "Batch loss:  0.26939037442207336\n",
      "Batch loss:  0.09045526385307312\n",
      "Batch loss:  0.18268728256225586\n",
      "Batch loss:  0.09802516549825668\n",
      "Batch loss:  0.24335725605487823\n",
      "Batch loss:  0.2355888932943344\n",
      "Batch loss:  0.25485461950302124\n",
      "Batch loss:  0.16473488509655\n",
      "Batch loss:  0.36998802423477173\n",
      "Batch loss:  0.24186013638973236\n",
      "Batch loss:  0.15928244590759277\n",
      "Batch loss:  0.3186950385570526\n",
      "Batch loss:  0.16033194959163666\n",
      "Batch loss:  0.09784050285816193\n",
      "Batch loss:  0.4240976572036743\n",
      "Batch loss:  0.3151823580265045\n",
      "Batch loss:  0.121910460293293\n",
      "Batch loss:  0.10038133710622787\n",
      "Batch loss:  0.17520393431186676\n",
      "Batch loss:  0.13329458236694336\n",
      "Batch loss:  0.09131191670894623\n",
      "Batch loss:  0.09455784410238266\n",
      "Batch loss:  0.09306886792182922\n",
      "Batch loss:  0.19887927174568176\n",
      "Batch loss:  0.23608119785785675\n",
      "Batch loss:  0.20707838237285614\n",
      "Batch loss:  0.15648193657398224\n",
      "Batch loss:  0.18977738916873932\n",
      "Batch loss:  0.25873810052871704\n",
      "Batch loss:  0.16900157928466797\n",
      "Batch loss:  0.1899324655532837\n",
      "Batch loss:  0.14398270845413208\n",
      "Batch loss:  0.21476498246192932\n",
      "Batch loss:  0.19133837521076202\n",
      "Batch loss:  0.224809467792511\n",
      "Batch loss:  0.22529591619968414\n",
      "Batch loss:  0.1723717302083969\n",
      "Batch loss:  0.08058257400989532\n",
      "Batch loss:  0.19118399918079376\n",
      "Batch loss:  0.1852361112833023\n",
      "Batch loss:  0.24220237135887146\n",
      "Batch loss:  0.3334055542945862\n",
      "Batch loss:  0.1112135797739029\n",
      "Batch loss:  0.08829661458730698\n",
      "Batch loss:  0.208168625831604\n",
      "Batch loss:  0.2739331126213074\n",
      "Batch loss:  0.27616915106773376\n",
      "Batch loss:  0.10000047832727432\n",
      "Batch loss:  0.2856113910675049\n",
      "Batch loss:  0.16003736853599548\n",
      "Batch loss:  0.19518287479877472\n",
      "Batch loss:  0.14536044001579285\n",
      "Batch loss:  0.12015507370233536\n",
      "Batch loss:  0.12193591892719269\n",
      "Batch loss:  0.13841094076633453\n",
      "Batch loss:  0.09354029595851898\n",
      "Batch loss:  0.10448751598596573\n",
      "Batch loss:  0.09631799161434174\n",
      "Batch loss:  0.09035686403512955\n",
      "Batch loss:  0.12146089971065521\n",
      "Current average loss: 0.20646505045909305\n",
      "Batch loss:  0.11781636625528336\n",
      "Batch loss:  0.05957615375518799\n",
      "Batch loss:  0.2486971914768219\n",
      "Batch loss:  0.2648046910762787\n",
      "Batch loss:  0.21356526017189026\n",
      "Batch loss:  0.06212417781352997\n",
      "Batch loss:  0.21836979687213898\n",
      "Batch loss:  0.1637684851884842\n",
      "Batch loss:  0.1642160266637802\n",
      "Batch loss:  0.2084784209728241\n",
      "Batch loss:  0.27121806144714355\n",
      "Batch loss:  0.13635435700416565\n",
      "Batch loss:  0.19446144998073578\n",
      "Batch loss:  0.1456584632396698\n",
      "Batch loss:  0.11439342796802521\n",
      "Batch loss:  0.14475828409194946\n",
      "Batch loss:  0.19800391793251038\n",
      "Batch loss:  0.4021512567996979\n",
      "Batch loss:  0.23466596007347107\n",
      "Batch loss:  0.27516862750053406\n",
      "Batch loss:  0.19133710861206055\n",
      "Batch loss:  0.31118008494377136\n",
      "Batch loss:  0.21677902340888977\n",
      "Batch loss:  0.3058429956436157\n",
      "Batch loss:  0.4387783706188202\n",
      "Batch loss:  0.25942695140838623\n",
      "Batch loss:  0.15371903777122498\n",
      "Batch loss:  0.1445336639881134\n",
      "Batch loss:  0.18855033814907074\n",
      "Batch loss:  0.08477528393268585\n",
      "Batch loss:  0.09298915416002274\n",
      "Batch loss:  0.16869311034679413\n",
      "Batch loss:  0.16568508744239807\n",
      "Batch loss:  0.23016993701457977\n",
      "Batch loss:  0.2586464285850525\n",
      "Batch loss:  0.18021398782730103\n",
      "Batch loss:  0.1721573919057846\n",
      "Batch loss:  0.23924551904201508\n",
      "Batch loss:  0.17662620544433594\n",
      "Batch loss:  0.25774258375167847\n",
      "Batch loss:  0.15417470037937164\n",
      "Batch loss:  0.10953106731176376\n",
      "Batch loss:  0.3248055577278137\n",
      "Batch loss:  0.21514353156089783\n",
      "Batch loss:  0.26226532459259033\n",
      "Batch loss:  0.2102205604314804\n",
      "Batch loss:  0.25879761576652527\n",
      "Batch loss:  0.1847074329853058\n",
      "Batch loss:  0.19174587726593018\n",
      "Batch loss:  0.09568753838539124\n",
      "Batch loss:  0.22339993715286255\n",
      "Batch loss:  0.19000078737735748\n",
      "Batch loss:  0.1844642609357834\n",
      "Batch loss:  0.236195906996727\n",
      "Batch loss:  0.1872485876083374\n",
      "Batch loss:  0.20333755016326904\n",
      "Batch loss:  0.21507631242275238\n",
      "Batch loss:  0.17069773375988007\n",
      "Batch loss:  0.12997762858867645\n",
      "Batch loss:  0.09683714061975479\n",
      "Batch loss:  0.3225715160369873\n",
      "Batch loss:  0.17314821481704712\n",
      "Batch loss:  0.09456390142440796\n",
      "Batch loss:  0.21036943793296814\n",
      "Batch loss:  0.17599840462207794\n",
      "Batch loss:  0.16624601185321808\n",
      "Batch loss:  0.1823541671037674\n",
      "Batch loss:  0.11207519471645355\n",
      "Batch loss:  0.3252291977405548\n",
      "Batch loss:  0.23053143918514252\n",
      "Batch loss:  0.12689638137817383\n",
      "Batch loss:  0.18285688757896423\n",
      "Batch loss:  0.14840663969516754\n",
      "Batch loss:  0.1464104950428009\n",
      "Batch loss:  0.2075994461774826\n",
      "Batch loss:  0.2678931951522827\n",
      "Batch loss:  0.1164112463593483\n",
      "Batch loss:  0.27783018350601196\n",
      "Batch loss:  0.23439890146255493\n",
      "Batch loss:  0.09362104535102844\n",
      "Batch loss:  0.28144702315330505\n",
      "Batch loss:  0.21050168573856354\n",
      "Batch loss:  0.2413208782672882\n",
      "Batch loss:  0.4705737233161926\n",
      "Batch loss:  0.13592684268951416\n",
      "Batch loss:  0.17557597160339355\n",
      "Batch loss:  0.18172362446784973\n",
      "Batch loss:  0.257524311542511\n",
      "Batch loss:  0.15918390452861786\n",
      "Batch loss:  0.2381664663553238\n",
      "Batch loss:  0.1297655701637268\n",
      "Batch loss:  0.19986897706985474\n",
      "Batch loss:  0.14309005439281464\n",
      "Batch loss:  0.15443430840969086\n",
      "Batch loss:  0.29884496331214905\n",
      "Batch loss:  0.24283438920974731\n",
      "Batch loss:  0.14509068429470062\n",
      "Batch loss:  0.22340403497219086\n",
      "Batch loss:  0.15850995481014252\n",
      "Batch loss:  0.08100690692663193\n",
      "Current average loss: 0.2063738515985959\n",
      "Batch loss:  0.18609383702278137\n",
      "Batch loss:  0.10775948315858841\n",
      "Batch loss:  0.24690848588943481\n",
      "Batch loss:  0.12523135542869568\n",
      "Batch loss:  0.18220825493335724\n",
      "Batch loss:  0.14365889132022858\n",
      "Batch loss:  0.1591813564300537\n",
      "Batch loss:  0.17711861431598663\n",
      "Batch loss:  0.25269070267677307\n",
      "Batch loss:  0.13797561824321747\n",
      "Batch loss:  0.17346951365470886\n",
      "Batch loss:  0.15288309752941132\n",
      "Batch loss:  0.1875515580177307\n",
      "Batch loss:  0.21358655393123627\n",
      "Batch loss:  0.1707918345928192\n",
      "Batch loss:  0.15898314118385315\n",
      "Batch loss:  0.17011675238609314\n",
      "Batch loss:  0.1372102051973343\n",
      "Batch loss:  0.13112179934978485\n",
      "Batch loss:  0.08682098984718323\n",
      "Batch loss:  0.16474922001361847\n",
      "Batch loss:  0.18426208198070526\n",
      "Batch loss:  0.1021629348397255\n",
      "Batch loss:  0.1581364870071411\n",
      "Batch loss:  0.3245679438114166\n",
      "Batch loss:  0.1768399327993393\n",
      "Batch loss:  0.3142920434474945\n",
      "Batch loss:  0.1990738809108734\n",
      "Batch loss:  0.19211909174919128\n",
      "Batch loss:  0.14569757878780365\n",
      "Batch loss:  0.13871526718139648\n",
      "Batch loss:  0.10095660388469696\n",
      "Batch loss:  0.2304309606552124\n",
      "Batch loss:  0.12646716833114624\n",
      "Batch loss:  0.2480388879776001\n",
      "Batch loss:  0.22585918009281158\n",
      "Batch loss:  0.1863097995519638\n",
      "Batch loss:  0.12734907865524292\n",
      "Batch loss:  0.18718114495277405\n",
      "Batch loss:  0.17143109440803528\n",
      "Batch loss:  0.08436579257249832\n",
      "Batch loss:  0.2002384066581726\n",
      "Batch loss:  0.1400209665298462\n",
      "Batch loss:  0.11274202167987823\n",
      "Batch loss:  0.15185271203517914\n",
      "Batch loss:  0.18808892369270325\n",
      "Batch loss:  0.19395893812179565\n",
      "Batch loss:  0.24672597646713257\n",
      "Batch loss:  0.2014850378036499\n",
      "Batch loss:  0.13250593841075897\n",
      "Batch loss:  0.3237967789173126\n",
      "Batch loss:  0.24922138452529907\n",
      "Batch loss:  0.07971092313528061\n",
      "Batch loss:  0.08771724998950958\n",
      "Batch loss:  0.5368700623512268\n",
      "Batch loss:  0.17378690838813782\n",
      "Batch loss:  0.09377364814281464\n",
      "Batch loss:  0.17402313649654388\n",
      "Batch loss:  0.07470603287220001\n",
      "Batch loss:  0.14121809601783752\n",
      "Batch loss:  0.15787023305892944\n",
      "Batch loss:  0.4480988383293152\n",
      "Batch loss:  0.347444087266922\n",
      "Batch loss:  0.10336407274007797\n",
      "Batch loss:  0.3665783107280731\n",
      "Batch loss:  0.22546517848968506\n",
      "Batch loss:  0.14411067962646484\n",
      "Batch loss:  0.267292320728302\n",
      "Batch loss:  0.15912587940692902\n",
      "Batch loss:  0.1950187236070633\n",
      "Batch loss:  0.1772329956293106\n",
      "Batch loss:  0.11290866881608963\n",
      "Batch loss:  0.19150209426879883\n",
      "Batch loss:  0.18480348587036133\n",
      "Batch loss:  0.14964336156845093\n",
      "Batch loss:  0.19388535618782043\n",
      "Batch loss:  0.15205954015254974\n",
      "Batch loss:  0.0832100361585617\n",
      "Batch loss:  0.19216668605804443\n",
      "Batch loss:  0.2773171365261078\n",
      "Batch loss:  0.28745245933532715\n",
      "Batch loss:  0.19285663962364197\n",
      "Batch loss:  0.13118772208690643\n",
      "Batch loss:  0.12279809266328812\n",
      "Batch loss:  0.12102900445461273\n",
      "Batch loss:  0.19286485016345978\n",
      "Batch loss:  0.206033855676651\n",
      "Batch loss:  0.166854590177536\n",
      "Batch loss:  0.08128906786441803\n",
      "Batch loss:  0.28242284059524536\n",
      "Batch loss:  0.13539104163646698\n",
      "Batch loss:  0.21246519684791565\n",
      "Batch loss:  0.18910948932170868\n",
      "Batch loss:  0.11290694028139114\n",
      "Batch loss:  0.12064898014068604\n",
      "Batch loss:  0.22437939047813416\n",
      "Batch loss:  0.1332695633172989\n",
      "Batch loss:  0.23050029575824738\n",
      "Batch loss:  0.25629326701164246\n",
      "Batch loss:  0.20906862616539001\n",
      "Current average loss: 0.2061147060868459\n",
      "Batch loss:  0.15614433586597443\n",
      "Batch loss:  0.35923194885253906\n",
      "Batch loss:  0.2180248647928238\n",
      "Batch loss:  0.15918393433094025\n",
      "Batch loss:  0.31269651651382446\n",
      "Batch loss:  0.3100336492061615\n",
      "Batch loss:  0.22205784916877747\n",
      "Batch loss:  0.17598214745521545\n",
      "Batch loss:  0.25337401032447815\n",
      "Batch loss:  0.20613546669483185\n",
      "Batch loss:  0.23786622285842896\n",
      "Batch loss:  0.21863263845443726\n",
      "Batch loss:  0.25633886456489563\n",
      "Batch loss:  0.22277909517288208\n",
      "Batch loss:  0.100694939494133\n",
      "Batch loss:  0.2339833676815033\n",
      "Batch loss:  0.11068613827228546\n",
      "Batch loss:  0.3070320188999176\n",
      "Batch loss:  0.17463132739067078\n",
      "Batch loss:  0.3244335949420929\n",
      "Batch loss:  0.2441115528345108\n",
      "Batch loss:  0.16818423569202423\n",
      "Batch loss:  0.13267236948013306\n",
      "Batch loss:  0.1451207995414734\n",
      "Batch loss:  0.20352190732955933\n",
      "Batch loss:  0.2199001908302307\n",
      "Batch loss:  0.21499665081501007\n",
      "Batch loss:  0.1665966510772705\n",
      "Batch loss:  0.13744109869003296\n",
      "Batch loss:  0.442838579416275\n",
      "Batch loss:  0.2020072340965271\n",
      "Batch loss:  0.2035665661096573\n",
      "Batch loss:  0.13703739643096924\n",
      "Batch loss:  0.10731632262468338\n",
      "Batch loss:  0.13777346909046173\n",
      "Batch loss:  0.356597900390625\n",
      "Batch loss:  0.22695215046405792\n",
      "Batch loss:  0.1484943926334381\n",
      "Batch loss:  0.19391930103302002\n",
      "Batch loss:  0.11222436279058456\n",
      "Batch loss:  0.2556731700897217\n",
      "Batch loss:  0.20190821588039398\n",
      "Batch loss:  0.1696740835905075\n",
      "Batch loss:  0.3123304545879364\n",
      "Batch loss:  0.09018762409687042\n",
      "Batch loss:  0.17614294588565826\n",
      "Batch loss:  0.211864173412323\n",
      "Batch loss:  0.13174599409103394\n",
      "Batch loss:  0.11263478547334671\n",
      "Batch loss:  0.37706828117370605\n",
      "Batch loss:  0.15633375942707062\n",
      "Batch loss:  0.0801621824502945\n",
      "Batch loss:  0.1499508172273636\n",
      "Batch loss:  0.14321079850196838\n",
      "Batch loss:  0.1877993941307068\n",
      "Batch loss:  0.20660027861595154\n",
      "Batch loss:  0.2661443054676056\n",
      "Batch loss:  0.26636815071105957\n",
      "Batch loss:  0.045575615018606186\n",
      "Batch loss:  0.19199854135513306\n",
      "Batch loss:  0.12300919741392136\n",
      "Batch loss:  0.1319085657596588\n",
      "Batch loss:  0.23993735015392303\n",
      "Batch loss:  0.1865924894809723\n",
      "Batch loss:  0.21933314204216003\n",
      "Batch loss:  0.11715779453516006\n",
      "Batch loss:  0.19326038658618927\n",
      "Batch loss:  0.1934119462966919\n",
      "Batch loss:  0.22442100942134857\n",
      "Batch loss:  0.14085592329502106\n",
      "Batch loss:  0.3113969564437866\n",
      "Batch loss:  0.2293136566877365\n",
      "Batch loss:  0.1697884052991867\n",
      "Batch loss:  0.4101942181587219\n",
      "Batch loss:  0.1633796989917755\n",
      "Batch loss:  0.26851174235343933\n",
      "Batch loss:  0.2940741181373596\n",
      "Batch loss:  0.2470327466726303\n",
      "Batch loss:  0.08027122914791107\n",
      "Batch loss:  0.200103759765625\n",
      "Batch loss:  0.16670988500118256\n",
      "Batch loss:  0.05574645847082138\n",
      "Batch loss:  0.12132528424263\n",
      "Batch loss:  0.20777671039104462\n",
      "Batch loss:  0.16256354749202728\n",
      "Batch loss:  0.23500287532806396\n",
      "Batch loss:  0.30891022086143494\n",
      "Batch loss:  0.2086053043603897\n",
      "Batch loss:  0.2648015022277832\n",
      "Batch loss:  0.1354004442691803\n",
      "Batch loss:  0.22289077937602997\n",
      "Batch loss:  0.2782100439071655\n",
      "Batch loss:  0.11139141768217087\n",
      "Batch loss:  0.14003820717334747\n",
      "Batch loss:  0.3994998335838318\n",
      "Batch loss:  0.2532051205635071\n",
      "Batch loss:  0.19525626301765442\n",
      "Batch loss:  0.3576769530773163\n",
      "Batch loss:  0.1912066638469696\n",
      "Batch loss:  0.16216282546520233\n",
      "Current average loss: 0.20610420210970903\n",
      "Batch loss:  0.07984928786754608\n",
      "Batch loss:  0.27251362800598145\n",
      "Batch loss:  0.27154117822647095\n",
      "Batch loss:  0.3262456953525543\n",
      "Batch loss:  0.13453041017055511\n",
      "Batch loss:  0.13348931074142456\n",
      "Batch loss:  0.3050650954246521\n",
      "Batch loss:  0.2191075086593628\n",
      "Batch loss:  0.1527000218629837\n",
      "Batch loss:  0.1696784347295761\n",
      "Batch loss:  0.1723557859659195\n",
      "Batch loss:  0.15341109037399292\n",
      "Batch loss:  0.24443911015987396\n",
      "Batch loss:  0.2173946350812912\n",
      "Batch loss:  0.12961143255233765\n",
      "Batch loss:  0.16836637258529663\n",
      "Batch loss:  0.19552169740200043\n",
      "Batch loss:  0.24251775443553925\n",
      "Batch loss:  0.2669428884983063\n",
      "Batch loss:  0.11055886745452881\n",
      "Batch loss:  0.17159217596054077\n",
      "Batch loss:  0.13723844289779663\n",
      "Batch loss:  0.21969252824783325\n",
      "Batch loss:  0.2398560494184494\n",
      "Batch loss:  0.20441830158233643\n",
      "Batch loss:  0.15833701193332672\n",
      "Batch loss:  0.11565708369016647\n",
      "Batch loss:  0.19696541130542755\n",
      "Batch loss:  0.13651883602142334\n",
      "Batch loss:  0.1981002241373062\n",
      "Batch loss:  0.17271724343299866\n",
      "Batch loss:  0.2091168463230133\n",
      "Batch loss:  0.23028439283370972\n",
      "Batch loss:  0.20595292747020721\n",
      "Batch loss:  0.14414651691913605\n",
      "Batch loss:  0.21560803055763245\n",
      "Batch loss:  0.11342049390077591\n",
      "Batch loss:  0.18767660856246948\n",
      "Batch loss:  0.17209506034851074\n",
      "Batch loss:  0.3421725332736969\n",
      "Batch loss:  0.11401551216840744\n",
      "Batch loss:  0.150307759642601\n",
      "Batch loss:  0.13394035398960114\n",
      "Batch loss:  0.29186680912971497\n",
      "Batch loss:  0.14943435788154602\n",
      "Batch loss:  0.24672135710716248\n",
      "Batch loss:  0.07960080355405807\n",
      "Batch loss:  0.12373244762420654\n",
      "Batch loss:  0.085027314722538\n",
      "Batch loss:  0.1180669292807579\n",
      "Batch loss:  0.0999073013663292\n",
      "Batch loss:  0.09861002117395401\n",
      "Batch loss:  0.09845998883247375\n",
      "Batch loss:  0.13590583205223083\n",
      "Batch loss:  0.3511294722557068\n",
      "Batch loss:  0.14795958995819092\n",
      "Batch loss:  0.12530885636806488\n",
      "Batch loss:  0.22948503494262695\n",
      "Batch loss:  0.26983505487442017\n",
      "Batch loss:  0.103743776679039\n",
      "Batch loss:  0.2673284113407135\n",
      "Batch loss:  0.1351182907819748\n",
      "Batch loss:  0.11298584192991257\n",
      "Batch loss:  0.19249378144741058\n",
      "Batch loss:  0.23316502571105957\n",
      "Batch loss:  0.2039719820022583\n",
      "Batch loss:  0.20565496385097504\n",
      "Batch loss:  0.31436043977737427\n",
      "Batch loss:  0.18797585368156433\n",
      "Batch loss:  0.13640469312667847\n",
      "Batch loss:  0.2708519697189331\n",
      "Batch loss:  0.12629666924476624\n",
      "Batch loss:  0.10356391221284866\n",
      "Batch loss:  0.14097179472446442\n",
      "Batch loss:  0.11616123467683792\n",
      "Batch loss:  0.2013457864522934\n",
      "Batch loss:  0.2243994027376175\n",
      "Batch loss:  0.2649697959423065\n",
      "Batch loss:  0.23171870410442352\n",
      "Batch loss:  0.14020675420761108\n",
      "Batch loss:  0.25534653663635254\n",
      "Batch loss:  0.180564746260643\n",
      "Batch loss:  0.18298137187957764\n",
      "Batch loss:  0.07002747803926468\n",
      "Batch loss:  0.10733433812856674\n",
      "Batch loss:  0.13847467303276062\n",
      "Batch loss:  0.13007734715938568\n",
      "Batch loss:  0.05374624580144882\n",
      "Batch loss:  0.12963508069515228\n",
      "Batch loss:  0.36801156401634216\n",
      "Batch loss:  0.13784348964691162\n",
      "Batch loss:  0.29403451085090637\n",
      "Batch loss:  0.12130890041589737\n",
      "Batch loss:  0.2583247125148773\n",
      "Batch loss:  0.3300459682941437\n",
      "Batch loss:  0.09714311361312866\n",
      "Batch loss:  0.20273016393184662\n",
      "Batch loss:  0.1153116449713707\n",
      "Batch loss:  0.1539754420518875\n",
      "Batch loss:  0.04042251408100128\n",
      "Current average loss: 0.2058248185734705\n",
      "Batch loss:  0.11524998396635056\n",
      "Batch loss:  0.3522585928440094\n",
      "Batch loss:  0.22165395319461823\n",
      "Batch loss:  0.2697332203388214\n",
      "Batch loss:  0.20543242990970612\n",
      "Batch loss:  0.13172076642513275\n",
      "Batch loss:  0.17687080800533295\n",
      "Batch loss:  0.17752192914485931\n",
      "Batch loss:  0.20288802683353424\n",
      "Batch loss:  0.10034716129302979\n",
      "Batch loss:  0.23452866077423096\n",
      "Batch loss:  0.1604621708393097\n",
      "Batch loss:  0.21296651661396027\n",
      "Batch loss:  0.15782630443572998\n",
      "Batch loss:  0.09301324188709259\n",
      "Batch loss:  0.10819920897483826\n",
      "Batch loss:  0.16682477295398712\n",
      "Batch loss:  0.22254081070423126\n",
      "Batch loss:  0.15843503177165985\n",
      "Batch loss:  0.21343934535980225\n",
      "Batch loss:  0.17801153659820557\n",
      "Batch loss:  0.13730081915855408\n",
      "Batch loss:  0.23425321280956268\n",
      "Batch loss:  0.320455402135849\n",
      "Batch loss:  0.26330840587615967\n",
      "Batch loss:  0.2104097455739975\n",
      "Batch loss:  0.1740434765815735\n",
      "Batch loss:  0.16314460337162018\n",
      "Batch loss:  0.3946079909801483\n",
      "Batch loss:  0.16334640979766846\n",
      "Batch loss:  0.18373002111911774\n",
      "Batch loss:  0.1042698472738266\n",
      "Batch loss:  0.1693428009748459\n",
      "Batch loss:  0.1651461273431778\n",
      "Batch loss:  0.44638147950172424\n",
      "Batch loss:  0.19678643345832825\n",
      "Batch loss:  0.0982547402381897\n",
      "Batch loss:  0.1609509289264679\n",
      "Batch loss:  0.2782772481441498\n",
      "Batch loss:  0.25241950154304504\n",
      "Batch loss:  0.144708514213562\n",
      "Batch loss:  0.21211889386177063\n",
      "Batch loss:  0.27685365080833435\n",
      "Batch loss:  0.17234115302562714\n",
      "Batch loss:  0.34254005551338196\n",
      "Batch loss:  0.13185830414295197\n",
      "Batch loss:  0.20864500105381012\n",
      "Batch loss:  0.17810754477977753\n",
      "Batch loss:  0.16131813824176788\n",
      "Batch loss:  0.14184634387493134\n",
      "Batch loss:  0.14322158694267273\n",
      "Batch loss:  0.22716155648231506\n",
      "Batch loss:  0.18153677880764008\n",
      "Batch loss:  0.14093250036239624\n",
      "Batch loss:  0.11103934049606323\n",
      "Batch loss:  0.44439467787742615\n",
      "Batch loss:  0.21463613212108612\n",
      "Batch loss:  0.20442844927310944\n",
      "Batch loss:  0.14622578024864197\n",
      "Batch loss:  0.1509547382593155\n",
      "Batch loss:  0.10629720240831375\n",
      "Batch loss:  0.24082762002944946\n",
      "Batch loss:  0.08476188778877258\n",
      "Batch loss:  0.10429588705301285\n",
      "Batch loss:  0.10990258306264877\n",
      "Batch loss:  0.14660172164440155\n",
      "Batch loss:  0.18543219566345215\n",
      "Batch loss:  0.18806850910186768\n",
      "Batch loss:  0.21054352819919586\n",
      "Batch loss:  0.10822656750679016\n",
      "Batch loss:  0.16377730667591095\n",
      "Batch loss:  0.30631422996520996\n",
      "Batch loss:  0.20190183818340302\n",
      "Batch loss:  0.15276679396629333\n",
      "Batch loss:  0.1389324963092804\n",
      "Batch loss:  0.09927014261484146\n",
      "Batch loss:  0.3301871716976166\n",
      "Batch loss:  0.17285063862800598\n",
      "Batch loss:  0.35855063796043396\n",
      "Batch loss:  0.13414309918880463\n",
      "Batch loss:  0.29498153924942017\n",
      "Batch loss:  0.18267019093036652\n",
      "Batch loss:  0.21948161721229553\n",
      "Batch loss:  0.17261111736297607\n",
      "Batch loss:  0.16649337112903595\n",
      "Batch loss:  0.18312996625900269\n",
      "Batch loss:  0.15457439422607422\n",
      "Batch loss:  0.12549398839473724\n",
      "Batch loss:  0.0877414271235466\n",
      "Batch loss:  0.23924176394939423\n",
      "Batch loss:  0.14160820841789246\n",
      "Batch loss:  0.30425623059272766\n",
      "Batch loss:  0.09906765073537827\n",
      "Batch loss:  0.36513492465019226\n",
      "Batch loss:  0.3243950605392456\n",
      "Batch loss:  0.2813342809677124\n",
      "Batch loss:  0.2213214486837387\n",
      "Batch loss:  0.16029499471187592\n",
      "Batch loss:  0.2241273820400238\n",
      "Batch loss:  0.17249156534671783\n",
      "Current average loss: 0.20571535700743268\n",
      "Batch loss:  0.36201685667037964\n",
      "Batch loss:  0.17443090677261353\n",
      "Batch loss:  0.19425927102565765\n",
      "Batch loss:  0.18539376556873322\n",
      "Batch loss:  0.25948581099510193\n",
      "Batch loss:  0.19423246383666992\n",
      "Batch loss:  0.1710597723722458\n",
      "Batch loss:  0.2227923423051834\n",
      "Batch loss:  0.1358853578567505\n",
      "Batch loss:  0.26208239793777466\n",
      "Batch loss:  0.2680959105491638\n",
      "Batch loss:  0.13340754806995392\n",
      "Batch loss:  0.12028127163648605\n",
      "Batch loss:  0.10308750718832016\n",
      "Batch loss:  0.15924786031246185\n",
      "Batch loss:  0.1518227607011795\n",
      "Batch loss:  0.1689172089099884\n",
      "Batch loss:  0.1876090168952942\n",
      "Batch loss:  0.44679713249206543\n",
      "Batch loss:  0.21177060902118683\n",
      "Batch loss:  0.2245747148990631\n",
      "Batch loss:  0.18558025360107422\n",
      "Batch loss:  0.27382367849349976\n",
      "Batch loss:  0.19228936731815338\n",
      "Batch loss:  0.3673228323459625\n",
      "Batch loss:  0.15000317990779877\n",
      "Batch loss:  0.11984004825353622\n",
      "Batch loss:  0.1478438824415207\n",
      "Batch loss:  0.2891257405281067\n",
      "Batch loss:  0.16635368764400482\n",
      "Batch loss:  0.32340776920318604\n",
      "Batch loss:  0.32892945408821106\n",
      "Batch loss:  0.1072201058268547\n",
      "Batch loss:  0.29369035363197327\n",
      "Batch loss:  0.19300448894500732\n",
      "Batch loss:  0.19420073926448822\n",
      "Batch loss:  0.0844959244132042\n",
      "Batch loss:  0.1709357500076294\n",
      "Batch loss:  0.13343563675880432\n",
      "Batch loss:  0.2378372699022293\n",
      "Batch loss:  0.3189936578273773\n",
      "Batch loss:  0.24224385619163513\n",
      "Batch loss:  0.14057420194149017\n",
      "Batch loss:  0.17798545956611633\n",
      "Batch loss:  0.3540458381175995\n",
      "Batch loss:  0.21347421407699585\n",
      "Batch loss:  0.11286290735006332\n",
      "Batch loss:  0.23372487723827362\n",
      "Batch loss:  0.15764079988002777\n",
      "Batch loss:  0.11866853386163712\n",
      "Batch loss:  0.13399727642536163\n",
      "Batch loss:  0.14102132618427277\n",
      "Batch loss:  0.27720698714256287\n",
      "Batch loss:  0.13396070897579193\n",
      "Batch loss:  0.17650814354419708\n",
      "Batch loss:  0.23171226680278778\n",
      "Batch loss:  0.22488975524902344\n",
      "Batch loss:  0.0962820053100586\n",
      "Batch loss:  0.20929430425167084\n",
      "Batch loss:  0.15246860682964325\n",
      "Batch loss:  0.20565815269947052\n",
      "Batch loss:  0.3499019742012024\n",
      "Batch loss:  0.35765641927719116\n",
      "Batch loss:  0.26206541061401367\n",
      "Batch loss:  0.1849365085363388\n",
      "Batch loss:  0.21271593868732452\n",
      "Batch loss:  0.20370063185691833\n",
      "Batch loss:  0.2060449719429016\n",
      "Batch loss:  0.2659929096698761\n",
      "Batch loss:  0.10224872827529907\n",
      "Batch loss:  0.1859833151102066\n",
      "Batch loss:  0.14375615119934082\n",
      "Batch loss:  0.3426121175289154\n",
      "Batch loss:  0.17746120691299438\n",
      "Batch loss:  0.1628585308790207\n",
      "Batch loss:  0.2582651376724243\n",
      "Batch loss:  0.09502401947975159\n",
      "Batch loss:  0.19314803183078766\n",
      "Batch loss:  0.1513928472995758\n",
      "Batch loss:  0.1424473226070404\n",
      "Batch loss:  0.1151774525642395\n",
      "Batch loss:  0.1283421516418457\n",
      "Batch loss:  0.2097616344690323\n",
      "Batch loss:  0.06420452147722244\n",
      "Batch loss:  0.2451101541519165\n",
      "Batch loss:  0.2849222719669342\n",
      "Batch loss:  0.21750901639461517\n",
      "Batch loss:  0.22499245405197144\n",
      "Batch loss:  0.28425249457359314\n",
      "Batch loss:  0.27379733324050903\n",
      "Batch loss:  0.12935931980609894\n",
      "Batch loss:  0.0865791067481041\n",
      "Batch loss:  0.3558669686317444\n",
      "Batch loss:  0.3248498737812042\n",
      "Batch loss:  0.15148332715034485\n",
      "Batch loss:  0.3028915524482727\n",
      "Batch loss:  0.3015907108783722\n",
      "Batch loss:  0.24519459903240204\n",
      "Batch loss:  0.23454836010932922\n",
      "Batch loss:  0.22804701328277588\n",
      "Current average loss: 0.205735243182822\n",
      "Batch loss:  0.07870980352163315\n",
      "Batch loss:  0.19953365623950958\n",
      "Batch loss:  0.2135111689567566\n",
      "Batch loss:  0.11416225880384445\n",
      "Batch loss:  0.37265369296073914\n",
      "Batch loss:  0.18570640683174133\n",
      "Batch loss:  0.19071997702121735\n",
      "Batch loss:  0.2348112016916275\n",
      "Batch loss:  0.13685639202594757\n",
      "Batch loss:  0.2977078855037689\n",
      "Batch loss:  0.14322488009929657\n",
      "Batch loss:  0.11154591292142868\n",
      "Batch loss:  0.22680115699768066\n",
      "Batch loss:  0.16404996812343597\n",
      "Batch loss:  0.16357648372650146\n",
      "Batch loss:  0.2577575147151947\n",
      "Batch loss:  0.12527188658714294\n",
      "Batch loss:  0.21619851887226105\n",
      "Batch loss:  0.28384020924568176\n",
      "Batch loss:  0.11651014536619186\n",
      "Batch loss:  0.11006125807762146\n",
      "Batch loss:  0.18369613587856293\n",
      "Batch loss:  0.29414668679237366\n",
      "Batch loss:  0.249226376414299\n",
      "Batch loss:  0.12527944147586823\n",
      "Batch loss:  0.1247503012418747\n",
      "Batch loss:  0.20464207231998444\n",
      "Batch loss:  0.18547323346138\n",
      "Batch loss:  0.20442944765090942\n",
      "Batch loss:  0.18543978035449982\n",
      "Batch loss:  0.1265096366405487\n",
      "Batch loss:  0.1344522088766098\n",
      "Batch loss:  0.17177531123161316\n",
      "Batch loss:  0.15861403942108154\n",
      "Batch loss:  0.16790415346622467\n",
      "Batch loss:  0.2001238912343979\n",
      "Batch loss:  0.16169439256191254\n",
      "Batch loss:  0.2577962279319763\n",
      "Batch loss:  0.18438884615898132\n",
      "Batch loss:  0.1474970132112503\n",
      "Batch loss:  0.1888476312160492\n",
      "Batch loss:  0.2579534947872162\n",
      "Batch loss:  0.17809557914733887\n",
      "Batch loss:  0.14239667356014252\n",
      "Batch loss:  0.14096316695213318\n",
      "Batch loss:  0.26326897740364075\n",
      "Batch loss:  0.27091148495674133\n",
      "Batch loss:  0.16541489958763123\n",
      "Batch loss:  0.18746930360794067\n",
      "Batch loss:  0.1173274964094162\n",
      "Batch loss:  0.19361649453639984\n",
      "Batch loss:  0.16023939847946167\n",
      "Batch loss:  0.2436993271112442\n",
      "Batch loss:  0.23328281939029694\n",
      "Batch loss:  0.14410117268562317\n",
      "Batch loss:  0.14445064961910248\n",
      "Batch loss:  0.21516768634319305\n",
      "Batch loss:  0.1860358715057373\n",
      "Batch loss:  0.3027183413505554\n",
      "Batch loss:  0.11471853405237198\n",
      "Batch loss:  0.4003813564777374\n",
      "Batch loss:  0.11572488397359848\n",
      "Batch loss:  0.1985301673412323\n",
      "Batch loss:  0.1638752967119217\n",
      "Batch loss:  0.24711333215236664\n",
      "Batch loss:  0.15965859591960907\n",
      "Batch loss:  0.1557687520980835\n",
      "Batch loss:  0.11768335849046707\n",
      "Batch loss:  0.17492717504501343\n",
      "Batch loss:  0.25591278076171875\n",
      "Batch loss:  0.2517991065979004\n",
      "Batch loss:  0.3485735356807709\n",
      "Batch loss:  0.37027978897094727\n",
      "Batch loss:  0.07188622653484344\n",
      "Batch loss:  0.18559087812900543\n",
      "Batch loss:  0.19969551265239716\n",
      "Batch loss:  0.13346605002880096\n",
      "Batch loss:  0.2377341389656067\n",
      "Batch loss:  0.30899861454963684\n",
      "Batch loss:  0.30584773421287537\n",
      "Batch loss:  0.23731806874275208\n",
      "Batch loss:  0.11578426510095596\n",
      "Batch loss:  0.14956916868686676\n",
      "Batch loss:  0.1453913152217865\n",
      "Batch loss:  0.159855917096138\n",
      "Batch loss:  0.08189065754413605\n",
      "Batch loss:  0.15083710849285126\n",
      "Batch loss:  0.09596561640501022\n",
      "Batch loss:  0.25999778509140015\n",
      "Batch loss:  0.14695203304290771\n",
      "Batch loss:  0.14785504341125488\n",
      "Batch loss:  0.263373464345932\n",
      "Batch loss:  0.08225218951702118\n",
      "Batch loss:  0.07805002480745316\n",
      "Batch loss:  0.21644644439220428\n",
      "Batch loss:  0.15019744634628296\n",
      "Batch loss:  0.159316286444664\n",
      "Batch loss:  0.1738688200712204\n",
      "Batch loss:  0.1994517296552658\n",
      "Batch loss:  0.13063688576221466\n",
      "Current average loss: 0.20554767396832424\n",
      "Batch loss:  0.1785123497247696\n",
      "Batch loss:  0.22917060554027557\n",
      "Batch loss:  0.1127367913722992\n",
      "Batch loss:  0.3155037760734558\n",
      "Batch loss:  0.2402411550283432\n",
      "Batch loss:  0.1452808529138565\n",
      "Batch loss:  0.1659102737903595\n",
      "Batch loss:  0.2453826665878296\n",
      "Batch loss:  0.1908455342054367\n",
      "Batch loss:  0.19622956216335297\n",
      "Batch loss:  0.2234601527452469\n",
      "Batch loss:  0.16613374650478363\n",
      "Batch loss:  0.10238213837146759\n",
      "Batch loss:  0.18210099637508392\n",
      "Batch loss:  0.31878700852394104\n",
      "Batch loss:  0.09261513501405716\n",
      "Batch loss:  0.11158856004476547\n",
      "Batch loss:  0.27919232845306396\n",
      "Batch loss:  0.20207643508911133\n",
      "Batch loss:  0.10042939335107803\n",
      "Batch loss:  0.0939883217215538\n",
      "Batch loss:  0.1441020518541336\n",
      "Batch loss:  0.488366961479187\n",
      "Batch loss:  0.142431378364563\n",
      "Batch loss:  0.31556251645088196\n",
      "Batch loss:  0.14502878487110138\n",
      "Batch loss:  0.13700667023658752\n",
      "Batch loss:  0.3012325167655945\n",
      "Batch loss:  0.21608364582061768\n",
      "Batch loss:  0.23090144991874695\n",
      "Batch loss:  0.1831946223974228\n",
      "Batch loss:  0.24450664222240448\n",
      "Batch loss:  0.18263578414916992\n",
      "Batch loss:  0.1619884967803955\n",
      "Batch loss:  0.14048030972480774\n",
      "Batch loss:  0.14366526901721954\n",
      "Batch loss:  0.1953907459974289\n",
      "Batch loss:  0.13493482768535614\n",
      "Batch loss:  0.22779835760593414\n",
      "Batch loss:  0.14598822593688965\n",
      "Batch loss:  0.20023132860660553\n",
      "Batch loss:  0.15095767378807068\n",
      "Batch loss:  0.15943573415279388\n",
      "Batch loss:  0.12224195897579193\n",
      "Batch loss:  0.18356212973594666\n",
      "Batch loss:  0.14861217141151428\n",
      "Batch loss:  0.2366344928741455\n",
      "Batch loss:  0.1943047046661377\n",
      "Batch loss:  0.06498748809099197\n",
      "Batch loss:  0.1467740684747696\n",
      "Batch loss:  0.19081531465053558\n",
      "Batch loss:  0.18716102838516235\n",
      "Batch loss:  0.1440444141626358\n",
      "Batch loss:  0.1819036900997162\n",
      "Batch loss:  0.21471945941448212\n",
      "Batch loss:  0.1551860123872757\n",
      "Batch loss:  0.1608739197254181\n",
      "Batch loss:  0.20194052159786224\n",
      "Batch loss:  0.20503610372543335\n",
      "Batch loss:  0.1827843189239502\n",
      "Batch loss:  0.12102567404508591\n",
      "Batch loss:  0.2099626660346985\n",
      "Batch loss:  0.08730160444974899\n",
      "Batch loss:  0.30376386642456055\n",
      "Batch loss:  0.30444827675819397\n",
      "Batch loss:  0.20495975017547607\n",
      "Batch loss:  0.14977245032787323\n",
      "Batch loss:  0.1187673881649971\n",
      "Batch loss:  0.2636178135871887\n",
      "Batch loss:  0.25097742676734924\n",
      "Batch loss:  0.1616797298192978\n",
      "Batch loss:  0.3849364221096039\n",
      "Batch loss:  0.19539916515350342\n",
      "Batch loss:  0.06739988178014755\n",
      "Batch loss:  0.19814296066761017\n",
      "Batch loss:  0.15780754387378693\n",
      "Batch loss:  0.10882333666086197\n",
      "Batch loss:  0.47146400809288025\n",
      "Batch loss:  0.11964283138513565\n",
      "Batch loss:  0.12774322926998138\n",
      "Batch loss:  0.1285402476787567\n",
      "Batch loss:  0.2627347409725189\n",
      "Batch loss:  0.28870511054992676\n",
      "Batch loss:  0.20800648629665375\n",
      "Batch loss:  0.1840723603963852\n",
      "Batch loss:  0.2067634016275406\n",
      "Batch loss:  0.12682801485061646\n",
      "Batch loss:  0.14676161110401154\n",
      "Batch loss:  0.38973185420036316\n",
      "Batch loss:  0.20328089594841003\n",
      "Batch loss:  0.2747705280780792\n",
      "Batch loss:  0.20432232320308685\n",
      "Batch loss:  0.09359484910964966\n",
      "Batch loss:  0.17992791533470154\n",
      "Batch loss:  0.12654773890972137\n",
      "Batch loss:  0.2569732964038849\n",
      "Batch loss:  0.13927048444747925\n",
      "Batch loss:  0.09114935249090195\n",
      "Batch loss:  0.30335843563079834\n",
      "Batch loss:  0.1906476765871048\n",
      "Current average loss: 0.20540715502581902\n",
      "Batch loss:  0.09164562821388245\n",
      "Batch loss:  0.20267143845558167\n",
      "Batch loss:  0.2531168758869171\n",
      "Batch loss:  0.17783693969249725\n",
      "Batch loss:  0.24008797109127045\n",
      "Batch loss:  0.09454862028360367\n",
      "Batch loss:  0.09690044075250626\n",
      "Batch loss:  0.1547597050666809\n",
      "Batch loss:  0.15039990842342377\n",
      "Batch loss:  0.331712007522583\n",
      "Batch loss:  0.15114454925060272\n",
      "Batch loss:  0.15828047692775726\n",
      "Batch loss:  0.13390658795833588\n",
      "Batch loss:  0.23364795744419098\n",
      "Batch loss:  0.2782934904098511\n",
      "Batch loss:  0.17111904919147491\n",
      "Batch loss:  0.24932177364826202\n",
      "Batch loss:  0.2515459954738617\n",
      "Batch loss:  0.11760910600423813\n",
      "Batch loss:  0.10548029839992523\n",
      "Batch loss:  0.3191353678703308\n",
      "Batch loss:  0.0999506264925003\n",
      "Batch loss:  0.09260280430316925\n",
      "Batch loss:  0.09773606806993484\n",
      "Batch loss:  0.3179982006549835\n",
      "Batch loss:  0.18903952836990356\n",
      "Batch loss:  0.12785695493221283\n",
      "Batch loss:  0.13648855686187744\n",
      "Batch loss:  0.19166073203086853\n",
      "Batch loss:  0.14097531139850616\n",
      "Batch loss:  0.2706461548805237\n",
      "Batch loss:  0.195060133934021\n",
      "Batch loss:  0.19476079940795898\n",
      "Batch loss:  0.2717798352241516\n",
      "Batch loss:  0.12550762295722961\n",
      "Batch loss:  0.2287091463804245\n",
      "Batch loss:  0.18350136280059814\n",
      "Batch loss:  0.2650320827960968\n",
      "Batch loss:  0.25661417841911316\n",
      "Batch loss:  0.14141762256622314\n",
      "Batch loss:  0.2260972559452057\n",
      "Batch loss:  0.14325332641601562\n",
      "Batch loss:  0.11971322447061539\n",
      "Batch loss:  0.10194005817174911\n",
      "Batch loss:  0.12516988813877106\n",
      "Batch loss:  0.07282818108797073\n",
      "Batch loss:  0.1840524971485138\n",
      "Batch loss:  0.19259072840213776\n",
      "Batch loss:  0.17441704869270325\n",
      "Batch loss:  0.2667089104652405\n",
      "Batch loss:  0.2063916027545929\n",
      "Batch loss:  0.0990665927529335\n",
      "Batch loss:  0.14010949432849884\n",
      "Batch loss:  0.17223770916461945\n",
      "Batch loss:  0.18143807351589203\n",
      "Batch loss:  0.14314216375350952\n",
      "Batch loss:  0.13587386906147003\n",
      "Batch loss:  0.12918376922607422\n",
      "Batch loss:  0.1885005384683609\n",
      "Batch loss:  0.3453594148159027\n",
      "Batch loss:  0.16288074851036072\n",
      "Batch loss:  0.07157457619905472\n",
      "Batch loss:  0.3780822455883026\n",
      "Batch loss:  0.1908610761165619\n",
      "Batch loss:  0.16447409987449646\n",
      "Batch loss:  0.397747665643692\n",
      "Batch loss:  0.168618306517601\n",
      "Batch loss:  0.1568206548690796\n",
      "Batch loss:  0.19088247418403625\n",
      "Batch loss:  0.1707187443971634\n",
      "Batch loss:  0.16916581988334656\n",
      "Batch loss:  0.17173437774181366\n",
      "Batch loss:  0.05682903528213501\n",
      "Batch loss:  0.2503598630428314\n",
      "Batch loss:  0.15326383709907532\n",
      "Batch loss:  0.07821177691221237\n",
      "Batch loss:  0.23425869643688202\n",
      "Batch loss:  0.1639157384634018\n",
      "Batch loss:  0.15706311166286469\n",
      "Batch loss:  0.375228613615036\n",
      "Batch loss:  0.1888100802898407\n",
      "Batch loss:  0.1718442291021347\n",
      "Batch loss:  0.18199503421783447\n",
      "Batch loss:  0.2817007899284363\n",
      "Batch loss:  0.15158046782016754\n",
      "Batch loss:  0.20376268029212952\n",
      "Batch loss:  0.19560357928276062\n",
      "Batch loss:  0.10312313586473465\n",
      "Batch loss:  0.26509889960289\n",
      "Batch loss:  0.10752217471599579\n",
      "Batch loss:  0.12644492089748383\n",
      "Batch loss:  0.13512541353702545\n",
      "Batch loss:  0.25565478205680847\n",
      "Batch loss:  0.24070735275745392\n",
      "Batch loss:  0.31257668137550354\n",
      "Batch loss:  0.38664770126342773\n",
      "Batch loss:  0.11980900913476944\n",
      "Batch loss:  0.21088604629039764\n",
      "Batch loss:  0.20845256745815277\n",
      "Batch loss:  0.19155530631542206\n",
      "Current average loss: 0.20521961758897808\n",
      "Batch loss:  0.11600664258003235\n",
      "Batch loss:  0.298073410987854\n",
      "Batch loss:  0.19088628888130188\n",
      "Batch loss:  0.3200247287750244\n",
      "Batch loss:  0.21463418006896973\n",
      "Batch loss:  0.20009848475456238\n",
      "Batch loss:  0.2746550440788269\n",
      "Batch loss:  0.17041875422000885\n",
      "Batch loss:  0.20030160248279572\n",
      "Batch loss:  0.15353341400623322\n",
      "Batch loss:  0.09911340475082397\n",
      "Batch loss:  0.1235114112496376\n",
      "Batch loss:  0.1667993813753128\n",
      "Batch loss:  0.1317906677722931\n",
      "Batch loss:  0.3220272362232208\n",
      "Batch loss:  0.26025038957595825\n",
      "Batch loss:  0.1566232442855835\n",
      "Batch loss:  0.141449436545372\n",
      "Batch loss:  0.34841400384902954\n",
      "Batch loss:  0.2120881974697113\n",
      "Batch loss:  0.217104971408844\n",
      "Batch loss:  0.17496153712272644\n",
      "Batch loss:  0.1676183044910431\n",
      "Batch loss:  0.0912613645195961\n",
      "Batch loss:  0.058161698281764984\n",
      "Batch loss:  0.19192039966583252\n",
      "Batch loss:  0.15368807315826416\n",
      "Batch loss:  0.25483834743499756\n",
      "Batch loss:  0.17430001497268677\n",
      "Batch loss:  0.2408626675605774\n",
      "Batch loss:  0.14529436826705933\n",
      "Batch loss:  0.36384570598602295\n",
      "Batch loss:  0.13387775421142578\n",
      "Batch loss:  0.10008175671100616\n",
      "Batch loss:  0.3425368666648865\n",
      "Batch loss:  0.16634710133075714\n",
      "Batch loss:  0.1577417552471161\n",
      "Batch loss:  0.19201669096946716\n",
      "Batch loss:  0.23564094305038452\n",
      "Batch loss:  0.19464416801929474\n",
      "Batch loss:  0.3170042335987091\n",
      "Batch loss:  0.1419883519411087\n",
      "Batch loss:  0.17030714452266693\n",
      "Batch loss:  0.1564134806394577\n",
      "Batch loss:  0.11063031107187271\n",
      "Batch loss:  0.1625630110502243\n",
      "Batch loss:  0.22432394325733185\n",
      "Batch loss:  0.2751944363117218\n",
      "Batch loss:  0.19809386134147644\n",
      "Batch loss:  0.33957764506340027\n",
      "Batch loss:  0.2854149639606476\n",
      "Batch loss:  0.06163505092263222\n",
      "Batch loss:  0.264224648475647\n",
      "Batch loss:  0.3522713780403137\n",
      "Batch loss:  0.2011888325214386\n",
      "Batch loss:  0.15785911679267883\n",
      "Batch loss:  0.27161705493927\n",
      "Batch loss:  0.3555006682872772\n",
      "Batch loss:  0.18062126636505127\n",
      "Batch loss:  0.24909482896327972\n",
      "Batch loss:  0.3495786488056183\n",
      "Batch loss:  0.1455552875995636\n",
      "Batch loss:  0.21694879233837128\n",
      "Batch loss:  0.18618905544281006\n",
      "Batch loss:  0.15800246596336365\n",
      "Batch loss:  0.14373859763145447\n",
      "Batch loss:  0.07580006867647171\n",
      "Batch loss:  0.13275311887264252\n",
      "Batch loss:  0.1114514172077179\n",
      "Batch loss:  0.19296348094940186\n",
      "Batch loss:  0.2741760313510895\n",
      "Batch loss:  0.156205415725708\n",
      "Batch loss:  0.20595549046993256\n",
      "Batch loss:  0.2175631821155548\n",
      "Batch loss:  0.10501056909561157\n",
      "Batch loss:  0.17963945865631104\n",
      "Batch loss:  0.19410227239131927\n",
      "Batch loss:  0.17199699580669403\n",
      "Batch loss:  0.4555414021015167\n",
      "Batch loss:  0.14167220890522003\n",
      "Batch loss:  0.1391056329011917\n",
      "Batch loss:  0.11829016357660294\n",
      "Batch loss:  0.19508835673332214\n",
      "Batch loss:  0.15535864233970642\n",
      "Batch loss:  0.15839089453220367\n",
      "Batch loss:  0.2502313256263733\n",
      "Batch loss:  0.0624430775642395\n",
      "Batch loss:  0.1370495706796646\n",
      "Batch loss:  0.2281348556280136\n",
      "Batch loss:  0.1809999644756317\n",
      "Batch loss:  0.2556690573692322\n",
      "Batch loss:  0.1394319236278534\n",
      "Batch loss:  0.18292321264743805\n",
      "Batch loss:  0.17118200659751892\n",
      "Batch loss:  0.31515979766845703\n",
      "Batch loss:  0.15329036116600037\n",
      "Batch loss:  0.14745813608169556\n",
      "Batch loss:  0.2795790433883667\n",
      "Batch loss:  0.10800664126873016\n",
      "Batch loss:  0.13865461945533752\n",
      "Current average loss: 0.20513120383015\n",
      "Batch loss:  0.1427159607410431\n",
      "Batch loss:  0.1448415219783783\n",
      "Batch loss:  0.3479030430316925\n",
      "Batch loss:  0.11276279389858246\n",
      "Batch loss:  0.18189142644405365\n",
      "Batch loss:  0.1513565331697464\n",
      "Batch loss:  0.1787061244249344\n",
      "Batch loss:  0.1764189600944519\n",
      "Batch loss:  0.11887045949697495\n",
      "Batch loss:  0.25982582569122314\n",
      "Batch loss:  0.17507514357566833\n",
      "Batch loss:  0.1336483359336853\n",
      "Batch loss:  0.42805957794189453\n",
      "Batch loss:  0.22858399152755737\n",
      "Batch loss:  0.3830120861530304\n",
      "Batch loss:  0.18681880831718445\n",
      "Batch loss:  0.18869386613368988\n",
      "Batch loss:  0.1906074434518814\n",
      "Batch loss:  0.12107156217098236\n",
      "Batch loss:  0.16943781077861786\n",
      "Batch loss:  0.2024606615304947\n",
      "Batch loss:  0.10203266888856888\n",
      "Batch loss:  0.1993521749973297\n",
      "Batch loss:  0.16568751633167267\n",
      "Batch loss:  0.2210528701543808\n",
      "Batch loss:  0.171711727976799\n",
      "Batch loss:  0.17516236007213593\n",
      "Batch loss:  0.2015807032585144\n",
      "Batch loss:  0.08354595303535461\n",
      "Batch loss:  0.1639202982187271\n",
      "Batch loss:  0.1077108308672905\n",
      "Batch loss:  0.21256080269813538\n",
      "Batch loss:  0.22346928715705872\n",
      "Batch loss:  0.15809959173202515\n",
      "Batch loss:  0.19269010424613953\n",
      "Batch loss:  0.39816203713417053\n",
      "Batch loss:  0.14781224727630615\n",
      "Batch loss:  0.18800324201583862\n",
      "Batch loss:  0.4485306143760681\n",
      "Batch loss:  0.21673353016376495\n",
      "Batch loss:  0.16386784613132477\n",
      "Batch loss:  0.17766067385673523\n",
      "Batch loss:  0.25041690468788147\n",
      "Batch loss:  0.1682257205247879\n",
      "Batch loss:  0.18515075743198395\n",
      "Batch loss:  0.17651274800300598\n",
      "Batch loss:  0.27562588453292847\n",
      "Batch loss:  0.4022970497608185\n",
      "Batch loss:  0.315944641828537\n",
      "Batch loss:  0.11069528758525848\n",
      "Batch loss:  0.08916030079126358\n",
      "Batch loss:  0.15848639607429504\n",
      "Batch loss:  0.21097950637340546\n",
      "Batch loss:  0.17081673443317413\n",
      "Batch loss:  0.3044617772102356\n",
      "Batch loss:  0.09387733787298203\n",
      "Batch loss:  0.23291601240634918\n",
      "Batch loss:  0.1875462681055069\n",
      "Batch loss:  0.20028753578662872\n",
      "Batch loss:  0.16490906476974487\n",
      "Batch loss:  0.16516809165477753\n",
      "Batch loss:  0.2801584005355835\n",
      "Batch loss:  0.24954985082149506\n",
      "Batch loss:  0.16256394982337952\n",
      "Batch loss:  0.14773431420326233\n",
      "Batch loss:  0.3622836768627167\n",
      "Batch loss:  0.26118582487106323\n",
      "Batch loss:  0.2002386450767517\n",
      "Batch loss:  0.13566617667675018\n",
      "Batch loss:  0.2576867640018463\n",
      "Batch loss:  0.3447091579437256\n",
      "Batch loss:  0.22336344420909882\n",
      "Batch loss:  0.35131946206092834\n",
      "Batch loss:  0.18263021111488342\n",
      "Batch loss:  0.23098620772361755\n",
      "Batch loss:  0.07430537045001984\n",
      "Batch loss:  0.1742931306362152\n",
      "Batch loss:  0.11242201924324036\n",
      "Batch loss:  0.2535459101200104\n",
      "Batch loss:  0.30599474906921387\n",
      "Batch loss:  0.1348143219947815\n",
      "Batch loss:  0.099629707634449\n",
      "Batch loss:  0.1305823177099228\n",
      "Batch loss:  0.30605587363243103\n",
      "Batch loss:  0.2529792785644531\n",
      "Batch loss:  0.19428794085979462\n",
      "Batch loss:  0.14150461554527283\n",
      "Batch loss:  0.10233280062675476\n",
      "Batch loss:  0.15337933599948883\n",
      "Batch loss:  0.1540968418121338\n",
      "Batch loss:  0.21168145537376404\n",
      "Batch loss:  0.12298137694597244\n",
      "Batch loss:  0.22068184614181519\n",
      "Batch loss:  0.05663781985640526\n",
      "Batch loss:  0.18768008053302765\n",
      "Batch loss:  0.1421469748020172\n",
      "Batch loss:  0.1616954803466797\n",
      "Batch loss:  0.10687562823295593\n",
      "Batch loss:  0.09347156435251236\n",
      "Batch loss:  0.3969787359237671\n",
      "Current average loss: 0.20506729177181154\n",
      "Batch loss:  0.1336878091096878\n",
      "Batch loss:  0.14555107057094574\n",
      "Batch loss:  0.24458517134189606\n",
      "Batch loss:  0.3338611423969269\n",
      "Batch loss:  0.1433373987674713\n",
      "Batch loss:  0.3475284278392792\n",
      "Batch loss:  0.19721078872680664\n",
      "Batch loss:  0.1669054478406906\n",
      "Batch loss:  0.20935334265232086\n",
      "Batch loss:  0.08798791468143463\n",
      "Batch loss:  0.17166098952293396\n",
      "Batch loss:  0.21243228018283844\n",
      "Batch loss:  0.24191898107528687\n",
      "Batch loss:  0.14522603154182434\n",
      "Batch loss:  0.1029031053185463\n",
      "Batch loss:  0.16746364533901215\n",
      "Batch loss:  0.333357572555542\n",
      "Training loss epoch: 0.20505698693980326\n",
      "Training accuracy epoch: 93.65%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005af14",
   "metadata": {
    "id": "4005af14"
   },
   "source": [
    "I ended up with a training loss of about 0.21 and a training accuracy of 93.65%. Specific values may differ.\n",
    "\n",
    "At this point, it's a good idea to save the model (or rather the parameter dictionary) so we can continue evaluating the model without having to retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "542403f2",
   "metadata": {
    "id": "542403f2"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"srl_model_fulltrain_2epoch_finetune_1e-05.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc1647",
   "metadata": {
    "id": "01dc1647"
   },
   "source": [
    "## 4. Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44d7b2cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44d7b2cc",
    "outputId": "354bbb39-7cd3-4de5-f7a1-71aa149a841d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-56056f4e4de6>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# Optional step: If you stopped working after part 3, first load the trained model\n",
    "\n",
    "model = SrlModel().to('cuda')\n",
    "model.load_state_dict(torch.load(\"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"))\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e07781",
   "metadata": {
    "id": "44e07781"
   },
   "source": [
    "Now that we have a trained model, let's try labelling an unseen example sentence. The function decode_output takes the logits returned by the model, extracts the argmax to obtain the label predictions for each token, and then translate the result into a list of string labels. The function label_sentence takes a list of input tokens and a predicate index, prepares the model input, calls the model and then calls decode_output to produce a final result.\n",
    "\n",
    "Note that we have already implemented all components necessary (preparing the input data from the token list and predicate index, decoding the model output). But now we are putting it together in one convenient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b72fc567",
   "metadata": {
    "id": "b72fc567"
   },
   "outputs": [],
   "source": [
    "tokens = \"AUN. team spent an hour inside the hospital, where it found evident signs of shelling and gunfire.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9026d718",
   "metadata": {
    "id": "9026d718"
   },
   "outputs": [],
   "source": [
    "def decode_output(logits): # it will be useful to have this in a separate function later on\n",
    "    \"\"\"\n",
    "    Given the model output, return a list of string labels for each token.\n",
    "    \"\"\"\n",
    "    # Apply argmax to get the predicted label index for each token\n",
    "    predicted_labels = torch.argmax(logits, dim=2)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Flatten the tensor to (seq_len,) for easy access\n",
    "    predicted_labels = predicted_labels.view(-1)\n",
    "\n",
    "    # Convert the predicted label indices to their corresponding label names using id_to_role\n",
    "    predicted_label_names = [id_to_role.get(label.item(), -100) for label in predicted_labels]\n",
    "\n",
    "    return predicted_label_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76f21f29",
   "metadata": {
    "id": "76f21f29"
   },
   "outputs": [],
   "source": [
    "def label_sentence(tokens, pred_idx):\n",
    "\n",
    "    # complete this function to prepare token_ids, attention mask, predicate mask, then call the model.\n",
    "    # Decode the output to produce a list of labels.\n",
    "    # Tokenize the input tokens (turn them into token IDs)\n",
    "    tokens_with_specials = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "    padding_length = 128 - len(tokens_with_specials)\n",
    "    if padding_length > 0:\n",
    "        tokens_with_specials = tokens_with_specials + ['[PAD]'] * padding_length\n",
    "    else:\n",
    "        tokens_with_specials = tokens_with_specials[:128]\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens_with_specials)\n",
    "\n",
    "    # Create the attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = [1] * len(tokens_with_specials) + [0] * (128 - len(tokens_with_specials))\n",
    "\n",
    "    # Create the predicate indicator mask (1 at the predicate index, 0 elsewhere)\n",
    "    predicate_mask = [0] * 128\n",
    "    predicate_mask[pred_idx + 1] = 1  # Add 1 to pred_idx due to the [CLS] token at the start\n",
    "\n",
    "    # Convert the inputs to PyTorch tensors and move them to the GPU\n",
    "    token_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
    "    predicate_mask = torch.tensor(predicate_mask).unsqueeze(0).to(device)\n",
    "\n",
    "    # Call the model to get the logits\n",
    "    #vwith torch.no_grad():  # No need to track gradients for inference\n",
    "    logits = model(input_ids=token_ids, attn_mask=attention_mask, pred_indicator=predicate_mask)\n",
    "\n",
    "    # Decode the logits into string labels\n",
    "    label_predictions = decode_output(logits)\n",
    "\n",
    "    return label_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98431b63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98431b63",
    "outputId": "7cf8dbab-8863-4b71-d4f3-c29a452874cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AUN., O)\n",
      "(team, O)\n",
      "(spent, O)\n",
      "(an, O)\n",
      "(hour, O)\n",
      "(inside, O)\n",
      "(the, O)\n",
      "(hospital,, O)\n",
      "(where, O)\n",
      "(it, O)\n",
      "(found, O)\n",
      "(evident, O)\n",
      "(signs, B-ARG0)\n",
      "(of, I-ARG0)\n",
      "(shelling, B-V)\n",
      "(and, B-ARG1)\n",
      "(gunfire., I-ARG1)\n"
     ]
    }
   ],
   "source": [
    "# Now you should be able to run\n",
    "\n",
    "label_test = label_sentence(tokens, 13) # Predicate is \"found\"\n",
    "zip(tokens, label_test)\n",
    "\n",
    "# Print the results\n",
    "for token, label in zip(tokens, label_test):\n",
    "    print(f\"({token}, {label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c710b",
   "metadata": {
    "id": "1d6c710b"
   },
   "source": [
    "The expected output is somethign like this:\n",
    "```   \n",
    " ('A', 'O'),\n",
    " ('U.', 'O'),\n",
    " ('N.', 'O'),\n",
    " ('team', 'O'),\n",
    " ('spent', 'O'),\n",
    " ('an', 'O'),\n",
    " ('hour', 'O'),\n",
    " ('inside', 'O'),\n",
    " ('the', 'B-ARGM-LOC'),\n",
    " ('hospital', 'I-ARGM-LOC'),\n",
    " (',', 'O'),\n",
    " ('where', 'B-ARGM-LOC'),\n",
    " ('it', 'B-ARG0'),\n",
    " ('found', 'B-V'),\n",
    " ('evident', 'B-ARG1'),\n",
    " ('signs', 'I-ARG1'),\n",
    " ('of', 'I-ARG1'),\n",
    " ('shelling', 'I-ARG1'),\n",
    " ('and', 'I-ARG1'),\n",
    " ('gunfire', 'I-ARG1'),\n",
    " ('.', 'O'),\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0237e",
   "metadata": {
    "id": "b6c0237e"
   },
   "source": [
    "### 5. Evaluation 1: Token-Based Accuracy\n",
    "We want to evaluate the model on the dev or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc7aa897",
   "metadata": {
    "id": "cc7aa897"
   },
   "outputs": [],
   "source": [
    "dev_data = SrlData(\"propbank_dev.tsv\") # Takes a while because we preprocess all data offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd62569e",
   "metadata": {
    "id": "dd62569e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dev_data, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bcb3a78",
   "metadata": {
    "id": "4bcb3a78"
   },
   "outputs": [],
   "source": [
    "# Optional: Load the model again if you stopped working prior to this step.\n",
    "# model = SrlModel()\n",
    "# model.load_state_dict(torch.load(\"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"))\n",
    "# model = mode.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ababd",
   "metadata": {
    "id": "cc8ababd"
   },
   "source": [
    "Let us complete the evaluate_token_accuracy function below. The function should iterate through the items in the data loader (see training loop in part 3). Running the model on each sentence/predicate pair and extract the predictions.\n",
    "\n",
    "For each sentence, counting the correct predictions and the total predictions. Finally, computing the accuracy as #correct_predictions / #total_predictions\n",
    "\n",
    "Careful: We need to filter out the padded positions ([PAD] target tokens), as well as [CLS] and [SEP]. It's okay to include [B-V] in the count though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa091465",
   "metadata": {
    "id": "fa091465"
   },
   "outputs": [],
   "source": [
    "def evaluate_token_accuracy(model, loader):\n",
    "\n",
    "    model.eval() # put model in evaluation mode\n",
    "\n",
    "    # Variables to track accuracy\n",
    "    total_correct = 0  # Number of correct predictions\n",
    "    total_predictions = 0  # Total number of tokens considered for accuracy\n",
    "\n",
    "    # Iterate over the data loader (this will return batches)\n",
    "    for batch in loader:\n",
    "        # Assuming `batch` is a tuple: (input_ids, attention_mask, predicate_mask, labels)\n",
    "        input_ids, attention_mask, predicate_mask, labels = batch\n",
    "\n",
    "        # Move tensors to the correct device (e.g., GPU if available)\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "        predicate_mask = predicate_mask.to(model.device)\n",
    "        labels = labels.to(model.device)\n",
    "\n",
    "        # Run the model on the input data\n",
    "        with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, predicate_mask=predicate_mask)\n",
    "\n",
    "        # Get the predicted token labels (argmax of logits)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Flatten the predictions and labels to compare them token by token\n",
    "        predictions = predictions.flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        # Iterate over each token in the batch (ignoring padding tokens)\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            # We will ignore [PAD], [CLS], and [SEP] tokens\n",
    "            if label.item() == role_to_id['[PAD]']:\n",
    "                continue  # Skip padding tokens\n",
    "            if label.item() == role_to_id['[CLS]'] or label.item() == role_to_id['[SEP]']:\n",
    "                continue  # Skip [CLS] and [SEP] tokens\n",
    "\n",
    "            # Count correct predictions\n",
    "            total_predictions += 1\n",
    "            if pred.item() == label.item():\n",
    "                total_correct += 1\n",
    "\n",
    "    # Compute accuracy\n",
    "    if total_predictions > 0:\n",
    "        acc = total_correct / total_predictions\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "    else:\n",
    "        print(\"No valid tokens to evaluate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb27239",
   "metadata": {
    "id": "9fb27239"
   },
   "source": [
    "### 6. Span-Based evaluation\n",
    "\n",
    "While the accuracy score in part 5 is encouraging, an accuracy-based evaluation is problematic for two reasons. First, most of the target labels are actually 0. Second, it only tells us that per-token prediction works, but does not directly evaluate the SRL performance.\n",
    "\n",
    "Instead, SRL systems are typically evaluated on micro-averaged precision, recall, and F1-score for predicting labeled spans.\n",
    "\n",
    "More specifically, for each sentence/predicate input, we run the model, decode the output, and extract a set of labeled spans (from the output and the target labels). These spans are (i,j,label) tuples.  \n",
    "\n",
    "We then compute the true_positives, false_positives, and false_negatives based on these spans.\n",
    "\n",
    "In the end, we can compute\n",
    "\n",
    "* Precision:  true_positive / (true_positives + false_positives)  , that is the number of correct spans out of all predicted spans.\n",
    "\n",
    "* Recall: true_positives / (true_positives + false_negatives) , that is the number of correct spans out of all target spans.\n",
    "\n",
    "* F1-score:   (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "For example, consider\n",
    "\n",
    "| |[CLS]|The|judge|scheduled|to|preside|over|his|trial|was|removed|from|the|case|today|.|             \n",
    "|--||---|-----|---------|--|-------|----|---|-----|---|-------|----|---|----|-----|-|             \n",
    "||0|1|2|3|4|5|6|7|8|9|1O|11|12|13|14|15|\n",
    "|target|[CLS]|B-ARG1|I-ARG1|B-V|B-ARG2|I-ARG2|I-ARG2|I-ARG2|I-ARG2|O|O|O|O|O|O|O|\n",
    "|prediction|[CLS]|B-ARG1|I-ARG1|B-V|I-ARG2|I-ARG2|O|O|O|O|O|O|O|O|B-ARGM-TMP|O|\n",
    "\n",
    "The target spans are (1,2,\"ARG1\"), and (4,8,\"ARG2\").\n",
    "\n",
    "The predicted spans would be (1,2,\"ARG1\"), (14,14,\"ARGM-TMP\"). Note that in the prediction, there is no proper ARG2 span because we are missing the B-ARG2 token, so this span should not be created.\n",
    "\n",
    "So for this sentence we would get: true_positives: 1 false_positives: 1 false_negatives: 1\n",
    "\n",
    "Let us complete the function evaluate_spans that performs the span-based evaluation on the given model and data loader. We can use the provided extract_spans function, which returns the spans as a dictionary. For example\n",
    "{(1,2): \"ARG1\", (4,8):\"ARG2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "379cfe6a",
   "metadata": {
    "id": "379cfe6a"
   },
   "outputs": [],
   "source": [
    "def extract_spans(labels):\n",
    "    spans = {} # map (start,end) ids to label\n",
    "    current_span_start = 0\n",
    "    current_span_type = \"\"\n",
    "    inside = False\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.startswith(\"B\"):\n",
    "            if inside:\n",
    "                if current_span_type != \"V\":\n",
    "                    spans[(current_span_start,i)] = current_span_type\n",
    "            current_span_start = i\n",
    "            current_span_type = label[2:]\n",
    "            inside = True\n",
    "        elif inside and label.startswith(\"O\"):\n",
    "            if current_span_type != \"V\":\n",
    "                spans[(current_span_start,i)] = current_span_type\n",
    "            inside = False\n",
    "        elif inside and label.startswith(\"I\") and label[2:] != current_span_type:\n",
    "            if current_span_type != \"V\":\n",
    "                spans[(current_span_start,i)] = current_span_type\n",
    "            inside = False\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a5486ca",
   "metadata": {
    "id": "1a5486ca"
   },
   "outputs": [],
   "source": [
    "def evaluate_spans(model, loader):\n",
    "\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idx, batch in enumerate(loader):\n",
    "\n",
    "        # Assuming batch is a tuple: (input_ids, attention_mask, predicate_mask, labels)\n",
    "        input_ids, attention_mask, predicate_mask, labels = batch\n",
    "\n",
    "        # Move tensors to the correct device\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "        predicate_mask = predicate_mask.to(model.device)\n",
    "        labels = labels.to(model.device)\n",
    "\n",
    "        # Get the model predictions (logits)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, predicate_mask=predicate_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)  # Get the index of the highest probability (predicted label)\n",
    "\n",
    "        # Flatten the predictions and labels for easier comparison\n",
    "        predictions = predictions.flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        # Extract spans from the predictions and labels\n",
    "        predicted_spans = extract_spans([id_to_role[label.item()] for label in predictions])\n",
    "        target_spans = extract_spans([id_to_role[label.item()] for label in labels])\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        for span in predicted_spans:\n",
    "            if span in target_spans and target_spans[span] == predicted_spans[span]:\n",
    "                total_tp += 1\n",
    "            else:\n",
    "                total_fp += 1\n",
    "\n",
    "        for span in target_spans:\n",
    "            if span not in predicted_spans:\n",
    "                total_fn += 1\n",
    "\n",
    "    # Calculate Precision, Recall, and F1-score\n",
    "    total_p = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    total_r = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    total_f = (2 * total_p * total_r) / (total_p + total_r) if (total_p + total_r) > 0 else 0\n",
    "\n",
    "    print(f\"Overall P: {total_p}  Overall R: {total_r}  Overall F1: {total_f}\")\n",
    "\n",
    "#evaluate(model, loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e35d04",
   "metadata": {
    "id": "c0e35d04"
   },
   "source": [
    "In my evaluation, I got an F score of 0.82  (which slightly below the state-of-the art in 2018)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
